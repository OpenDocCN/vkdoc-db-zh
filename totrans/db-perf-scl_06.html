<html><head></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-1-4842-9711-7_7"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">F. C. Mendes et al.</span><span class="ContextInformationBookTitles"><span class="BookTitle">Database Performance at Scale</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-1-4842-9711-7_7">https://doi.org/10.1007/978-1-4842-9711-7_7</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">7. Infrastructure and Deployment Models</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Felipe Cardeneti Mendes</span><sup><a href="#Aff5">1</a> <span class="ContactIcon"> </span></sup>, </span><span class="Author"><span class="AuthorName">Piotr Sarna</span><sup><a href="#Aff6">2</a></sup>, </span><span class="Author"><span class="AuthorName">Pavel Emelyanov</span><sup><a href="#Aff7">3</a></sup> and </span><span class="Author"><span class="AuthorName">Cynthia Dunlop</span><sup><a href="#Aff8">4</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">São Paulo, São Paulo, Brazil</div></div><div class="Affiliation" id="Aff6"><span class="AffiliationNumber">(2)</span><div class="AffiliationText">Pruszków, Poland</div></div><div class="Affiliation" id="Aff7"><span class="AffiliationNumber">(3)</span><div class="AffiliationText">Moscow, Russia</div></div><div class="Affiliation" id="Aff8"><span class="AffiliationNumber">(4)</span><div class="AffiliationText">Carpinteria, CA, USA</div></div><div class="ClearBoth"> </div></div></div><div class="ArticleOrChapterToc"><div class="TocLine"><a href="#Sec1">Core Hardware Considerations for Speed at Scale</a></div><div class="TocLine"><a href="#Sec5">Recommendations for Specific Hardware Components</a></div><div class="TocLine"><a href="#Sec16">Considerations in the Cloud</a></div><div class="TocLine"><a href="#Sec17">Fully Managed Database-as-a-Service</a></div><div class="TocLine"><a href="#Sec18">Serverless Deployment Models</a></div><div class="TocLine"><a href="#Sec19">Containerization and Kubernetes</a></div><div class="TocLine"><a href="#Sec20">Summary</a></div></div><!--End Abstract--><div class="Fulltext">
        <p class="Para" id="Par2">As noted in the previous chapter, many modern databases offer capabilities beyond “just” storing and retrieving data. But all databases are ultimately built from the ground up in order to serve I/O in the most efficient way possible. And it’s crucial to remember this when selecting your infrastructure and deployment model of choice.</p>
        <p class="Para" id="Par3">In theory, a database’s purpose is fairly simple: You submit a request and expect to receive a response. But as you have seen in the previous chapters, an insane level of engineering effort is spent on continuously enhancing and speeding up this process. Very likely, years and years were dedicated to optimizing algorithms that may give you a processing boost of a few CPU cycles, or minimizing the amount of memory fragmentation, or reducing the amount of storage I/O needed to look up a specific set of data. All these advancements, eventually, converge to create a database suitable for performance at scale.</p>
        <p class="Para" id="Par4">Regardless of your database selection, you may eventually hit a wall that no engineering effort can break through: the database’s <span id="ITerm1">physical hardware</span>. It makes very little sense to have a solution engineered for performance when the hardware you throw at it may be suboptimal. Similarly, a less performant database will likely be unable to make efficient use of an abundance of available physical resources.</p>
        <p class="Para" id="Par5">This chapter looks at critical considerations and tradeoffs when selecting CPUs, memory, storage, and networking for your distributed database infrastructure. It describes how different resources cooperate and how to configure the database to deliver the best performance. Special attention is drawn to storage I/O as the most difficult component to deal with. There’s also a close look at optimal <span id="ITerm2">cloud-based deployments</span> suitable for highly-performant distributed databases (given that these are the deployment preference of most businesses).</p>
        <p class="Para" id="Par6">While it is true that a Database-as-a-Service (DBaaS) <span id="ITerm3">deployment</span> will shield you from many infrastructure and hardware decisions through your selection process, a fundamental understanding of the generic compute resources required by any database is important for identifying potential bottlenecks that may limit performance. After an introduction to the hardware that’s involved in every deployment model—whether you think about it or not—the chapter shifts focus to different deployment options and their impact on performance. It covers the special considerations associated with cloud-hosted deployments, database-as-a-service, serverless, containerization, and container orchestration technologies, such as Kubernetes.</p>
        <section class="Section1 RenderAsSection1" id="Sec1">
          <h2 class="Heading">Core Hardware Considerations for Speed at Scale</h2>
          <div class="Para" id="Par7">When you are designing systems to handle large amounts of data and requests at scale, the primary <span id="ITerm4">hardware considerations</span> are:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
              <p class="Para" id="Par8">Storage</p>
            </li><li>
              <p class="Para" id="Par9">CPU (cores)</p>
            </li><li>
              <p class="Para" id="Par10">Memory (RAM)</p>
            </li><li>
              <p class="Para" id="Par11">Network interfaces</p>
            </li></ul></div></div>
          <p class="Para" id="Par12">Each could be a potential bottleneck for internal database latency: The delay from when a request is received by the database (or a node in the database) and when the database provides a response.</p>
          <section class="Section2 RenderAsSection2" id="Sec2">
            <h3 class="Heading">Identifying the Source of Your Performance Bottlenecks</h3>
            <p class="Para" id="Par13">Knowing your database’s write and read paths is helpful for identifying potential <span id="ITerm5">performance bottlenecks</span> and tracking down the culprit. It’s also key to understanding what physical resources your use case may be mostly bound against.</p>
            <p class="Para" id="Par14">For example, write-optimized databases carry this nomenclature because writes primarily go to memory, rather than being immediately persisted into disk. However, most modern databases need to employ some “crash-recovery” mechanism and avoid data loss caused by unexpected service interruptions. As a result, even write-optimized databases will also resort to disk access to quickly persist your data, just in case. For example, writes to Cassandra clusters will be persisted to a “write ahead log” disk structure called the “<span id="ITerm6">commit log</span>” and a memory structure that’s named a “memtable.” A write is considered successful only after both operations succeed.</p>
            <p class="Para" id="Par15">On the other side of the spectrum, the database’s read path will typically also involve several physical components. Assuming that you’re not using an in-memory database, then the read path will start by checking whether the data you are looking for is present within the database cache. But if it’s not, the database needs to look up and retrieve the data from disk, de-serialize it, and then answer with the results.</p>
            <p class="Para" id="Par16">Network also plays a crucial role throughout the entire process. When you write, data needs to be rapidly replicated to other <span id="ITerm7">replicas</span>. When you read, the database needs to select the correct replicas (shards) containing the data that the application is after, thus potentially having to communicate with other nodes in the cluster. Moreover, strong consistency use cases always require the response of a majority of members for an operation to be successful—so delayed responses from a replica can dramatically increase the tail latency of a request routed to it.</p>
          </section>

          <section class="Section2 RenderAsSection2" id="Sec3">
            <h3 class="Heading">Achieving Balance</h3>
            <p class="Para" id="Par17"><span id="ITerm8">Balance</span> is key to <em class="EmphasisTypeItalic ">any</em> distributed system, including and beyond databases. It makes very little sense to try to achieve 1 million <span id="ITerm9">operations per second (OPS)</span> in a system that has the fastest network link available but relies on very few CPUs. Similarly, it’s not very efficient to purchase the most expensive and performant infrastructure for your solution if your use case requires only 10K OPS.</p>
            <p class="Para" id="Par18">Additionally, it’s important to recognize that a cluster imbalance can easily drag down performance across your entire distributed system. This happens because a distributed system cannot be faster than your slowest component—a fact that frequently surprises people.</p>
            <div class="Para" id="Par19">Here’s a real-life example. A customer reported elevated latencies affecting their entire 18-node cluster. After collecting system information, we noticed that the majority of their nodes were properly using locally-attached <span id="ITerm10">nonvolatile memory express (NVMe) disks</span>—except for one that had a software <span id="ITerm11">Redundant Array of Independent Disks (RAID)</span> with a mix of NVMes and network-attached disks. The customer clarified that they were running out of storage space and decided to attach another disk in order to relieve the problem. However, they weren’t aware that this introduced a ticking time bomb into their entire cluster. Here’s a brief explanation of what happened from a technical <span id="ITerm12">perspective</span>:<div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent">
                  <p class="Para" id="Par20">With a slow disk introduced in their RAID array, storage I/O operations in that specific replica took longer to complete.</p>
                </div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent">
                  <p class="Para" id="Par21">As a result, the remaining replicas took additional time whenever sending or waiting for a response that would require disk I/O.</p>
                </div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent">
                  <p class="Para" id="Par22">As more and more requests came in, all these delays eventually created a waiting queue on the replicas.</p>
                </div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">4.</div><div class="ItemContent">
                  <p class="Para" id="Par23">As the queue kept growing, this eventually affected the replicas’ performance, which ended up affecting the entire cluster’s performance.</p>
                </div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">5.</div><div class="ItemContent">
                  <p class="Para" id="Par24">From that point on, the entire cluster speed was impeded by the speed of its slowest node: the one that had the slowest disk.</p>
                </div><div class="ClearBoth"> </div></li></ol></div></div>
          </section>

          <section class="Section2 RenderAsSection2" id="Sec4">
            <h3 class="Heading">Setting Realistic Expectations</h3>
            <div class="Para" id="Par25">Even the most powerful hardware cannot ensure impressive <em class="EmphasisTypeItalic ">end-to-end (or round-trip)</em> latency—the entire cycle time from when a client sends a request to the server until it obtains a response. The end-to-end latency could be undermined by factors that might be outside of the database’s <span id="ITerm13">control</span>. For example:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
                <p class="Para" id="Par26"><span id="ITerm14">Multi-hop routing</span> of packets from your client application to the database server, adding hundreds of milliseconds in latency</p>
              </li><li>
                <p class="Para" id="Par27">Client driver settings, connecting and sending requests to a remote datacenter</p>
              </li><li>
                <p class="Para" id="Par28">Consistency levels that require both local and remote datacenter responses</p>
              </li><li>
                <p class="Para" id="Par29">Poor network performance between clients and database <span id="ITerm15">servers</span></p>
              </li><li>
                <p class="Para" id="Par30">Protocol overheads</p>
              </li><li>
                <p class="Para" id="Par31">Client-side performance bottlenecks</p>
              </li></ul></div></div>
          </section>

        </section>

        <section class="Section1 RenderAsSection1" id="Sec5">
          <h2 class="Heading">Recommendations for Specific Hardware Components</h2>
          <div class="Para" id="Par32">This section takes a deeper look at each of the primary hardware considerations:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
              <p class="Para" id="Par33">Storage</p>
            </li><li>
              <p class="Para" id="Par34">CPU (cores)</p>
            </li><li>
              <p class="Para" id="Par35">Memory (RAM)</p>
            </li><li>
              <p class="Para" id="Par36">Network interfaces</p>
            </li></ul></div></div>
          <section class="Section2 RenderAsSection2" id="Sec6">
            <h3 class="Heading">Storage</h3>
            <p class="Para" id="Par37">One of the fastest ways to undermine all your other performance optimizations is to send every read and write <span id="ITerm16">operation</span> through an unsuitable disk. Although recent technology advancements greatly improved the performance of storage devices, disks are (by far) still the slowest component in a computer system.</p>
            <div class="Para" id="Par38">From a performance standpoint, <span id="ITerm17">disk performance</span> is typically measured in two dimensions:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
                <p class="Para" id="Par39">The bandwidth available for sequential reads and writes</p>
              </li><li>
                <p class="Para" id="Par40">The IOPS for random reads and writes</p>
              </li></ul></div></div>
            <p class="Para" id="Par41">Database engineers obsess over optimizing disk access patterns with respect to those two <span id="ITerm18">dimensions</span>. People who are selecting, managing, or using a database should focus on two additional disk considerations: the storage technology and the disk size.</p>
            <section class="Section3 RenderAsSection3" id="Sec7">
              <h4 class="Heading">Disk Types</h4>
              <p class="Para" id="Par42">Locally-attached NVMe <span id="ITerm19">Solid State Drives (SSDs)</span><span id="ITerm20"/> are the standard when latency is critical. Compared with other bus interfaces, NVMe SSDs connected to a Peripheral Component Interconnect Express (PCIe) interface will generally deliver lower latencies than the <span id="ITerm21">Serial AT Attachment (SATA) interface</span>. If your workload isn’t super latency sensitive, you could also consider using disks via the SATA interface. But, definitely avoid using network-attached disks if you expect single-digit millisecond latencies. Being network attached, these disks require an additional hop to reach a storage server, and that ends up increasing latency for every database request.</p>
              <p class="Para" id="Par43">If your focus is on throughput and latency really doesn’t matter for your use case (e.g., for moving data into a data warehouse), you <em class="EmphasisTypeItalic ">might</em> be able to get away with a persistent disk—but it’s not recommended. By persistent disks, we mean durable network storage devices that your VMs can access like physical disks, but are located independently from your VMs. We’re not going to pick on any specific vendors, but a little research should reveal issues like subpar performance and overall instability. If you’re forced to work with persistent disks, be prepared to craft a creative solution.<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source" role="doc-noteref">1</a></sup></p>
              <p class="Para" id="Par45"><span id="ITerm22">Hard disk drives (HDDs)</span><span id="ITerm23"/> might fast become a bottleneck. Since SSDs are getting progressively cheaper and cheaper, using HDDs is not recommended. Some workloads may work with HDDs, especially if they play nice and minimize random seeks. An example of an HDD-friendly workload is a write-mostly (98 percent writes) workload with minimal random reads. If you decide to use HDDs, try to allocate a separate disk for the commit log.</p>
              <div class="Para" id="Par46">ScyllaDB published benchmarking results of several different storage devices— demonstrating how they perform under extreme load simulating typical database access patterns.<sup><a epub:type="noteref" href="#Fn2" id="Fn2_source" role="doc-noteref">2</a></sup> For example, Figures <span class="InternalRef"><a href="#Fig1">7-1</a></span> through <span class="InternalRef"><a href="#Fig4">7-4</a></span> visualize the different performance characteristics from two NVMes—a persistent disk and an <span id="ITerm24">HDD</span>.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1">
                  <img alt="" aria-describedby="d65e854" src="../images/541783_1_En_7_Chapter/541783_1_En_7_Fig1_HTML.png" style="width:42.93em"/><div class="TextObject" id="d65e854">
                    <p class="Para" id="Par147">Two graphs of p 50 and p 95 latency from 0 to 800 M B per second. The graphs depict a decreasing trend. Two shaded strips are given to the right of the graphs.</p>
                  </div>
                  
                </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 7-1</span>
                    <p class="SimplePara">NVMe bandwidth/latency graphs for an AWS i3.2xlarge instance type</p>
                  </div></figcaption></figure><figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2">
                  <img alt="" aria-describedby="d65e877" src="../images/541783_1_En_7_Chapter/541783_1_En_7_Fig2_HTML.png" style="width:42.82em"/><div class="TextObject" id="d65e877">
                    <p class="Para" id="Par148">Two graphs of p 50 and p 95 latency from 0 to 1 G B per second. The graphs depict a decreasing trend. Two shaded strips are given to the right of the graphs.</p>
                  </div>
                  
                </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 7-2</span>
                    <p class="SimplePara">Bandwidth/latency graphs for an AWS Im4gn.4xlarge instance type using AWS Nitro SSDs</p>
                  </div></figcaption></figure><figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3">
                  <img alt="" aria-describedby="d65e912" src="../images/541783_1_En_7_Chapter/541783_1_En_7_Fig3_HTML.jpg" style="width:42.82em"/><div class="TextObject" id="d65e912">
                    <p class="Para" id="Par149">Two graphs of p 50 and p 95 latency from 0 to 600 M B per second. The graphs depict a decreasing trend. Two shaded strips are given to the right of the graphs.</p>
                  </div>
                  
                </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 7-3</span>
                    <p class="SimplePara">Bandwidth/latency graphs for a Google Cloud n2-standard-8 instance type with a 2TB <span id="ITerm25">SSD</span> persistent disk<sup><a epub:type="noteref" href="#Fn3" id="Fn3_source" role="doc-noteref">3</a></sup></p>
                  </div></figcaption></figure><figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4">
                  <img alt="" aria-describedby="d65e946" src="../images/541783_1_En_7_Chapter/541783_1_En_7_Fig4_HTML.png" style="width:42.93em"/><div class="TextObject" id="d65e946">
                    <p class="Para" id="Par150">Two graphs of p 50 and p 95 latency from 0 to 175 M B per second. The graphs depict a decreasing trend. Two shaded strips are given to the right of the graphs.</p>
                  </div>
                  
                </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 7-4</span>
                    <p class="SimplePara">Bandwidth/latency <span id="ITerm26">graphs</span> for a Toshiba DT01ACA200 hard disk drive<sup><a epub:type="noteref" href="#Fn4" id="Fn4_source" role="doc-noteref">4</a></sup></p>
                  </div></figcaption></figure></div>
            </section>

            <section class="Section3 RenderAsSection3" id="Sec8">
              <h4 class="Heading">Disk Setup</h4>
              <p class="Para" id="Par50">We hear a lot of questions about RAID setups. Hardware RAIDs are commonly used to avoid outages introduced by disk failures. As a result, the RAID-5 (distributed parity) setup is often used.</p>
              <p class="Para" id="Par51">However, distributed <span id="ITerm27">databases</span> typically have their own internal replication mechanism to allow for business continuity and achieve high availability. Therefore, RAID setups employing data mirroring or distributed parity have proven to be very detrimental to disk I/O performance and, fairly often, are used redundantly. On top of that, we have found that some hardware RAID vendors deliver poor performance results depending on your database access mechanisms. One notable example: hardware RAIDs that are unable to perform efficiently via asynchronous I/O or direct I/O calls. If you believe your disk I/O is suboptimal, consider directly exposing the disks from your hardware RAID to your operating <span id="ITerm28">system</span>.</p>
              <p class="Para" id="Par52">Conversely, RAID-0 (striping) setups often provide a boost in disk I/O performance and allow the database to achieve higher IOPS and bandwidth than a single disk can provide. The general recommendation for creating a RAID-0 setup is to use all disks of the same type and capacity to avoid variable performance during your daily workload. While it is true you would lose the entire RAID array in the event of a disk failure, the replication performed by your distributed database should be sufficient to ensure that your data remains available.</p>
              <div class="Para" id="Par53">A couple of additional considerations related to disk setup:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
                  <p class="Para" id="Par54"><strong class="EmphasisTypeBold ">Storage servers often serve several other users and workloads at the same time.</strong> Therefore, even though disks would be dedicated to the database, your access performance can be undermined by factors like the level to which the storage system is serving other users concurrently. Most of the time, the storage medium provided to you will not be optimal for supporting a low-latency database workload. This can often be mitigated by ensuring that the disks are allocated from a high-performing disk pool.</p>
                </li><li>
                  <p class="Para" id="Par55"><strong class="EmphasisTypeBold ">It’s important to expose your database infrastructure disks directly to the operating system guest from your hypervisor.</strong> We have seen many situations where the I/O capacity of a database was greatly impacted when disks were virtualized. To eliminate any possible bottlenecks in a low-latency environment, give your database direct access to your disks so that they can perform I/O as they were designed to.</p>
                </li></ul></div></div>
            </section>

            <section class="Section3 RenderAsSection3" id="Sec9">
              <h4 class="Heading">Disk Size</h4>
              <p class="Para" id="Par56">When considering how much storage you need, be sure to account for your existing data—replicated—plus your anticipated near-term data <span id="ITerm29">growth</span>, and also leave sufficient room for the overhead of internal operations (like compactions [for LSM-tree-based databases], the commit log, backups, etc.).</p>
              <div class="Para" id="Par57">As Chapter <span class="ExternalRef"><a href="541783_1_En_8_Chapter.xhtml"><span class="RefSource">8</span></a></span> discusses, the most common topology involves three replicas for each dataset. Assume you have 5TB of raw data and use a replication factor of three:<div class="UnorderedList"><ul class="UnorderedListMarkNone"><li>
                  <p class="Para" id="Par58">5TB Data X 3 RF = 15TB</p>
                </li></ul></div></div>
              <div class="Para" id="Par59">But <span id="ITerm30">15TB</span> is just a starting point since there are other sizing criteria:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
                  <p class="Para" id="Par60">What is your dataset’s growth rate? (How much do you ingest per hour or day?)</p>
                </li><li>
                  <p class="Para" id="Par61">Will you store everything forever, or will you have an eviction process (for example, based on Time To Live [TTL])?</p>
                </li><li>
                  <p class="Para" id="Par62">Is your growth rate stable (a fixed rate of ingestion per week/day/hour) or is it stochastic and bursty? The former would make it more predictable; the latter may mean you have to give yourself more leeway to account for unpredictable but probabilistic events.</p>
                </li></ul></div></div>
              <p class="Para" id="Par63">You can model your data’s growth rate based on the number of users or endpoints and how that number is expected to grow over time. Alternately, data models are often enriched over time, resulting in more data per source. Or your sampling rate may increase. For example, your system may begin ingesting data every five seconds rather than every minute. All of these considerations impact your data storage volume.</p>
              <p class="Para" id="Par64">It’s strongly recommended that you select storage that’s suitable for where you expect to end up after a certain time span. If you’re running your database on a public cloud provider (self-managed or as a fully-managed Database-as-a-Service <span id="ITerm31">[DBaaS]</span>), you won’t need very much lead <span id="ITerm32">time</span> to provision new hardware and expand your cluster. However, for an on-premises hardware purchase, you may need to provision based on your quarterly or annual budgeting process. You could also face delays due to the supply chain disruptions that have become increasingly common.</p>
              <p class="Para" id="Par65">Also, be sure to leave storage space for internal temporary operations such as compaction, repairs, backups, and commit logs, as well as any other background process that may temporarily introduce a space amplification. On the other hand, if you’re using compression, be sure to factor in the amount of space that your selected compression algorithm can save you.</p>
              <p class="Para" id="Par66">Finally, recognize that every database has an ideal memory-to-storage ratio—for example, a certain amount of TB or GB per node that it can support with optimal performance. If this isn’t readily apparent in your database’s documentation, press your vendor for their recommendation.</p>
            </section>

            <section class="Section3 RenderAsSection3" id="Sec10">
              <h4 class="Heading">Raw Devices and Custom Drivers</h4>
              <div class="Para" id="Par67">Some database vendors require direct access to storage devices—without needing a filesystem to <span id="ITerm33">exist</span>. Such direct access is often referred to as creating a “raw” device, which refers to the fact that the operating system won’t know how to manage it, and any I/O is handled directly by the database. Issuing I/O directly to the underlying storage device may provide a performance boost to the database. However, it is important to understand some of this approach’s drawbacks, which may not be important for your specific deployment.<div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent">
                    <p class="Para" id="Par68"><strong class="EmphasisTypeBold ">Error prone</strong>: Directly issuing I/O to a disk rather than through a filesystem is error prone. While it will provide a performance gain, incorrect handling of the underlying storage could result in data corruption, data loss, or unexpected bugs.</p>
                  </div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent">
                    <p class="Para" id="Par69"><strong class="EmphasisTypeBold ">Complex</strong>: Raw devices are not as common as one might expect. In fact, very few databases decided to implement that approach. It’s important to note that since raw devices aren’t typically mounted as regular filesystems, their manageability will be fully dependent on what your vendor provides.</p>
                  </div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent">
                    <p class="Para" id="Par70"><strong class="EmphasisTypeBold ">Lock-in</strong>: Once you are using a raw device, it’s extremely difficult to move away from it. You can’t mount raw devices or query their storage consumption via typical operating system mechanisms. All of your disks need to be arranged in a certain way, and you can’t easily go back to a regular filesystem.</p>
                  </div><div class="ClearBoth"> </div></li></ol></div></div>
            </section>

            <section class="Section3 RenderAsSection3" id="Sec11">
              <h4 class="Heading">Maintaining Disk Performance Over Time</h4>
              <p class="Para" id="Par71"><span id="ITerm34">Databases</span> are very storage I/O intensive, so disks <em class="EmphasisTypeItalic ">will</em> wear out over time. Most disk vendors provide estimates concerning the performance durability of their products. Check on those and compare.</p>
              <p class="Para" id="Par72">There are multiple tools and programs that can help with SSD performance over time. One example is the <span class="EmphasisFontCategoryNonProportional ">fstrim</span> program, which is frequently run weekly to discard unused filesystem blocks. <span class="EmphasisFontCategoryNonProportional ">fstrim</span> is an operating system background process that doesn’t require any database action and may improve <span id="ITerm35">I/O</span> to a significant extent.</p>
              <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar1">
                <div class="Heading">Tip</div>
                <p class="Para FirstParaInFormalPara" id="Par73">If you have to choose one place to invest—on CPU, storage, memory, or networking—we recommend splurging on storage. Everything else has evolved faster and better than storage. It still remains the slowest component in most systems.</p>
              </div>
            </section>

            <section class="Section3 RenderAsSection3" id="Sec12">
              <h4 class="Heading">Tiered Storage</h4>
              <p class="Para" id="Par74">Many use cases have different latency requirements for different sets of data. Similarly, industries may see exponential storage utilization growth over time. It is not always desirable, or even possible, to get rid of old data (for example, due to compliance regulations, third-party contracts, or simply because it still carries relevance for the business).</p>
              <p class="Para" id="Par75">Teams with storage-heavy use cases often seek ways to minimize the costs of storage consumption: by reducing the replication factor of their dataset, using less performant (although cheaper) storage disks, or by employing a manual data rotation process from faster to slower disks.</p>
              <p class="Para" id="Par76"><span id="ITerm36">Tiered storage</span><span id="ITerm37"/> is a solution implemented by some databases in order to address most of these concerns. It allows users to configure the database to use distinct storage tiers, and to define which criteria the database should use to ensure that the data is correctly replicated to its relevant tier. For example, MongoDB allows you to determine how data is replicated to a specific storage tier by assigning different tier tags to shards, allowing its balancer to migrate data between tiers automatically. On top of that, Atlas Online Archive also allows the database to offload historical datasets to cloud storage.</p>
            </section>

          </section>

          <section class="Section2 RenderAsSection2" id="Sec13">
            <h3 class="Heading">CPUs (Cores)</h3>
            <p class="Para" id="Par77">Next is the <span id="ITerm38">CPU</span>. As of this writing, you are probably looking at modern servers running some reasonably modern Intel, AMD, or ARM chips, which are commonly found across most cloud providers and enterprise hardware vendors. Along with storage, CPUs are another compute resource which—if not correctly sized—may introduce contention to your workload and impact your latencies. Clusters handling hundreds of thousands up to millions of operations per second tend to get very high CPU loads.</p>
            <p class="Para" id="Par78">More cores will generally mean better performance. This is important for achieving optimal performance from databases that are architected to benefit from multithreading, and it’s absolutely essential for databases that are architected with a shard-per-core architecture—running a separate shard on each core in each server. In this case, the more cores the CPU has, the more shards—and the better data distribution—the database will have.</p>
            <p class="Para" id="Par79">A combination of vendor recommendations and benchmarking (see Chapter <span class="ExternalRef"><a href="541783_1_En_9_Chapter.xhtml"><span class="RefSource">9</span></a></span>) can help you determine how much throughput each multicore chip can support. A general recommendation is to avoid running production systems close to the <span id="ITerm39">CPU</span> limits and find the sweet spot between supporting your expected performance and leaving room for throughput growth. On top of that, when doing benchmarking, remember to also factor in background database operations that might be detrimental to your performance. For example, Cassandra and Cassandra-compatible databases often need to run repair: a weekly process to ensure data consistency across the cluster. This process requires a lot of coordination and communication across the entire cluster. If your workload is not properly sized to accommodate background database operations and other events (such as node failures), your latency may increase to a level that surprises you.</p>
            <p class="Para" id="Par80">When using virtual machines, containers, or the public cloud, remember that each virtual CPU is mapped to a single logical core, or thread. In many cloud deployments, nodes are provided on a vCPU basis. The vCPU is typically a single hyperthread from a dual hyperthread x86 physical core for Intel/AMD variants, or a single core for ARM chips.</p>
            <p class="Para" id="Par81">No matter what your deployment of choice involves, avoid overcommitting CPU resources if performance is a priority. Doing so will prevent other guests from stealing CPU time<sup><a epub:type="noteref" href="#Fn5" id="Fn5_source" role="doc-noteref">5</a></sup> from your database.</p>
          </section>

          <section class="Section2 RenderAsSection2" id="Sec14">
            <h3 class="Heading">Memory (RAM)</h3>
            <p class="Para" id="Par83">If you’re working with an in-memory <span id="ITerm40">database</span>, having enough memory to hold your entire dataset is an absolute must. But every database uses in-memory caching to some extent. For example, some databases require enough memory space for indexes to avoid expensive round-trips to storage disks. Others leverage an internal data cache to allow for lower latencies when retrieving recently used data, Cassandra and Cassandra-like databases implement memtables, and some databases allow you to control which tables are served entirely from memory. The more memory the database has at its disposal, the better you can take advantage of those <span id="ITerm41">mechanisms</span>. After all, even the fastest NVMe can’t come close to the speed of RAM access.</p>
            <p class="Para" id="Par84">In general, there is no blanket recommendation for “how much memory is enough” for a database. Different vendors have different requirements and different use cases also require different memory sizes. However, latency-sensitive use cases typically require high memory footprints in order to achieve high cache hit rates and serve low-latency read requests efficiently.</p>
            <p class="Para" id="Par85">For example, a use case with a higher payload size requires a larger memory footprint than one with a smaller payload size. Another interesting aspect to consider is how frequently the use case in question reads data that may be present in memory (hot data) as opposed to data that was never read (cold data). As mentioned in Chapter <span class="ExternalRef"><a href="541783_1_En_2_Chapter.xhtml"><span class="RefSource">2</span></a></span>, the latter can easily undermine your latencies.</p>
            <p class="Para" id="Par86">Without a sufficient disk-to-memory ratio, you will be hitting your storage far more than you probably want if you intend to keep your latencies low. The ideal ratio varies from database to database since every caching implementation is different, so be sure to ask your vendor for their specific recommendations. For example, ScyllaDB currently recommends that for every 1GB of memory allocated to a node, you can store up to 100GB of data (so if you have 32GB of memory, you can handle around 3TB). The higher your memory-to-storage ratio gets, the less room you have for caching your total dataset. Every database has some sort of hard physical limit. If you don’t have enough memory and you have to run a workload on top of a very large dataset, it’s either going to be rather slow or increase the risk of the database running out of memory.</p>
            <p class="Para" id="Par87">Another ratio to keep in mind: memory per CPU core. At ScyllaDB, we recommend at least 8GB of memory per CPU core for production purposes (because, given our shared-nothing architecture, every shard works independently and has its own allocated memory for caching). 8GB per vCPU is the same ratio used by most cloud providers for NoSQL or Big Data-oriented instance types. Again, the recommended ratio will vary across vendors, depending on the database’s specific internal cache implementation and other implementation details. For example, in Cassandra and Cassandra-like databases, part of the memory will be allocated for some of its SSTable-components in order to speed up disk lookups when reading cold data. Aerospike will typically store all indexes in RAM. And MongoDB, on average, requires 1GB of RAM per 100K assets.</p>
            <p class="Para" id="Par88">Distributed databases are notoriously high memory consumers. Regardless of its implementation, the <span id="ITerm42">database</span> will always need to store some relevant parts of your dataset in memory in order to avoid wasting time on disk I/O. Insufficient memory can manifest itself as unpredictable, erratic database behavior—even crashes.</p>
          </section>

          <section class="Section2 RenderAsSection2" id="Sec15">
            <h3 class="Heading">Network</h3>
            <p class="Para" id="Par89">Lastly, you have to ensure that network I/O does not become a bottleneck. Networking is often an overlooked component. As with any distributed system, a database involves a lot of traffic between all the cluster <span id="ITerm43">members</span> to check for liveness, replicate state and topology changes, and so on. As a result, network delays not only deteriorate your application’s latency, but also prevent internode communication from functioning effectively.</p>
            <p class="Para" id="Par90">At ScyllaDB, we recommend a minimum network bandwidth of 10Gbps because internal database operations such as streaming, repairs, and gossip can become very network intensive. On top of that, you also need to factor in the actual throughput required for the use case in question; the number of operations per second will certainly be the highest bandwidth consumer for your deployment.</p>
            <p class="Para" id="Par91">As with memory, the required network bandwidth will vary. Be sure to check your vendor recommendations and consider the nature of your use case. A low throughput workload will obviously consume less traffic than a higher throughput one.</p>
            <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar2">
              <div class="Heading">Tip: Use CPU pinning to mitigate the impact of hardware interrupts.</div>
              <p class="Para FirstParaInFormalPara" id="Par92">Hardware interrupts, which typically stem from (but are not limited to) high network Internet traffic, force the OS kernel to stop everything and respond to the hardware before returning to the job at hand. Too many interrupts (e.g., a high softirq percent) will degrade database performance, as your CPUs may stall during processing for serving network traffic. One way to resolve this is to use CPU pinning. This tells the system that all network interrupts should be handled by specific CPUs that are not being used by the database. With that setup, you can blast the database with network traffic and be reasonably confident that you won’t overwhelm it or stall the database processing during normal operations.</p>
            </div>
            <p class="Para" id="Par93">For cloud deployments, most IaaS vendors provide a modern network infrastructure with ample bandwidth between your database servers and between the database and the application <span id="ITerm44">clients</span>. Be sure to check on your client’s network bandwidth consumption if you suspect network problems. A common mistake we see in deployments involves application clients deployed with suboptimal network capacity.</p>
            <p class="Para" id="Par94">Also, be sure to place your application servers as close as possible to your database. If you are deploying them in a single region, a shorter physical distance between the servers will translate to better network performance (since it will require fewer network hops for communication) and, as a result, lower latencies. If you need to go multi-region and you require strong consistency or replication across these regions, then you need to pay the latency penalty for traversing regions—plus, you also have to pay, quite literally, with respect to cross-region networking transfer fees. For multi-region deployments with cross-region replication, a slow network link may create replication delays that cause the database to apply backpressure on your writes until it manages to replicate the data piled up.</p>
          </section>

        </section>

        <section class="Section1 RenderAsSection1" id="Sec16">
          <h2 class="Heading">Considerations in the Cloud</h2>
          <p class="Para" id="Par95">The “on-prem vs cloud” decision depends heavily on your organization’s security and regulatory requirements as well as its business strategy—and is well beyond the scope of this book. Instead of heading down that path, let’s focus on exploring performance considerations that are unique to cloud deployments.</p>
          <p class="Para" id="Par96">Most <span id="ITerm45">cloud providers</span> offer a wide range of instance types that you may choose to host your workload. In our experience, most of the mistakes and performance bottlenecks seen on distributed databases within cloud deployments are due to an incorrect instance or storage type selection during the initial cluster setup. A common misunderstanding (and concern) that many people have is the fact that NVMe-based storage may be more expensive than network-attached storage. The misconception likely stems from the assumption that since <span id="ITerm46">NVMes</span> are faster, they would incur elevated costs. However it turns out to be quite the opposite: Since NVMe disks on cloud environments are tied to the lifecycle of an instance, they end up being cheaper than network disks, which require holding up your dataset for a prolonged period of time. We encourage you to compare the costs of NVMe backed-up storage against network-attached disks on your cloud vendor of choice.</p>
          <p class="Para" id="Par97">Some <span id="ITerm47">cloud vendors</span> have different instance types for different distributed database workloads. For example, some workloads may benefit more from compute-heavy instance types, with more compute power than storage capacity. Conversely, storage-dense instance types typically feature a higher storage to memory ratio and are often used by storage-heavy workloads.</p>
          <p class="Para" id="Par98">To complicate things even more, some cloud providers may offer different CPU generations for the same instance type. If one CPU generation is considerably slower than other nodes, the wrong choice could introduce performance bottlenecks into your cluster.</p>
          <p class="Para" id="Par99">We have seen some (although rare) scenarios where a noisy neighbor dragged down an entire node performance with no reasonable explanation. The lack of visibility and control in cloud instances makes it harder to diagnose such situations. Often, you need to reach out to your cloud vendor directly to resolve the situation.</p>
          <p class="Para" id="Par100">As you start configuring your instance, remember that a cloud environment isn’t created exclusively for databases. You have access to a wide range of options, but it can be confusing to determine where to start and which options to use. In general, it’s best to check with your database vendor on which instance types are recommended for deployment. Even better, go beyond that and compare the results of their benchmarks against those same instance types running your workload.</p>
          <p class="Para" id="Par101">After you have decided on your instance types and deployment options, it’s time to think about instance placement. Most clouds will charge you for both inter-region traffic and inter-zone traffic, which may quite surprisingly increase the overall networking costs. Some companies try to mitigate this cost by placing all instances under a single availability zone (AZ), which also carries the risk of potentially having to face a cluster-wide outage if/when that AZ goes down. Others opt to ignore the cost aspect and deploy their replicas in different AZs to ensure data is properly replicated to an isolated environment. Regardless of your instance’s placement of choice, note that some database drivers allow clients in specific AZs to route queries only against database replicas living in the same availability zone in order to reduce costs. Similarly, you will also want to ensure that your application clients are located under the same zones as your database to minimize your networking costs.</p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec17">
          <h2 class="Heading">Fully Managed Database-as-a-Service</h2>
          <div class="Para" id="Par102">Does the database-as-a-service <span id="ITerm48">model</span><span id="ITerm49"/> help or hurt database performance? It really depends on the following:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
              <p class="Para" id="Par103">How much attention your database requires to achieve and consistently meet your performance expectations</p>
            </li><li>
              <p class="Para" id="Par104">Your team’s experience working with the specific database you’re using</p>
            </li><li>
              <p class="Para" id="Par105">Your team’s time and desire to tinker with that database</p>
            </li><li>
              <p class="Para" id="Par106">The level of expertise—especially with respect to performance—that your DBaaS provider dedicates to your account</p>
            </li></ul></div></div>
          <p class="Para" id="Par107">Managed DBaaS solutions can easily speed up your go-to-market and allow you to focus on priorities beyond your database. Most database vendors now provide some sort of managed solution. There are even independent companies in the business of providing this kind of service for a variety of different distributed databases.</p>
          <div class="Para" id="Par108">We have seen many examples where a managed solution helped users succeed, as well as numerous complaints over the fact that some managed solutions were rather limited. It is not our intention to recommend nor criticize any specific service provider in question. Here is some vendor-agnostic advice on things to consider before selecting a managed solution:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
              <p class="Para" id="Par109">Does the vendor satisfy your existing security requirements? Does it provide enough evidence of security certifications issued by a known security company?</p>
            </li><li>
              <p class="Para" id="Par110">What are the options for observability and how do you export the data in question to your monitoring platform of choice?</p>
            </li><li>
              <p class="Para" id="Par111">What kind of flexibility do you have with your deployment? What are the available tunable options and the support for those within your managed solution?</p>
            </li><li>
              <p class="Para" id="Par112">Does it allow you to peer traffic from your existing application network(s) to your database in a private and secure way?</p>
            </li><li>
              <p class="Para" id="Par113">What are the available support options and SLAs?</p>
            </li><li>
              <p class="Para" id="Par114">Which deployment options are available, what’s the flexibility among switching, and what’s the cost comparison if you were to deploy and maintain it on your own?</p>
            </li><li>
              <p class="Para" id="Par115">How easy is it for you to export your data if you need to move your deployment to a different <span id="ITerm50">vendor</span> in the future?</p>
            </li><li>
              <p class="Para" id="Par116">What, if any, migration options are available and what amount of effort do they require?</p>
            </li></ul></div></div>
          <p class="Para" id="Par117">These are just some of the many questions and concerns that we’ve frequently heard teams asking (or wishing they asked before they got caught in an undesirable option). Considering a third-party vendor to manage a relatively critical aspect of your infrastructure is very often challenging. However, under the right circumstances and vendor-user fit, it can be a great option for reducing your admin burden and optimizing your performance.</p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec18">
          <h2 class="Heading">Serverless Deployment Models</h2>
          <p class="Para" id="Par118"><span id="ITerm51"><em class="EmphasisTypeItalic ">Serverless</em></span><span id="ITerm52"/> refers to database solutions that offer near-instant scaling up or scaling down of database infrastructure—and charge you for the capacity and storage that you actually consume.</p>
          <div class="Para" id="Par119">A serverless model could theoretically yield a performance advantage. Before serverless, many organizations faced a tradeoff:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
              <p class="Para" id="Par120">(Slightly or generously, depending on your risk tolerance) overestimate the capacity they need to guarantee adequate performance.</p>
            </li><li>
              <p class="Para" id="Par121">Watch performance suffer if their overly-conservative capacity estimates proved inadequate.</p>
            </li></ul></div></div>
          <p class="Para" id="Par122">Serverless can help in a few different ways and situations.</p>
          <p class="Para" id="Par123">First, with variable workloads. Since the database can rapidly scale up as your workload increases, you can worry less about performance issues stemming from inadequate capacity. If your traffic ebbs and flows across the day/week/month, you can spend less during the light periods and dedicate those resources to supporting the peak periods. And if your company suddenly experiences “catastrophic <span id="ITerm53">success</span>,” you don’t have to worry about the headaches associated with needing to suddenly scale your infrastructure. If all goes well, the vendor will “automagically” ensure that you’re covered, with acceptable performance. You won’t need to procure any additional servers, or even contact your cloud provider.</p>
          <p class="Para" id="Par124">Serverless is also a great option to consider if you’re working on a new project and are not sure what capacity you need to meet performance expectations. It gives you the freedom to start fast and scale (or shrink) depending on real-world usage. Database sizing is one less thing to worry about. And you don’t need to predict the future.</p>
          <p class="Para" id="Par125">Finally, serverless also makes it simpler to justify the spend internally. With this model, you can assure your organization that you are never overprovisioned—at least not for long. You’re paying for exactly the amount of performance that the database vendor determines you need at all times.</p>
          <p class="Para" id="Par126">However, a serverless deployment also carries the risk of cost overruns and the uncertainty of unpredictable costs. For example, DynamoDB pricing may not be very attractive for write-heavy workloads. Similarly, serverless database services may charge an arm and a leg (or an eye and a knee) depending on the number of operations per second you plan to sustain over an extended period of time. In some cases, it could become a double-edged sword from a cost perspective if your goal is to sustain a high-throughput performant system at large scale.</p>
          <p class="Para" id="Par127">Another aspect to consider when thinking about a serverless solution is whether the solution in question is compatible with your existing infrastructure components. For example, you’ll want to explore what amount of effort is required to connect your message queueing or analytics tool with that specific serverless solution.</p>
          <p class="Para" id="Par128">Remember that the overall concept behind serverless is to abstract away the underlying infrastructure in such a way that not all database-configurable options are available to you. As a result, troubleshooting potential performance problems is often more challenging since you might need to rely on your vendor’s input and guidance to understand which actions to take. Being serverless also means that you lack visibility into whether the <span id="ITerm54">infrastructure</span> you consume is shared with other tenants. Many distributed database vendors may also offer you different pricing tiers for shared and dedicated environments.</p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec19">
          <h2 class="Heading">Containerization and Kubernetes</h2>
          <p class="Para" id="Par129">Containers and <span id="ITerm55">Kubernetes</span><span id="ITerm56"/> are now ubiquitous, even for stateful systems like databases. Should you use them? Probably—unless you have a good reason not to.</p>
          <p class="Para" id="Par130">But be aware that there is a performance penalty for the operational convenience of using containers. This is to be expected because of the extra layer of abstraction (the container itself), relaxation of resource isolation, and increased context switches. The good news is that it can certainly be overcome. In our testing using ScyllaDB, we found it is possible to take what was originally a 69 percent reduction in peak throughput down to a 3 percent performance penalty.<sup><a epub:type="noteref" href="#Fn6" id="Fn6_source" role="doc-noteref">6</a></sup></p>
          <div class="Para" id="Par132">Here’s the TL;DR on that specific experiment:<div class="UnorderedList"><ul class="UnorderedListMarkNone"><li>
              <p class="Para ParaOneEmphasisChild" id="Par133"><em class="EmphasisTypeItalic ">Containerizing applications is not free. In particular, processes comprising the containers have to be run in Linux cgroups and the container receives a virtualized view of the network. Still, the biggest cost of running a close-to-hardware, thread-per-core application like ScyllaDB inside a Docker container comes from the opportunity cost of having to disable most of the performance optimizations that the database employs in VM and bare-metal environments to enable it to run in potentially shared and overcommitted platforms.</em></p>
            </li><li>
              <p class="Para ParaOneEmphasisChild" id="Par134"><em class="EmphasisTypeItalic ">The best results with Docker are obtained when resources are statically partitioned and we can bring back bare-metal optimizations like CPU pinning and interrupt isolation. There is only a 10 percent performance penalty in this case as compared to the underlying platform—a penalty that is mostly attributed to the network virtualization. Docker allows users to expose the host network directly for specialized deployments. In cases in which this is possible, we saw that the performance difference compared to the underlying platform falls down to 3 percent.</em></p>
            </li></ul></div></div>
          <div class="Para" id="Par135">Of course, the potential penalty and strategies for mitigating will vary from database to database. But the key takeaway is that there is likely a significant performance penalty—so be sure to hunt it down and research how to mitigate it. Some common mitigation strategies <span id="ITerm57">include</span>:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
              <p class="Para" id="Par136">Ensure that your containers have direct access to the database’s underlying storage.</p>
            </li><li>
              <p class="Para" id="Par137">Expose the host OS network to the container in order to avoid the performance penalty due to its network virtualization layer.</p>
            </li><li>
              <p class="Para" id="Par138">Allocate enough resources to the container in question, and ensure these are not overcommitted with other containers or processes running within the underlying host OS.</p>
            </li></ul></div></div>
          <div class="Para" id="Par139">Kubernetes adds yet another virtualization layer—and thus opens the door to yet another layer of performance issues, as well as different strategies for mitigating them. First off, if you have the choice of multiple options for deploying and managing database clusters on Kubernetes, test them out with an eye on performance. Once you settle on the best fit for your needs, dive into the configuration options that could impact performance. Here are some performance tips that cross databases:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
              <p class="Para" id="Par140">Consider dedicating specific and independent Kubernetes nodes for your database workloads and use affinities in order to configure their placement.</p>
            </li><li>
              <p class="Para" id="Par141">Enable <span class="EmphasisFontCategoryNonProportional ">hostNetworking</span> and be sure to set up the required kernel parameters as recommended by your vendor (for example, <span class="EmphasisFontCategoryNonProportional ">fs.aio-max-nr</span> for increasing the number of events available for asynchronous I/O processing in the Linux kernel).</p>
            </li><li>
              <p class="Para" id="Par142">Ensure that your database pods have a Guaranteed QoS class<sup><a epub:type="noteref" href="#Fn7" id="Fn7_source" role="doc-noteref">7</a></sup> to avoid other pods from potentially hurting your main workload.</p>
            </li><li>
              <p class="Para" id="Par144">Be sure to use an operator<sup><a epub:type="noteref" href="#Fn8" id="Fn8_source" role="doc-noteref">8</a></sup> in order to orchestrate and control the lifecycle of your existing Kubernetes database cluster. For example, <span id="ITerm58">ScyllaDB</span><span id="ITerm59"/> has its ScyllaDB Operator project.</p>
            </li></ul></div></div>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec20">
          <h2 class="Heading">Summary</h2>
          <p class="Para" id="Par146">This chapter kicked off the final part of this book, focused on sharing recommendations for getting better performance out of your database deployment. It looked at infrastructure and deployment model considerations that are important to understand whether you’re managing your own deployment or opting for a <span id="ITerm60">database-as-a-service</span> (maybe serverless) deployment model. The next chapter looks at performance considerations relevant to the topology itself: replication, geographic distribution, scaling up and/or out, and intermediaries like external caches, load balancers, and abstraction layers.</p>
        </section>

      <div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a>
            <p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
            <p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
          </div><aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1" role="doc-footnote"><p class="Para" id="Par44">For inspiration, consider Discord’s approach—but recognize that this is certainly not a one-size-fits-all solution. It’s described in their blog, “How Discord Supercharges Network Disks for Extreme Low Latency” (<span class="ExternalRef"><a href="https://discord.com/blog/how-discord-supercharges-network-disks-for-extreme-low-latency"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://discord.com/blog/how-discord-supercharges-network-disks-for-extreme-low-latency</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2" role="doc-footnote"><p class="Para" id="Par47">You can find the results, as well as the tool to reproduce the results, at <span class="ExternalRef"><a href="https://github.com/scylladb/diskplorer#sample-results"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://github.com/scylladb/diskplorer#sample-results</span></span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn3" role="doc-footnote"><p class="Para" id="Par48">Strangely, the 95th percentile at low rates is worse than at high rates.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn4" role="doc-footnote"><p class="Para" id="Par49">Note the throughput and IOPS were allowed to miss by a 15 percent margin rather than the normal 3 percent margin.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn5_source">5</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn5" role="doc-footnote"><p class="Para" id="Par82">For more on CPU steal time, see “Detecting CPU Steal Time in Guest Virtual Machines” by Jamie Fargen (<span class="ExternalRef"><a href="https://opensource.com/article/20/1/cpu-steal-time"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://opensource.com/article/20/1/cpu-steal-time</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn6_source">6</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn6" role="doc-footnote"><p class="Para" id="Par131">See “The Cost of Containerization for Your ScyllaDB” on the ScyllaDB blog (<span class="ExternalRef"><a href="https://www.scylladb.com/2018/08/09/cost-containerization-scylla/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://www.scylladb.com/2018/08/09/cost-containerization-scylla/</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn7_source">7</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn7" role="doc-footnote"><p class="Para" id="Par143">For more detail, see “Create a Pod that Gets Assigned a QoS Class of Guaranteed” in the Kubernetes docs (<span class="ExternalRef"><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn8_source">8</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn8" role="doc-footnote"><p class="Para" id="Par145">For more detail, see “Operator Pattern” in the Kubernetes docs <span class="ExternalRef"><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://kubernetes.io/docs/concepts/extend-kubernetes/operator/</span></span></a></span>.</p></div><div class="ClearBoth"> </div></div></aside></div></div></body></html>