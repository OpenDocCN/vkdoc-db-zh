<html><head></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-1-4842-9711-7_2"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">F. C. Mendes et al.</span><span class="ContextInformationBookTitles"><span class="BookTitle">Database Performance at Scale</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-1-4842-9711-7_2">https://doi.org/10.1007/978-1-4842-9711-7_2</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">2. Your Project, Through the Lens of Database Performance</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Felipe Cardeneti Mendes</span><sup><a href="#Aff5">1</a> <span class="ContactIcon"> </span></sup>, </span><span class="Author"><span class="AuthorName">Piotr Sarna</span><sup><a href="#Aff6">2</a></sup>, </span><span class="Author"><span class="AuthorName">Pavel Emelyanov</span><sup><a href="#Aff7">3</a></sup> and </span><span class="Author"><span class="AuthorName">Cynthia Dunlop</span><sup><a href="#Aff8">4</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">São Paulo, São Paulo, Brazil</div></div><div class="Affiliation" id="Aff6"><span class="AffiliationNumber">(2)</span><div class="AffiliationText">Pruszków, Poland</div></div><div class="Affiliation" id="Aff7"><span class="AffiliationNumber">(3)</span><div class="AffiliationText">Moscow, Russia</div></div><div class="Affiliation" id="Aff8"><span class="AffiliationNumber">(4)</span><div class="AffiliationText">Carpinteria, CA, USA</div></div><div class="ClearBoth"> </div></div></div><div class="ArticleOrChapterToc"><div class="TocLine"><a href="#Sec1">Workload Mix (Read/Write Ratio)</a></div><div class="TocLine"><a href="#Sec7">Item Size</a></div><div class="TocLine"><a href="#Sec8">Item Type</a></div><div class="TocLine"><a href="#Sec9">Dataset Size</a></div><div class="TocLine"><a href="#Sec10">Throughput Expectations</a></div><div class="TocLine"><a href="#Sec11">Latency Expectations</a></div><div class="TocLine"><a href="#Sec12">Concurrency</a></div><div class="TocLine"><a href="#Sec13">Connected Technologies</a></div><div class="TocLine"><a href="#Sec14">Demand Fluctuations</a></div><div class="TocLine"><a href="#Sec15">ACID Transactions</a></div><div class="TocLine"><a href="#Sec16">Consistency Expectations</a></div><div class="TocLine"><a href="#Sec17">Geographic Distribution</a></div><div class="TocLine"><a href="#Sec18">High-Availability Expectations</a></div><div class="TocLine"><a href="#Sec19">Summary</a></div></div><!--End Abstract--><div class="Fulltext">
        <p class="Para" id="Par2">The specific database performance constraints and optimization opportunities your team will face vary wildly based on your specific workload, application, and business expectations. This chapter is designed to get you and your team talking about how much you can feasibly optimize your performance, spotlight some specific lessons related to common situations, and also help you set realistic expectations if you’re saddled with burdens like large payload sizes and strict consistency requirements. The chapter starts by looking at technical factors, such as the read/write <span id="ITerm1">ratio</span> of your workload, item size/type, and so on. Then, it shifts over to business considerations like <span id="ITerm2">consistency requirements</span> and high availability expectations. Throughout, the chapter talks about database attributes that have proven to be helpful—or limiting—in different contexts.</p>
        <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar1">
          <div class="Heading">Note</div>
          <p class="Para FirstParaInFormalPara" id="Par3">Since this chapter covers a broad range of scenarios, not everything will be applicable to your specific project and workload. Feel free to skim this chapter and focus on the sections that seem most relevant.</p>
        </div>
        <section class="Section1 RenderAsSection1" id="Sec1">
          <h2 class="Heading">Workload Mix (Read/Write Ratio)</h2>
          <p class="Para" id="Par4">Whether it’s read-heavy, write-heavy, evenly-mixed, delete-heavy, and so on, understanding and accommodating your read/write ratio is a critical but commonly overlooked aspect of database performance. Some databases shine with read-<span id="ITerm3">heavy workloads</span>, others are optimized for write-heavy situations, and some are built to accommodate both. Selecting, or sticking with, one that’s a poor fit for your current and future situation will be a significant burden that will be difficult to overcome, no matter how strategically you optimize everything else.</p>
          <p class="Para" id="Par5">There’s also a significant impact to cost. That might not seem directly related to performance, but if you can’t afford (or get approval for) the infrastructure that you truly need to support your workload, this will clearly limit your performance.<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source" role="doc-noteref">1</a></sup></p>
          <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar2">
            <div class="Heading">Tip</div>
            <p class="Para FirstParaInFormalPara" id="Par7">Not sure what your workload looks like? This is one of many situations where observability is your friend. If your existing database doesn’t help you profile your workload, consider if it’s feasible to try your workloads on a compatible database that enables deeper visibility.</p>
          </div>
          <section class="Section2 RenderAsSection2" id="Sec2">
            <h3 class="Heading">Write-Heavy Workloads</h3>
            <p class="Para" id="Par8">If you have a <span id="ITerm4">write-heavy workload</span>, we strongly recommend a database that stores data in immutable files (e.g., Cassandra, ScyllaDB, and others that use LSM trees).<sup><a epub:type="noteref" href="#Fn2" id="Fn2_source" role="doc-noteref">2</a></sup> These databases optimize write speed because: 1) writes are sequential, which is faster in terms of disk I/O and 2) writes are performed immediately, without first worrying about reading or updating existing values (like databases that rely on B-trees do). As a result, you can typically write a lot of data with very low latencies.</p>
            <p class="Para" id="Par10">However, if you opt for a write-optimized database, be prepared for higher storage requirements and the potential for slower reads. When you work with immutable files, you’ll need sufficient storage to keep all the immutable files that build up until compaction runs.<sup><a epub:type="noteref" href="#Fn3" id="Fn3_source" role="doc-noteref">3</a></sup> You can mitigate the storage needs to some extent by choosing compaction strategies carefully. Plus, storage is relatively inexpensive these days.</p>
            <p class="Para" id="Par12">The potential for read amplification is generally a more significant concern with write-optimized databases (given all the files to search through, more disk reads are required per read request).</p>
            <p class="Para" id="Par13">But read performance doesn’t necessarily need to suffer. You can often minimize this tradeoff with a write-optimized database that implements its own caching subsystem (as opposed to those that rely on the operating system’s built-in cache), enabling fast reads to coexist alongside extremely fast writes. Bypassing the underlying OS with a performance-focused built-in cache should speed up your reads nicely, to the point where the latencies are nearly comparable to read-optimized databases.</p>
            <p class="Para" id="Par14">With a <span id="ITerm5">write-heavy workload</span>, it’s also essential to have extremely fast storage, such as NVMe drives, if your peak throughput is high. Having a database that can <em class="EmphasisTypeItalic ">theoretically</em> store values rapidly ultimately won’t help if the disk itself can’t keep pace.</p>
            <p class="Para" id="Par15">Another consideration: beware that write-heavy workloads can result in surprisingly high costs as you scale. Writes cost around five times more than reads under some vendors’ pricing models. Before you invest too much effort in performance optimizations, and so on, it’s a good idea to price your solution at scale and make sure it’s a good long-term fit.</p>
          </section>

          <section class="Section2 RenderAsSection2" id="Sec3">
            <h3 class="Heading">Read-Heavy Workloads</h3>
            <p class="Para" id="Par16">With <span id="ITerm6">read-heavy workloads</span>, things change a bit. B-tree databases (such as DynamoDB) are optimized for reads (that’s the payoff for the extra time required to update values on the write path). However, the advantage that read-optimized databases offer for reads is generally not as significant as the advantage that write-optimized databases offer for writes, especially if the write-optimized database uses internal caching to make up the difference (as noted in the previous section).</p>
            <div class="Para" id="Par17">Careful data modeling will pay off in spades for optimizing your reads. So will careful selection of read consistency (are eventually consistent reads acceptable as opposed to strongly consistent ones?), locating your database near your application, and performing a thorough analysis of your query access patterns. Thinking about your access patterns is especially crucial for success with a read-heavy workload. Consider aspects such as the following:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
                <p class="Para" id="Par18">What is the nature of the data that the application will be querying mostly frequently? Does it tolerate potentially stale reads or does it require immediate <span id="ITerm7">consistency</span>?</p>
              </li><li>
                <p class="Para" id="Par19">How frequently is it accessed (e.g., is it frequently-accessed “hot” data that is likely cached, or is it rarely-accessed “cold” data)?</p>
              </li><li>
                <p class="Para" id="Par20">Does it require aggregations, JOINs, and/or querying flexibility on fields that are not part of your primary key component?</p>
              </li><li>
                <p class="Para" id="Par21">Speaking of primary keys, what is the level of cardinality?</p>
              </li></ul></div></div>
            <p class="Para" id="Par22">For example, assume that your use case requires dynamic querying capabilities (such as type-ahead use cases, report-building solutions, etc.) where you frequently need to query data from columns other than your primary/hash key component. In this case, you might find yourself performing full table scans all too frequently, or relying on too many indexes. Both of these, in one way or another, may eventually undermine your read performance.</p>
            <p class="Para" id="Par23">On the infrastructure side, selecting servers with high memory footprints is key for enabling low read latencies if you will mostly serve data that is frequently accessed. On the other hand, if your <span id="ITerm8">reads</span> mostly hit cold data, you will want a nice balance between your storage speeds and memory. In fact, many distributed databases typically reserve some memory space specifically for caching indexes; this way, reads that inevitably require going to disk won’t waste I/O by scanning through irrelevant data.</p>
            <div class="Para" id="Par24">What if the use case requires reading from both hot and cold data at the same time? And what if you have different latency requirements for each set of data? Or what if you want to mix a real-time workload on top of your analytics workload for the very same dataset? Situations like this are quite common. There’s no one-size-fits-all answer, but here are a few important tips:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
                <p class="Para" id="Par25">Some databases will allow you to read data without polluting your cache (e.g., filling it up with data that is unlikely to be requested again). Using such a mechanism is especially important when you’re running large scans while simultaneously serving real-time data. If the large scans were allowed to override the previously cached entries that the real-time workload required, those reads would have to go through disk and get <span id="ITerm9">repopulated</span> into the cache again. This would effectively waste precious processing time and result in elevated latencies.</p>
              </li><li>
                <p class="Para" id="Par26">For use cases requiring a distinction between hot/cold data storage (for cost savings, different latency requirements, or both), then solutions using <em class="EmphasisTypeItalic ">tiered storage</em> (a method of prioritizing data storage based on a range of requirements, such as performance and costs) are likely a good fit.</p>
              </li><li>
                <p class="Para" id="Par27">Some databases will permit you to prioritize some workloads over others. If that’s not sufficient, you can go one step further and completely isolate such workloads logically.<sup><a epub:type="noteref" href="#Fn4" id="Fn4_source" role="doc-noteref">4</a></sup></p>
              </li></ul></div></div>
            <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar3">
              <div class="Heading">Note</div>
              <p class="Para FirstParaInFormalPara" id="Par29">You might not need all your reads. At ScyllaDB, we’ve come across a number of cases where teams are performing reads that they don’t really need. For example, by using a read-before-write approach to avoid race conditions where multiple clients are trying to update the same value with different updates at the same time. The details of the solution aren’t relevant here, but it is important to note that, by rethinking their approach, they were able to shave latencies off their writes as well as speed up the overall response by eliminating the unnecessary read. The moral here: Getting new eyes on your existing approaches might surface a way to unlock unexpected performance optimizations.</p>
            </div>
          </section>

          <section class="Section2 RenderAsSection2" id="Sec4">
            <h3 class="Heading">Mixed Workloads</h3>
            <p class="Para" id="Par30">More evenly <span id="ITerm10">mixed access patterns</span> are generally even more complex to analyze and accommodate. In general, the reason that mixed workloads are so complex in nature is due to the fact that there are two competing workloads from the database perspective. Databases are essentially made for just two things: reading and writing. The way that different databases handle a variety of competing workloads is what truly differentiates one solution from another. As you test and compare databases, experiment with different read/write ratios so you can adequately prepare yourself for scenarios when your access patterns may change.</p>
            <p class="Para" id="Par31">Be sure to consider nuances like whether your reads are from cold data (data not often accessed) or hot data (data that’s accessed often and likely cached). Analytics use cases tend to read cold data frequently because they need to process large amounts of data. In this case, disk speeds are very important for overall performance. Plus, you’ll want a comfortably large amount of memory so that the database’s cache can hold the data that you need to process. On the other hand, if you frequently access hot data, most of your data will be served from the cache, in such a way that the disk speeds become less important (although not negligible).</p>
            <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar4">
              <div class="Heading">Tip</div>
              <p class="Para FirstParaInFormalPara" id="Par32">Not sure if your reads are from cold or hot data? Take a look at the ratio of cache misses in your monitoring dashboards. For more on monitoring, see Chapter <span class="ExternalRef"><a href="541783_1_En_10_Chapter.xhtml"><span class="RefSource">10</span></a></span>.</p>
            </div>
            <p class="Para" id="Par33">If your ratio of cache misses is higher than hits, this means that reads need to frequently hit the disks in order to look up your data. This may happen because your database is underprovisioned in memory space, or simply because the application access patterns often read infrequently accessed data. It is important to understand the performance implications here. If you’re frequently reading from cold data, there’s a risk that I/O will become the bottleneck—for writes as well as reads. In that case, if you need to improve performance, adding more nodes or switching your storage medium to a faster solution could be helpful.</p>
            <p class="Para" id="Par34">As noted earlier, write-optimized databases can improve read latency via internal <span id="ITerm11">caching</span>, so it’s not uncommon for a team with, say, 60 percent reads and 40 percent writes to opt for a write-optimized database. Another option is to boost the latency of reads with a write-optimized database: If your database supports it, dedicate extra “shares” of resources to the reads so that your read workload is prioritized when there is resource contention.</p>
          </section>

          <section class="Section2 RenderAsSection2" id="Sec5">
            <h3 class="Heading">Delete-Heavy Workloads</h3>
            <p class="Para" id="Par35">What about <span id="ITerm12">delete-heavy workloads</span>, such as using your database as a durable queue (saving data from a producer until the consumer accesses it, deleting it, then starting the cycle over and over again)? Here, you generally want to avoid databases that store data in immutable files and use tombstones to mark rows and columns that are slated for deletion. The most notable examples are Cassandra and other Cassandra-compatible databases.</p>
            <p class="Para" id="Par36">Tombstones consume cache space and disk resources, and the database needs to search through all these tombstones to reach the live data. For many workloads, this is not a problem. But for delete-heavy workloads, generating an excessive amount of tombstones will, over time, significantly degrade your read latencies. There are ways and mechanisms to mitigate the impact of tombstones.<sup><a epub:type="noteref" href="#Fn5" id="Fn5_source" role="doc-noteref">5</a></sup> However, in general, if you have a delete-heavy workload, it may be best to use a different database.</p>
            <p class="Para" id="Par38">It is important to note that occasional deletes are generally fine on Cassandra and Cassandra-compatible databases. Just be aware of the fact that deletes on append-only databases result in tombstone writes. As a result, these may incur read amplification, elevating your read latencies. Tombstones and data eviction in these types of databases are potentially long and complex subjects that perhaps could have their own dedicated chapter. However, the high-level recommendation is to exercise caution if you have a potentially delete-heavy pattern that you might later read from, and be sure to combine it with a compaction strategy tailored for efficient data eviction.</p>
            <p class="Para" id="Par39">All that being said, it is interesting to note that some teams have successfully implemented delete-heavy workloads on top of Cassandra and Cassandra-like databases. The performance overhead carried by tombstones is generally circumvented by a combination of data modeling, a careful study of how deletes are performed, avoiding reads that potentially scan through a large set of deleted data, and careful tuning over the underlying <span id="ITerm13">table’s compaction strategy</span> to ensure that tombstones get evicted in a timely manner. For example, Tencent Games used the Time Window Compaction Strategy to aggressively expire tombstones and use it as the foundation for a time series distributed queue.<sup><a epub:type="noteref" href="#Fn6" id="Fn6_source" role="doc-noteref">6</a></sup></p>
          </section>

          <section class="Section2 RenderAsSection2" id="Sec6">
            <h3 class="Heading">Competing Workloads (Real-Time vs Batch)</h3>
            <div class="Para" id="Par41">If you’re working with two different types of workloads—one more latency-sensitive than the other—the ideal solution is to have the database dedicate more resources to the more latency-sensitive workloads to keep them from faltering due to insufficient resources. This is commonly the case when you are attempting to balance OLTP (real-time) <span id="ITerm14">workloads</span>, which are user-facing and require low latency responses, with OLAP (analytical) workloads, which can be run in batch mode and are more focused on throughput (see Figure <span class="InternalRef"><a href="#Fig1">2-1</a></span>). Or, you can prioritize analytics. Both are technically feasible; it just boils down to what’s most important for your use case.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1">
                <img alt="" aria-describedby="d65e774" src="../images/541783_1_En_2_Chapter/541783_1_En_2_Fig1_HTML.jpg" style="width:42.98em"/><div class="TextObject" id="d65e774">
                  <p class="Para" id="Par138">A block diagram of O L T P and OLAP. A database cluster sends and receives data to and from an O L T P client and an O L A P client.</p>
                </div>
                
              </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 2-1</span>
                  <p class="SimplePara">OLTP vs OLAP <span id="ITerm15">workloads</span></p>
                </div></figcaption></figure></div>
            <div class="Para" id="Par42">For example, assume you have a web server database with analytics. It must support two workloads:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
                <p class="Para" id="Par43">The main workload consists of queries triggered by a user clicking or navigating on some areas of the web page. Here, users expect high responsiveness, which usually translates to requirements for low latency. You need low timeouts with load shedding as your overload response, and you would like to have a lot of dedicated resources available whenever this workload needs them.</p>
              </li><li>
                <p class="Para" id="Par44">A second workload drives analytics being run periodically to collect some statistics or to aggregate some information that should be presented to users. This involves a series of computations. It’s a lot less sensitive to latency than the main workload; it’s more throughput oriented. You can have fairly large timeouts to accommodate for always full queues. You would like to throttle requests under load so the computation is stable and controllable. And finally, you would like the workload to have very few dedicated resources and use mostly unused resources to achieve better cluster utilization.</p>
              </li></ul></div></div>
            <p class="Para" id="Par45">Running on the same cluster, such workloads would be competing for resources. As system utilization <span id="ITerm16">rises</span>, the database must strictly prioritize which activities get what specific share of resources under contention. There are a few different ways you can handle this. Physical isolation, logical isolation, and scheduled isolation can all be acceptable choices under the right circumstances. Chapter <span class="ExternalRef"><a href="541783_1_En_8_Chapter.xhtml"><span class="RefSource">8</span></a></span> covers these options.</p>
          </section>

        </section>

        <section class="Section1 RenderAsSection1" id="Sec7">
          <h2 class="Heading">Item Size</h2>
          <p class="Para" id="Par46">The size of the <span id="ITerm17">items</span> you are storing in the database (average payload size) will dictate whether your workload is CPU bound or storage bound. For example, running 100K OPS with an average payload size of 90KB is much different than achieving the same throughput with a 1KB payload. Higher payloads require more processing, I/O, and network traffic than smaller payloads.</p>
          <p class="Para" id="Par47">Without getting too deep into database internals here, one notable impact is on the page cache. Assuming a default page cache size of 4KB, the database would have to serve several pages for the largest payload—that’s much more I/O to issue, process, merge, and serve back to the application clients. With the 1KB example, you could serve it from a single-page cache entry, which is less taxing from a compute resource perspective. Conversely, having a large number of smaller-sized items may introduce CPU overhead compared to having a smaller number of larger items because the database must process each arriving item individually.</p>
          <p class="Para" id="Par48">In general, the larger the payload gets, the more cache activity you will have. Most write-optimized databases will store your writes in memory before persisting that information to the disk (in fact, that’s one of the reasons why they are write-optimized). Larger payloads deplete the available cache space more <span id="ITerm18">frequently</span>, and this incurs a higher flushing activity to persist the information on disk in order to release space for more incoming writes. Therefore, more disk I/O is needed to persist that information. If you don’t size this properly, it can become a bottleneck throughout this repetitive process.</p>
          <p class="Para" id="Par49">When you’re working with extremely large payloads, it’s important to set realistic latency and throughput expectations. If you need to serve 200KB payloads, it’s unlikely that any database will enable you to achieve single-digit millisecond latencies. Even if the entire dataset is served from cache, there’s a physical barrier between your client and the database: networking. The network between them will eventually throttle your transfer speeds, even with an insanely fast client and database. Eventually, this will impact throughput as well as latency. As your latency increases, your client will eventually throttle down and you won’t be able to achieve the same throughput that you could with smaller payload sizes. The requests would be stalled, queuing in the network.<sup><a epub:type="noteref" href="#Fn7" id="Fn7_source" role="doc-noteref">7</a></sup></p>
          <p class="Para" id="Par51">Generally speaking, databases should not be used to store large blobs. We’ve seen people trying to store gigabytes of data within a single-key in a database—and this isn’t a great idea. If your item size is reaching this scale, consider alternative solutions. One solution is to use CDNs. Another is to store the largest chunk of your payload size in cold storage like Amazon S3 buckets, <span id="ITerm19">Google Cloud storage</span>, or Azure blob storage. Then, use the database as a metadata lookup: It can read the data and fetch an identifier that will help find the data in that cold storage. For example, this is the strategy used by a game developer converting extremely large (often in the gigabyte range) content to popular gaming platforms. They store structured objects with blobs that are referenced by a content hash. The largest payload is stored within a cloud vendor Object Storage solution, whereas the content hash is stored in a distributed NoSQL database.<sup><a epub:type="noteref" href="#Fn8" id="Fn8_source" role="doc-noteref">8</a></sup></p>
          <p class="Para" id="Par53">Note that some databases impose hard limits on item size. For example, <span id="ITerm20">DynamoDB</span><span id="ITerm21"/> currently has a maximum item size of 400KB. This might not suit your needs. On top of that, if you’re using an in-memory solution such as Redis, larger keys will quickly deplete your memory. In this case, it might make sense to hash/compress such large objects prior to storing them.</p>
          <p class="Para" id="Par54">No matter which database you choose, the smaller your payload, the greater your chances of introducing memory fragmentation. This might reduce your memory efficiency, which might in turn elevate costs because the database won’t be able to fully utilize its available memory.</p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec8">
          <h2 class="Heading">Item Type</h2>
          <p class="Para" id="Par55">The item type has a large impact on compression, which in turn impacts your storage utilization. If you’re frequently storing <span id="ITerm22">text</span>, expect to take advantage of a high compression ratio. But, that’s not the case for random and uncommon blob sequences. Here, compression is unlikely to make a measurable reduction in your storage footprint. If you’re concerned about your use case’s storage utilization, using a compression-friendly item type can make a big difference.</p>
          <p class="Para" id="Par56">If your use case dictates a certain item type, consider databases that are optimized for that type. For example, if you need to frequently process JSON data that you can’t easily transform, a document database like MongoDB might be a better option than a <span id="ITerm23">Cassandra-compatible database</span>. If you have JSON with some common fields and others that vary based on user input, it might be complicated—though possible—to model them in Cassandra. However, you’d incur a penalty from serialization/deserialization overhead required on the application side.</p>
          <p class="Para" id="Par57">As a general rule of thumb, choose the data type that’s the minimum needed to store the type of data you need. For example, you don’t need to store a year as a <span class="EmphasisFontCategoryNonProportional ">bigint</span>. If you define a field as a <span class="EmphasisFontCategoryNonProportional ">bigint</span>, most databases allocate relevant memory address spaces for holding it. If you can get by with a smaller type of <span class="EmphasisFontCategoryNonProportional ">int</span>, do it—you’ll save bytes of memory, which could add up at scale. Even if the database you use doesn’t pre-allocate memory address spaces according to data types, choosing the correct one is still a nice way to have an organized data model—and also to avoid future questions around why a particular data type was chosen as opposed to another.</p>
          <p class="Para" id="Par58">Many databases support additional item types which suit a variety of use cases. Collections, for example, allow you to store sets, lists, and maps (key-value pairs) under a single column in wide column databases. Such data types are often misused, and lead to severe performance problems. In fact, most of the data modeling problems we’ve come across involve misuse of collections. Collections are meant to store a small amount of information (such as phone numbers of an individual or different home/business addresses). However, collections with hundreds of thousands of entries are unfortunately not as rare as you might expect. They end up introducing a severe de-serialization overhead on the database. At best, this translates to higher latencies. At worst, this makes the data entirely unreadable due to the latency involved when scanning through the high number of items under such <span id="ITerm24">columns</span>.</p>
          <p class="Para" id="Par59">Some databases also support user created fields, such as <span id="ITerm25">User-Defined Types (UDTs)</span><span id="ITerm26"/> in Cassandra. UDTs can be a great ally for reducing the de-serialization overhead when you combine several columns into one. Think about it: Would you rather de-serialize four Boolean columns individually or a single column with four Boolean values? UDTs will typically shine on deserializing several values as a single column, which may give you a nice performance boost.<sup><a epub:type="noteref" href="#Fn9" id="Fn9_source" role="doc-noteref">9</a></sup> Just like collections, however, UDTs should not be misused—and misusing UDTs can lead to the same severe impacts that are incurred by collections.</p>
          <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar5">
            <div class="Heading">Note</div>
            <p class="Para FirstParaInFormalPara" id="Par61">UDTs are quite extensively covered in Chapter <span class="ExternalRef"><a href="541783_1_En_6_Chapter.xhtml"><span class="RefSource">6</span></a></span>.</p>
          </div>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec9">
          <h2 class="Heading">Dataset Size</h2>
          <p class="Para" id="Par62">Knowing your <span id="ITerm27">dataset size</span> is important for selecting appropriate infrastructure options. For example, AWS cloud instances have a broad array of NVMe storage offerings. Having a good grasp of how much storage you need can help you avoid selecting an instance that causes performance to suffer (if you end up with insufficient storage) or that’s wasteful from a cost perspective (if you overprovision).</p>
          <p class="Para" id="Par63">It’s important to note that your selected storage size should not be equal to your total dataset size. You also need to factor in replication and growth—plus steer clear of 100 percent storage utilization.</p>
          <p class="Para" id="Par64">For example, let’s assume you have 3TB of already compressed data. The bare minimum to support a workload is your current dataset size multiplied by your anticipated <span id="ITerm28">replication</span>. If you have 3TB of data with the common replication factor of three, that gives you 9TB. If you naively deployed this on three nodes supporting 3TB of data each, you’d hit near 100 percent disk utilization which, of course, is not optimal.</p>
          <p class="Para" id="Par65">Instead, if you factor in some free space and minimal room for growth, you’d want to start with at least six nodes of that size—each storing only 1.5TB of data. This gives you around 50 percent utilization. On the other hand, if your database cannot support that much data per node (every database has a limit) or if you do not foresee much future data growth, you could have six nodes supporting 2TB each, which would store approximately 1.5TB per replica under a 75 percent utilization. Remember: Factoring in your growth is critical for avoiding unpleasant surprises in production, from an operational as well as a budget perspective.</p>
          <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar6">
            <div class="Heading">Note</div>
            <p class="Para FirstParaInFormalPara" id="Par66">We very intentionally discussed the dataset size from a <em class="EmphasisTypeItalic ">compressed</em> data standpoint. Be aware that some database vendors measure your storage utilization with respect to <em class="EmphasisTypeItalic ">uncompressed</em> data. This often leads to confusion. If you’re moving data from one database solution to another and your data is uncompressed (or you’re not certain it’s compressed), consider loading a small fraction of your total dataset beforehand in order to determine its compression ratio. Effective compression can dramatically reduce your storage footprint.</p>
          </div>
          <p class="Para" id="Par67">If you’re working on a very fluid project and can’t define or predict your dataset size, a serverless database deployment model might be a good option to provide easy flexibility and scaling. But, be aware that rapid increases in overall dataset size and/or IOPS (depending on the pricing model) could cause the price to skyrocket exponentially. Even if you don’t explicitly pay a penalty for storing a large dataset, you might be charged a premium for the many operations that are likely associated with that large dataset. Serverless is discussed more in Chapter <span class="ExternalRef"><a href="541783_1_En_7_Chapter.xhtml"><span class="RefSource">7</span></a></span>.</p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec10">
          <h2 class="Heading">Throughput Expectations</h2>
          <p class="Para" id="Par68">Your expected throughput and <span id="ITerm29">latency</span> should be your “north star” from database and infrastructure selection all the way to monitoring. Let’s start with throughput.</p>
          <p class="Para" id="Par69">If you’re serious about database performance, it’s essential to know what throughput you’re trying to achieve—and “high throughput” is not an acceptable answer. Specifically, try to get all relevant stakeholders’ agreement on your target number of <em class="EmphasisTypeItalic ">peak</em> read operations per second and <em class="EmphasisTypeItalic ">peak</em> write operations per second <em class="EmphasisTypeItalic ">for each workload</em>.</p>
          <p class="Para" id="Par70">Let’s unravel that a little. First, be sure to separate read throughput vs write throughput. A database’s read path is usually quite distinct from its write path. It stresses different parts of the infrastructure and taps different database internals. And the client/user experience of reads is often quite different than that of writes. Lumping them together into a meaningless number won’t help you much with respect to performance measurement or optimization. The main use for average throughput is in applying Little’s Law (more on that in the “Concurrency” section a little later in this chapter).</p>
          <p class="Para" id="Par71">Another caveat: The same database’s past or current throughput with one use case is no guarantee of future results with another—even if it’s the same database hosted on identical infrastructure. There are too many different factors at play (item size, access patterns, concurrency… all the things in this chapter, really). What’s a great fit for one use case could be quite inappropriate for another.</p>
          <p class="Para" id="Par72">Also, note the emphasis on <em class="EmphasisTypeItalic ">peak</em> operations per second. If you build and optimize with an average in mind, you likely won’t be able to service beyond the upper ranges of that average. Focus on the peak throughput that you need to sustain to cover your core needs and business patterns—including surges. Realize that databases can often “boost” to sustain short bursts of exceptionally high load. However, to be safe, it’s best to plan for your likely peaks and reserve boosting for atypical situations.</p>
          <p class="Para" id="Par73">Also, be sure not to confuse concurrency with throughput. <span id="ITerm30"><em class="EmphasisTypeItalic ">Throughput</em></span> is the speed at which the database can perform read or write operations; it’s measured in the number of read or write operations per second. <span id="ITerm31"><em class="EmphasisTypeItalic ">Concurrency</em></span> is the number of requests that the client sends to the database at the same time (which, in turn, will eventually translate to a given number of concurrent requests queuing at the database for execution). Concurrency is expressed as a hard number, not a rate over a period of time. Not every request that is born at the same time will be able to be processed by the database at the same time. Your client could send 150K requests to the database, all at once. The database might blaze through all these concurrent requests if it’s running at 500K OPS. Or, it might take a while to process them if the database throughput tops out at 50K OPS.</p>
          <p class="Para" id="Par74">It is generally possible to increase throughput by increasing your cluster size (and/or power). But, you also want to pay special attention to concurrency, which will be discussed in more depth later in this chapter as well as in Chapter <span class="ExternalRef"><a href="541783_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span>. For the most part, high concurrency is essential for achieving impressive <span id="ITerm32">performance</span>. But if the clients end up overwhelming the database with a concurrency that it can’t handle, throughput will suffer, then latency will rise as a side effect. A friendly reminder that transcends the database world: No system, distributed or not, supports unlimited concurrency. Period.</p>
          <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar7">
            <div class="Heading">Note</div>
            <p class="Para FirstParaInFormalPara" id="Par75">Even though scaling a cluster boosts your database processing capacity, remember that the application access patterns directly contribute to how much impact that will ultimately make. One situation where scaling a cluster may not provide the desired throughput increase is during a <em class="EmphasisTypeItalic ">hot partition</em><sup><a epub:type="noteref" href="#Fn10" id="Fn10_source" role="doc-noteref">10</a></sup> situation, which causes traffic to be primarily targeted to a specific set of replicas. In these cases, throttling the access to such hot keys is fundamental for preserving the system’s overall performance.</p>
          </div>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec11">
          <h2 class="Heading">Latency Expectations</h2>
          <p class="Para" id="Par77"><span id="ITerm33">Latency</span> is a more complex challenge than throughput: You can increase throughput by adding more nodes, but there’s no simple solution for reducing latency. The lower the latency you need to achieve, the more important it becomes to understand and explore database tradeoffs and internal database optimizations that can help you shave milliseconds or microseconds off latencies. Database internals, driver optimizations, efficient CPU utilization, sufficient RAM, efficient data modeling… everything matters.</p>
          <p class="Para" id="Par78">As with throughput, aim for all relevant stakeholders’ agreement on the acceptable latencies. This is usually expressed as latency for a certain percentile of requests. For performance-sensitive workloads, tracking at the 99th percentile (P99) is common. Some teams go even higher, such as the P9999, which refers to the 99.99th percentile.</p>
          <p class="Para" id="Par79">As with throughput, avoid focusing on <em class="EmphasisTypeItalic ">average</em> (mean) or median (P50) latency measurements. Average latency is a theoretical measurement that is not directly correlated to anything systems or users experience in reality. Averages conceal outliers: Extreme deviations from the norm that may have a large and unexpected impact on overall system performance, and hence on user experience.</p>
          <div class="Para" id="Par80">For example, look at the discrepancy between average latencies and P99 latencies in Figure <span class="InternalRef"><a href="#Fig2">2-2</a></span> (different colors represent different database nodes). P99 latencies were often double the average for reads, and even worse for writes.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2">
              <img alt="" aria-describedby="d65e1149" src="../images/541783_1_En_2_Chapter/541783_1_En_2_Fig2_HTML.jpg" style="width:42.82em"/><div class="TextObject" id="d65e1149">
                <p class="Para" id="Par139">Six database graphs of average read or write latency versus instance. In the first graph, average read latency is mostly between 2 and 6 milliseconds.</p>
              </div>
              
            </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 2-2</span>
                <p class="SimplePara">A sample database monitoring dashboard. Note the difference between average and P99 <span id="ITerm34">latencies</span></p>
              </div></figcaption></figure></div>
          <p class="Para" id="Par81">Note that monitoring systems are sometimes configured in ways that omit outliers. For example, if a monitoring system is calibrated to measure latency on a scale of 0 to 1000ms, it is going to overlook any larger measurements—thus failing to detect the serious issues of query timeouts and retries.</p>
          <p class="Para" id="Par82">P99 and above percentiles are not perfect.<sup><a epub:type="noteref" href="#Fn11" id="Fn11_source" role="doc-noteref">11</a></sup> But for latency-sensitive use cases, they’re the number you’ll want to keep in mind as you are selecting your infrastructure, benchmarking, monitoring, and so on.</p>
          <p class="Para" id="Par84">Also, be clear about what exactly is involved in the P99 you are looking to achieve. Database latency is the time that elapses between when the database receives a request, processes it, and sends back an appropriate response. Client-side latency is broader: Here, the measurement starts with the client sending the request and ends with the client receiving the database’s response. It includes the network time and client-side processing. There can be quite a discrepancy between database latency and client-side <span id="ITerm35">latency</span>; a ten times higher client-side latency isn’t all that uncommon (although clearly not desirable). There could be many culprits to blame for a significantly higher client-side latency than database latency: excessive concurrency, inefficient application architecture, coding issues, and so on. But that’s beyond the scope of this discussion—beyond the scope of this book, even.</p>
          <p class="Para" id="Par85">The key point here is that your team and all the stakeholders need to be on the same page regarding what you’re measuring. For example, say you’re given a read latency requirement of 15ms. You work hard to get your database to achieve that and report that you met the expectation—then you learn that stakeholders actually expect 15ms for the full client-side latency. Back to the drawing board.</p>
          <p class="Para" id="Par86">Ultimately, it’s important to track both database latency and client-side latency. You can optimize the database all you want, but if the application is introducing latency issues from the client side, a fast database won’t have much impact. Without visibility into both the database and the client-side latencies, you’re essentially flying half blind.</p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec12">
          <h2 class="Heading">Concurrency</h2>
          <p class="Para" id="Par87">What level of <span id="ITerm36">concurrency</span> should your database be prepared to handle? Depending on the desired qualities of service from the database cluster, concurrency must be judiciously balanced to reach appropriate throughput and latency values. Otherwise, requests will pile up waiting to be processed—causing latencies to spike, timeouts to rise, and the overall user experience to degrade.</p>
          <div class="Para" id="Par88">Little’s Law establishes that:<div class="UnorderedList"><ul class="UnorderedListMarkNone"><li>
              <p class="Para" id="Par89">L=λW</p>
            </li></ul></div></div>
          <p class="Para" id="Par90">where λ is the average throughput, W is the average latency, and L represents the total number of requests either being processed or on queue at any given moment when the cluster reaches steady state. Given that your throughput and latency targets are usually fixed, you can use Little’s Law to estimate a realistic concurrency.</p>
          <p class="Para" id="Par91">For example, if you want a system to serve 500,000 requests per second at 2.5ms average latency, the best concurrency is around 1,250 in-flight requests. As you approach the saturation limit of the system—around 600,000 requests per second for read requests—increases in concurrency will keep constant since this is the physical limit of the database. Every new in-flight request will only cause increased latency. In fact, if you approximate 600,000 requests per second as the physical capacity of this database, you can calculate the expected average latency at a particular concurrency point. For example, at 6,120 in-flight requests, the average latency is expected to be 6120/600,000 = <span id="ITerm37">10ms</span>.</p>
          <p class="Para" id="Par92">Past the maximum throughput, increasing concurrency will increase latency. Conversely, reducing concurrency will reduce latency, provided that this reduction does not result in a decrease in throughput.</p>
          <p class="Para" id="Par93">In some use cases, it’s fine for queries to pile up on the client side. But many times it’s not. In those cases, you can scale out your cluster or increase the concurrency on the application side—at least to the point where the latency doesn’t suffer. It’s a delicate balancing act.<sup><a epub:type="noteref" href="#Fn12" id="Fn12_source" role="doc-noteref">12</a></sup></p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec13">
          <h2 class="Heading">Connected Technologies</h2>
          <p class="Para" id="Par95">A database can’t rise above the slowest-performing link in your <span id="ITerm38">distributed data system</span>. Even if your <span id="ITerm39">database</span> is processing reads and writes at blazing speeds, it won’t ultimately matter much if it interacts with an event-streaming platform that’s not optimized for performance or involves transformations from a poorly-configured Apache Spark instance, for example.</p>
          <p class="Para" id="Par96">This is just one of many reasons that taking a comprehensive and proactive approach to monitoring (more on this in Chapter <span class="ExternalRef"><a href="541783_1_En_10_Chapter.xhtml"><span class="RefSource">10</span></a></span>) is so important. Given the complexity of databases and distributed data systems, it’s hard to guess what component is to blame for a problem. Without a window into the state of the broader system, you could naively waste amazing amounts of time and resources trying to optimize something that won’t make any difference.</p>
          <p class="Para" id="Par97">If you’re looking to optimize an existing data system, don’t overlook the performance gains you can achieve by <span id="ITerm40">reviewing</span> and tuning its connected components. Or, if your monitoring efforts indicate that a certain component is to blame for your client-side performance problems but you feel you’ve hit your limit with it, explore what’s required to replace it with a more performant alternative. Use benchmarking to determine the severity of the impact from a performance perspective.</p>
          <p class="Para" id="Par98">Also, note that some database offerings may have ecosystem limitations. For example, if you’re considering a serverless deployment model, be aware that some Change Data Capture (CDC) connectors, drivers, and so on, might not be supported.</p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec14">
          <h2 class="Heading">Demand Fluctuations</h2>
          <p class="Para" id="Par99">Databases might experience a variety of different <span id="ITerm41">demand fluctuations</span>, ranging from predictable moderate fluctuations to unpredictable and dramatic spikes. For instance, the world’s most watched sporting event experiences different fluctuations than a food delivery service, which experiences different fluctuations than an ambulance-tracking service—and all require different strategies and infrastructure.</p>
          <p class="Para" id="Par100">First, let’s look at the predictable fluctuations. With predictability, it’s much easier to get ahead of the issue. If you’re expected to support periodic big events that are known in advance (Black Friday, sporting championships, ticket on sales, etc.), you should have adequate time to scale up your cluster for each anticipated spike. That means you can tailor your normal topology for the typical day-in, day-out demands without having to constantly incur the costs and admin burden of having that larger scale topology.</p>
          <p class="Para" id="Par101">On the other side of the spikiness spectrum, there’s applications with traffic with dramatic peaks and valleys across the course of each day. For example, consider food delivery businesses, which face a sudden increase around lunch, followed by a few hours of minimal traffic, then a second spike at dinner time (and sometimes breakfast the following morning). Expanding the cluster for each spike—even with “autoscaling” (more on autoscaling later in this chapter)—is unlikely to deliver the necessary performance gain fast enough. In these cases, you should provision an infrastructure that supports the peak traffic.</p>
          <p class="Para" id="Par102">But not all spikes are predictable. Certain industries—such as emergency services, news, and social media—are susceptible to sudden massive spikes. In this case, a good preventative strategy is to control your concurrency on the client side, so it doesn’t overwhelm your database. However, controlling concurrency might not be an option for use cases with strict end-to-end latency <span id="ITerm42">requirements</span>. You can also scramble to scale out your clusters as fast as feasible when the spike occurs. This is going to be markedly simpler if you’re on the cloud than if you’re on-prem. If you can start adding nodes immediately, increase capacity incrementally—with a close eye on your monitoring results—and keep going until you’re satisfied with the results, or until the peak has subsided. Unfortunately, there is a real risk that you won’t be able to sufficiently scale out before the spike ends. Even if the ramp up begins immediately, you need to account for the time it takes to get data over to add new nodes, stream data to them, and rebalance the cluster.</p>
          <p class="Para" id="Par103">If you’re selecting a new database and anticipate frequent and sharp spikes, be sure to rigorously test how your top contenders respond under realistic conditions. Also, consider the costs of maintaining acceptable performance throughout these peaks.</p>
          <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar8">
            <div class="Heading">Note</div>
            <p class="Para FirstParaInFormalPara" id="Par104">The word “autoscaling” insinuates that your database cluster auto-magically expands based on the traffic it is receiving. Not so. It’s simply a robot enabling/disabling capacity that’s pre-provisioned for you based on your target table settings. Even if you’re not using this capacity, you might be paying for the convenience of having it set aside and ready to go. Also, it’s important to realize that it’s not instantaneous. It takes upwards of 2.5 hours to go from 0 rps to 40k.<sup><a epub:type="noteref" href="#Fn13" id="Fn13_source" role="doc-noteref">13</a></sup> This is not ideal for unexpected or extreme spikes.</p>
            <p class="Para" id="Par106">Autoscaling is best when:</p>
          </div>
          <div class="Para" id="Par107"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li>
              <p class="Para ParaTypeImportant" id="Par108">Load changes have high amplitude</p>
            </li><li>
              <p class="Para ParaTypeImportant" id="Par109">The rate of change is in the magnitude of hours</p>
            </li><li>
              <p class="Para ParaTypeImportant" id="Par110">The load peak is narrow relative to the baseline<sup><a epub:type="noteref" href="#Fn14" id="Fn14_source" role="doc-noteref">14</a></sup></p>
            </li></ul></div></div>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec15">
          <h2 class="Heading">ACID Transactions</h2>
          <p class="Para" id="Par112">Does your use case require you to process a logical unit of work with <span id="ITerm43">ACID</span> (atomic, consistent, isolated, and <span id="ITerm44">durable</span>) properties? These transactions, which are historically the domain of RDBMS, bring a severe performance hit.</p>
          <p class="Para" id="Par113">It is true that distributed ACID compliant databases do exist—and that the past few years have brought some distinct progress in the effort to minimize the performance impact (e.g., through row-level locks or column-level locking and better conflict resolution algorithms). However, some level of penalty will still exist.</p>
          <p class="Para" id="Par114">As a general guidance, if you have an ACID-compliant use case, pay special attention to your master nodes; these can easily become your bottlenecks since they will often be your primary query coordinators (more on this in Appendix A). In addition, if at all possible, try to ensure that the majority of your transactions are isolated to the minimum amount of resources. For example, a transaction spanning a single row may involve a specific set of replicas, whereas a transaction involving several keys may span your cluster as a whole—inevitably increasing your latency. It is therefore important to understand which types of transactions your target database supports. Some vendors may support a mix of approaches, while others excel at specific ones. For instance, MongoDB introduced multi-document transactions on sharded clusters in its version 4.2; prior to that, it supported only multi-document transactions on replica sets.</p>
          <p class="Para" id="Par115">If it’s critical to support transactions in a more performant manner, sometimes it’s possible to rethink your data model and reimplement a use case in a way that makes it suitable for a database that’s not ACID compliant. For example, one team who started out with Postgres for all their use cases faced skyrocketing business growth. This is a very common situation with startups that begin small and then suddenly find themselves in a spot where they are unable to handle a spike in growth in a cost-effective way. They were able to move their use cases to NoSQL by conducting a careful data-modeling analysis and rethinking their use cases, access patterns, and the real business need of what truly required ACID and what did not. This certainly isn’t a quick fix, but in the right situation, it can pay off nicely.</p>
          <p class="Para" id="Par116">Another option to consider: Performance-focused NoSQL databases like Cassandra aim to support isolated conditional updates with capabilities such as lightweight transactions that allow “atomic compare and set” operations. That is, the database checks if a condition is true, and if so, it conducts the transaction. If the condition is not met, the transaction is not completed. They are named <span id="ITerm45">“lightweight</span>” since they do not truly lock the database for the transaction. Instead, they use a consensus protocol to ensure there is agreement between the nodes to commit the change. This capability was introduced by Cassandra and it’s supported in several ways across different Cassandra-compatible databases. If this is something you expect to use, it’s worth exploring the documentation to understand the differences.<sup><a epub:type="noteref" href="#Fn15" id="Fn15_source" role="doc-noteref">15</a></sup></p>
          <p class="Para" id="Par118">However, it’s important to note that lightweight transactions have their limits. They can’t support complex use cases like a retail transaction that updates the inventory only after a sale is completed with a successful payment. And just like ACID-compliant databases, lightweight transactions have their own performance implications. As a result, the choice of whether to use them will greatly depend on the amount of ACID compliance that your use case requires.</p>
          <p class="Para" id="Par119">DynamoDB is a prime example of how the need for transactions will require more compute resources (read: money). As a result, use cases relying heavily on ACID will fairly often require much more infrastructure power to satisfy heavy usage requirements. In the DynamoDB documentation, AWS recommends that you ensure the database is configured for auto-scaling or that it has enough read/write capacity to account for the additional overhead of transactions.<sup><a epub:type="noteref" href="#Fn16" id="Fn16_source" role="doc-noteref">16</a></sup></p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec16">
          <h2 class="Heading">Consistency Expectations</h2>
          <p class="Para" id="Par121">Most NoSQL databases opt for eventual consistency to gain performance. This is in stark contrast to the RDBMS model, where ACID compliance is achieved in the form of transactions, and, because everything is in a single node, the effort on locking and avoiding concurrency clashes is often minimized. When deciding between a database with strong or eventual consistency, you have to make a hard choice. Do you want to sacrifice scalability and performance or can you accept the risk of sometimes serving stale data?</p>
          <p class="Para" id="Par122">Can your use case tolerate eventual <span id="ITerm46">consistency</span>, or is strong consistency truly required? Your choice really boils down to how much risk your application—and your business—can tolerate with respect to inconsistency. For example, a retailer who (understandably) requires consistent pricing might want to pay the price for consistent writes upfront during a weekly catalog update so that they can later serve millions of low-latency read requests under more relaxed consistency levels. In other cases, it’s more important to ingest data quickly and pay the price for consistency later (for example, in the playback tracking use case that’s common in streaming platforms—where the database needs to record the last viewing position for many users concurrently). Or maybe both are equally important. For example, consider a social media platform that offers live chat. Here, you want consistency on both writes and reads, but you likely don’t need the highest consistency (the impact of an inconsistency here is likely much less than with a financial report).</p>
          <p class="Para" id="Par123">In some cases, “tunable consistency” will help you achieve a balance between strong consistency and performance. This gives you the ability to tune the consistency at the query level to suit what you’re trying to achieve. You can have some queries relying on a quorum of replicas, then have other queries that are much more relaxed.</p>
          <p class="Para" id="Par124">Regardless of your consistency requirements, you need to be aware of the implications involved when selecting a given consistency level. Databases that offer tunable consistency may be a blessing or a curse if you don’t know what you are doing. Consider a NoSQL deployment spanning three different regions, with three nodes each (nine nodes in total). A QUORUM read would essentially have to traverse two different regions in order to be acknowledged back to the client. In that sense, if your Network Round Trip Time (RTT)<sup><a epub:type="noteref" href="#Fn17" id="Fn17_source" role="doc-noteref">17</a></sup> is 50ms, then it will take <em class="EmphasisTypeItalic ">at least</em> this amount of time for the query to be considered successful by the database. Similarly, if you were to run operations with the highest possible consistency (involving all replicas), then the failure of a single node may bring your entire application down.</p>
          <div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar9">
            <div class="Heading">Note</div>
            <p class="Para FirstParaInFormalPara" id="Par126">NoSQL databases fairly often will provide you with ways to confine your queries to a specific region to prevent costly network round trips from impacting your latency. But again, it all boils down to you what your use case requires.</p>
          </div>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec17">
          <h2 class="Heading">Geographic Distribution</h2>
          <p class="Para" id="Par127">Does your business need to support a regional or global customer base in the near-term future? Where are your users and your application located? The greater the distance between your users, your application, and your database, the more they’re going to face high latencies that stem from the physical time it takes to move data across the network. Knowing this will influence where you locate your database and how you design your topology—more on this in Chapters <span class="ExternalRef"><a href="541783_1_En_6_Chapter.xhtml"><span class="RefSource">6</span></a></span> and <span class="ExternalRef"><a href="541783_1_En_8_Chapter.xhtml"><span class="RefSource">8</span></a></span>.</p>
          <p class="Para" id="Par128">The <span id="ITerm47">geographic distribution</span> of your cluster might also be a requirement from a disaster recovery perspective. In that sense, the cluster would typically serve data primarily from a specific region, but failover to another in the event of a disaster (such as a full region outage). These kinds of setups are costly, as they will require doubling your infrastructure spend. However, depending on the nature of your use case, sometimes it’s required.</p>
          <p class="Para" id="Par129">Some organizations that invest in a multi-region deployment for the primary purpose of disaster recovery end up using them to host isolated use cases. As explained in the “Competing Workloads” section of this chapter, companies often prefer to physically isolate OLTP from OLAP workloads. Moving some isolated (less critical) workloads to remote regions prevents these servers from being “idle” most of the time.</p>
          <div class="Para" id="Par130">Regardless of the magnitude of compelling reasons that may drive you toward a geographically dispersed deployment, here’s some important high-level advice from a performance perspective (you’ll learn some more technical tips in Chapter <span class="ExternalRef"><a href="541783_1_En_8_Chapter.xhtml"><span class="RefSource">8</span></a></span>):<div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent">
                <p class="Para" id="Par131">Consider the increased load that your target region or regions will receive in the event of a full region outage. For example, assume that you operate globally across three regions, and all these three regions serve your end-users. Are the two remaining regions able to sustain the load for a long period of time?</p>
              </div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent">
                <p class="Para" id="Par132">Recognize that simply having a geographically-dispersed database does <em class="EmphasisTypeItalic ">not</em> fully cover you in a disaster recovery situation. You also need to have your application, web servers, messaging queue systems, and so on, geographically replicated. If the only thing that’s geo-replicated is your database, you won’t be in a great position when your primary application goes down.</p>
              </div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent">
                <p class="Para" id="Par133">Consider the fact that geo-replicated databases typically require very good <span id="ITerm48">network</span> links. Especially when crossing large distances, the time to replicate your data is crucial to minimize losses in the event of a disaster. If your workload has a heavy write throughput, a slow network link may bottleneck the local region nodes. This may cause a queue to build up and eventually throttle down your writes.</p>
              </div><div class="ClearBoth"> </div></li></ol></div></div>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec18">
          <h2 class="Heading">High-Availability Expectations</h2>
          <p class="Para" id="Par134">Inevitably, s#*&amp; happens. To prepare for the worst, start by understanding what your use case and business can tolerate if a node goes down. Can you accept the data loss that could occur if a node storing unreplicated data goes down? Do you need to continue buzzing along without a noticeable performance impact even if an entire datacenter or availability zone goes down? Or is it okay if things slow down a bit from time to time? This will all impact how you architect your topology and configure things like replication factor and consistency levels (you’ll learn about this more in Chapter <span class="ExternalRef"><a href="541783_1_En_8_Chapter.xhtml"><span class="RefSource">8</span></a></span>).</p>
          <p class="Para" id="Par135">It’s important to note that <span id="ITerm49">replication</span> and consistency both come at a cost to performance. Get a good feel for your business’s risk tolerance and don’t opt for more than your business really needs.</p>
          <p class="Para" id="Par136">When considering your cluster topology, remember that quite a lot is at risk if you get it wrong (and you don’t want to be caught off-guard in the middle of the night). For example, the failure of a single node in a three-node cluster could make you momentarily lose 33 percent of your processing power. Quite often, that’s a significant blow, with discernable business impact. Similarly, the loss of a node in a six-node cluster would reduce the blast radius to only 16 percent. But there’s always a tradeoff. A sprawling deployment <span id="ITerm50">spanning</span> hundreds of nodes is not ideal either. The more nodes you have, the more likely you are to experience a node failure. Balance is key.</p>
        </section>

        <section class="Section1 RenderAsSection1" id="Sec19">
          <h2 class="Heading">Summary</h2>
          <p class="Para" id="Par137">The specific database challenges you encounter, as well as your options for addressing them, are highly dependent on your situation. For example, an <span id="ITerm51">AdTech use case</span> that demands single-digit millisecond P99 latencies for a large dataset with small item sizes requires a different treatment than a fraud detection use case that prioritizes the ingestion of massive amounts of data as rapidly as possible. One of the primary factors influencing how these workloads are handled is how your database is architected. That’s the focus for the next two chapters, which dive into database internals.</p>
        </section>

      <div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a>
            <p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
            <p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
          </div><aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1" role="doc-footnote"><p class="Para" id="Par6">With write-heavy workloads, you can easily spend millions per month with Bigtable or DynamoDB. Read-heavy workloads are typically less costly in these pricing models.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2" role="doc-footnote"><p class="Para" id="Par9">If you want a quick introduction to LSM trees and B-trees, see Appendix A. Chapter <span class="ExternalRef"><a href="541783_1_En_4_Chapter.xhtml"><span class="RefSource">4</span></a></span> also discusses B-trees in more detail.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn3" role="doc-footnote"><p class="Para" id="Par11"><em class="EmphasisTypeItalic ">Compaction</em> is a background process that databases with an LSM tree storage backend use to merge and optimize the shape of the data. Since files are immutable, the process essentially involves picking up two or more pre-existing files, merging their contents, and producing a sorted output file.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn4" role="doc-footnote"><p class="Para" id="Par28">The “Competing Workloads” section later in this chapter, as well as the “Workload Isolation” section in Chapter <span class="ExternalRef"><a href="541783_1_En_8_Chapter.xhtml"><span class="RefSource">8</span></a></span>, cover a few options for prioritizing and separating workloads.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn5_source">5</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn5" role="doc-footnote"><p class="Para" id="Par37">For some specific recommendations, see the DataStax blog, “Cassandra Anti-Patterns: Queues and Queue-like Datasets” (<span class="ExternalRef"><a href="http://www.datastax.com/blog/cassandra-anti-patterns-queues-and-queue-datasets"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.datastax.com/blog/cassandra-anti-patterns-queues-and-queue-datasets</span></span></a></span>)</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn6_source">6</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn6" role="doc-footnote"><p class="Para" id="Par40">See the article, “Tencent Games’ Real-Time Event-Driven Analytics System Built with ScyllaDB + Pulsar” (<span class="ExternalRef"><a href="https://www.scylladb.com/2023/05/15/tencent-games-real-time-event-driven-analytics-systembuilt-with-scylladb-pulsar/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://www.scylladb.com/2023/05/15/tencent-games-real-time-event-driven-analytics-systembuilt-with-scylladb-pulsar/</span></span></a></span>)</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn7_source">7</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn7" role="doc-footnote"><p class="Para" id="Par50">There are alternatives to this; for example, RDMA, DPDK and other solutions. However, most use cases do not require such solutions, so they are not covered in detail here.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn8_source">8</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn8" role="doc-footnote"><p class="Para" id="Par52">For details, see the Epic Games talk, “Using ScyllaDB for Distribution of Game Assets in Unreal Engine” (<span class="ExternalRef"><a href="http://www.youtube.com/watch?v=aEgP9YhAb08"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.youtube.com/watch?v=aEgP9YhAb08</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn9_source">9</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn9" role="doc-footnote"><p class="Para" id="Par60">For some specific examples of how UDTs impact performance, see the performance benchmark that ScyllaDB performed with different UDT sizes against individual columns: “If You Care About Performance, Employ User Defined Types” (<span class="ExternalRef"><a href="https://www.scylladb.com/2017/12/07/performance-udt/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://www.scylladb.com/2017/12/07/performance-udt/</span></span></a></span>)</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn10_source">10</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn10" role="doc-footnote"><p class="Para" id="Par76">A hot partition is a data access imbalance problem that causes specific partitions to receive more traffic compared to others, thus introducing higher load on a specific set of replica servers.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn11_source">11</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn11" role="doc-footnote"><p class="Para" id="Par83">For a detailed critique, see Gil Tene’s famous “Oh Sh*t” talk (<span class="ExternalRef"><a href="http://www.youtube.com/watch?v=lJ8ydIuPFeU"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.youtube.com/watch?v=lJ8ydIuPFeU</span></span></a></span><span class="EmphasisFontCategoryNonProportional ">) as well as his recent P99 CONF talk on Misery Metrics and Consequences</span> (<span class="ExternalRef"><a href="https://www.p99conf.io/session/misery-metrics-consequences/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://www.p99conf.io/session/misery-metrics-consequences/</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn12_source">12</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn12" role="doc-footnote"><p class="Para" id="Par94">For additional reading on concurrency, the Netflix blog “Performance Under Load” is a great resource (<span class="ExternalRef"><a href="https://netflixtechblog.medium.com/performance-under-load-3e6fa9a60581"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://netflixtechblog.medium.com/performance-under-load-3e6fa9a60581</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn13_source">13</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn13" role="doc-footnote"><p class="Para" id="Par105">See The Burning Monk blog, “Understanding the Scaling Behaviour of DynamoDB OnDemand Tables” (<span class="ExternalRef"><a href="https://theburningmonk.com/2019/03/understanding-the-scaling-behaviour-of-dynamodb-ondemand-tables/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://theburningmonk.com/2019/03/understanding-the-scaling-behaviour-of-dynamodb-ondemand-tables/</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn14_source">14</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn14" role="doc-footnote"><p class="Para" id="Par111">For more on the best and worst uses of autoscaling, see Avishai Ish Shalom’s blog, “DynamoDB Autoscaling Dissected: When a Calculator Beats a Robot” (<span class="ExternalRef"><a href="http://www.scylladb.com/2021/07/08/dynamodb-autoscaling-dissected-when-a-calculator-beats-a-robot/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.scylladb.com/2021/07/08/dynamodb-autoscaling-dissected-when-a-calculator-beats-a-robot/</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn15_source">15</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn15" role="doc-footnote"><p class="Para" id="Par117">See Kostja Osipov’s blog, “Getting the Most Out of Lightweight Transactions in ScyllaDB” (<span class="ExternalRef"><a href="http://www.scylladb.com/2020/07/15/getting-the-most-out-of-lightweight-transactions-in-scylla/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.scylladb.com/2020/07/15/getting-the-most-out-of-lightweight-transactions-in-scylla/</span></span></a></span>) for an example of how financial transactions can be implemented using Lightweight Transactions.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn16_source">16</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn16" role="doc-footnote"><p class="Para" id="Par120">See “Amazon DynamoDB Transactions: How it Works” (<span class="ExternalRef"><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html</span></span></a></span>).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn17_source">17</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn17" role="doc-footnote"><p class="Para" id="Par125"><em class="EmphasisTypeItalic ">RTT</em> is the duration, typically measured in milliseconds, that a network request takes to reach a destination, plus the time it takes for the packet to be received back at the origin.</p></div><div class="ClearBoth"> </div></div></aside></div></div></body></html>