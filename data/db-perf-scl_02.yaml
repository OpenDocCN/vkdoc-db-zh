- en: '3. Database Internals: Hardware and Operating System Interactions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 数据库内部结构：硬件和操作系统交互
- en: '[CPU](#Sec1)[Memory](#Sec10)[I/O](#Sec13)[Networking](#Sec28)[Summary](#Sec31)'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[CPU](#Sec1)[内存](#Sec10)[I/O](#Sec13)[网络](#Sec28)[总结](#Sec31)'
- en: A database’s internal architecture makes a tremendous impact on the latency
    it can achieve and the throughput it can handle. Being an extremely complex piece
    of software, a database doesn’t exist in a vacuum, but rather interacts with the
    environment, which includes the operating system and the hardware.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库的内部架构对其能够达到的延迟和能够处理的吞吐量有着巨大的影响。作为一个极其复杂的软件，数据库并非孤立存在，而是与环境相互作用，这包括操作系统和硬件。
- en: While it’s one thing to get massive terabyte-to-petabyte scale systems up and
    running, it’s a whole other thing to make sure they are operating at peak efficiency.
    In fact, it’s usually more than just “one other thing.” Performance optimization
    of large distributed systems is usually a multivariate problem—combining aspects
    of the underlying hardware, networking, tuning operating systems, and finagling
    with layers of virtualization and application architectures.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然让大规模的千兆字节到拍字节规模系统运行起来是一件事情，但确保它们以最高效率运行则是另一回事。实际上，这通常不仅仅是“另一件事”。大型分布式系统的性能优化通常是一个多变量问题——结合底层硬件、网络、调整操作系统以及与虚拟化和应用程序架构各层进行协调。
- en: Such a complex problem warrants exploration from multiple perspectives. This
    chapter begins the discussion of database internals by looking at ways that databases
    can optimize performance by taking advantage of modern hardware and operating
    systems. It covers how the database interacts with the operating system plus CPUs,
    memory, storage, and networking. Then, the next chapter shifts focus to algorithmic
    optimizations.^([1](#Fn1))
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这样复杂的问题需要从多个角度进行探索。本章通过探讨数据库如何通过利用现代硬件和操作系统来优化性能，开始了对数据库内部结构的讨论。它涵盖了数据库与操作系统、CPU、内存、存储和网络之间的交互。然后，下一章将重点转向算法优化。[1](#Fn1)
- en: CPU
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU
- en: Programming books tell programmers that they have this CPU that can run processes
    or threads, and what *runs* means is that there’s some simple sequential instruction
    execution. Then there’s a footnote explaining that with multiple threads you might
    need to consider doing some synchronization. In fact, how things are actually
    executed inside CPU cores is something completely different and much more complicated.
    It would be very difficult to program these machines if you didn’t have those
    abstractions from books, but they are a lie to some degree. How you can efficiently
    take advantage of CPU capabilities is still very important.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 编程书籍告诉程序员，他们有一个可以运行进程或线程的CPU，而“运行”意味着有一些简单的顺序指令执行。然后有一个脚注解释说，在多个线程的情况下，可能需要考虑进行一些同步。实际上，事情在CPU核心内部的实际执行方式完全不同，并且要复杂得多。如果没有从书籍中获得的这些抽象，编程这些机器将会非常困难，但它们在某种程度上是谎言。如何有效地利用CPU能力仍然非常重要。
- en: Share Nothing Across Cores
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心间无共享
- en: 'Individual CPU cores aren’t getting any faster. Their clock speeds reached
    a performance plateau long ago. Now, the ongoing increase of CPU performance continues
    horizontally: by increasing the number of processing units. In turn, the increase
    in the number of cores means that performance now depends on coordination across
    multiple cores (versus the throughput of a single core).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 单个CPU核心的速度并没有加快。它们的时钟速度早已达到性能平台期。现在，CPU性能的持续提升是横向的：通过增加处理单元的数量。反过来，核心数量的增加意味着性能现在取决于多个核心之间的协调（而不是单个核心的吞吐量）。
- en: 'On modern hardware, the performance of standard workloads depends more on the
    locking and coordination across cores than on the performance of an individual
    core. Software architects face two unattractive alternatives:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代硬件上，标准工作负载的性能更多地取决于锁定和核心间的协调，而不是单个核心的性能。软件架构师面临着两个不吸引人的选择：
- en: Coarse-grained locking, which will see application threads contend for control
    of the data and wait instead of producing useful work.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 粗粒度锁定，这会导致应用程序线程争夺对数据的控制权，并等待而不是产生有用的工作。
- en: Fine-grained locking, which, in addition to being hard to program and debug,
    sees significant overhead even when no contention occurs due to the locking primitives
    themselves.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细粒度锁定，除了编程和调试困难外，即使在没有竞争发生的情况下，由于锁定原语本身也会产生显著的开销。
- en: Consider an SSD drive. The typical time needed to communicate with an SSD on
    a modern NVMe device is quite lengthy—it’s about 20 μseconds. That’s enough time
    for the CPU to execute tens of thousands of instructions. Developers should consider
    it as a networked device but generally do not program in that way. Instead, they
    often use an API that is synchronous (we’ll return to this later), which produces
    a thread that can be blocked.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 SSD 驱动器。在现代 NVMe 设备上与 SSD 通信所需的典型时间相当长——大约 20 微秒。这足以让 CPU 执行数万条指令。开发者应该将其视为网络设备，但通常不会以这种方式编程。相反，他们经常使用同步（我们稍后会回到这一点）的
    API，这会产生一个可能会阻塞的线程。
- en: Looking at the image of the logical layout of an Intel Xeon Processor (see Figure
    [3-1](#Fig1)), it’s clear that this is also a networked device.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 观察英特尔至强处理器逻辑布局的图像（见图 [3-1](#Fig1)），很明显，这也是一个网络设备。
- en: '![](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig1_HTML.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig1_HTML.jpg)'
- en: An illustration of Intel Xeon processor layout. Four Skylake processors are
    interconnected with Intel U P I in between and each consisting of D D R 4 D I
    M Ms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔至强处理器布局示意图。四个斯凯莱处理器通过英特尔 UPI 互连，每个处理器都包含 DDR4 DIMMs。
- en: Figure 3-1
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-1
- en: The logical layout of an Intel Xeon Processor
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔至强处理器的逻辑布局
- en: The cores are all connected by what is essentially a network—a dual ring interconnected
    architecture. There are two such rings and they are bidirectional. Why should
    developers use a synchronous API for that then? Since sharing information across
    cores requires costly locking, a shared-nothing model is perfectly worth considering.
    In such a model, all requests are sharded onto individual cores, one application
    thread is run per core, and communication depends on explicit message passing,
    not shared memory between threads. This design avoids slow, unscalable lock primitives
    and cache bounces.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 核心之间通过本质上是一个网络——一个双环互连架构连接在一起。有两个这样的环，并且它们是双向的。那么开发者为什么要使用同步 API 呢？由于跨核心共享信息需要昂贵的锁定，无共享模型是值得考虑的。在这种模型中，所有请求都被分片到各个核心，每个核心运行一个应用程序线程，通信依赖于显式的消息传递，而不是线程间的共享内存。这种设计避免了缓慢、不可扩展的锁定原语和缓存波动。
- en: Any sharing of resources across cores in modern processors must be handled explicitly.
    For example, when two requests are part of the same session and two CPUs each
    get a request that depends on the same session state, one CPU must explicitly
    forward the request to the other. Either CPU may handle either response. Ideally,
    your database provides facilities that limit the need for cross-core communication—but
    when communication is inevitable, it provides high-performance non-blocking communication
    primitives to ensure performance is not degraded.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代处理器中，跨核心的资源共享必须显式处理。例如，当两个请求是同一会话的一部分，并且两个 CPU 各自收到一个依赖于相同会话状态的请求时，一个 CPU
    必须显式地将请求转发给另一个 CPU。任一 CPU 都可以处理任一响应。理想情况下，你的数据库提供限制跨核心通信需求的设施——但是当通信不可避免时，它提供高性能的非阻塞通信原语，以确保性能不会降低。
- en: Futures-Promises
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Futures-Promises
- en: There are many solutions for coordinating work across multiple cores. Some are
    highly programmer-friendly and enable the development of software that works exactly
    as if it were running on a single core. For example, the classic UNIX process
    model is designed to keep each process in total isolation and relies on kernel
    code to maintain a separate virtual memory space per process. Unfortunately, this
    increases the overhead at the OS level.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个核心之间协调工作有许多解决方案。其中一些非常符合程序员的需求，并能够开发出在单核上运行的软件。例如，经典的 UNIX 进程模型旨在保持每个进程的总隔离，并依赖于内核代码来维护每个进程的单独虚拟内存空间。不幸的是，这增加了操作系统层面的开销。
- en: There’s a model known as “futures and promises.” A *future* is a data structure
    that represents some yet-undetermined result. A *promise* is the provider of this
    result. It can be helpful to think of a promise/future pair as a first-in first-out
    (FIFO) queue with a maximum length of one item, which may be used only once. The
    promise is the producing end of the queue, while the future is the consuming end.
    Like FIFOs, futures and promises decouple the data producer and the data consumer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个被称为“future 和 promise”的模型。一个 *future* 是表示某些尚未确定的结果的数据结构。一个 *promise* 是这个结果提供者。有助于将
    promise/future 对象想象为一个最大长度为一件物品的先进先出（FIFO）队列，该物品只能使用一次。promise 是队列的生产端，而 future
    是消费端。像 FIFOs 一样，futures 和 promises 解耦了数据生产者和数据消费者。
- en: 'However, the optimized implementations of futures and promises need to take
    several considerations into account. While the standard implementation targets
    coarse-grained tasks that may block and take a long time to complete, optimized
    futures and promises are used to manage fine-grained, non-blocking tasks. In order
    to meet this requirement efficiently, they should:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，未来和承诺的优化实现需要考虑几个因素。虽然标准实现针对粗粒度任务，这些任务可能会阻塞并且需要很长时间才能完成，但优化后的未来和承诺用于管理细粒度、非阻塞任务。为了有效地满足这一要求，它们应该：
- en: Require no locking
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要锁定
- en: Not allocate memory
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不分配内存
- en: Support continuations
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持续传
- en: Future-promise design eliminates the costs associated with maintaining individual
    threads by the OS and allows close to complete utilization of the CPU. On the
    other hand, it calls for user-space CPU scheduling and very likely limits the
    developer with voluntary preemption scheduling. The latter, in turn, is prone
    to generating phantom jams in popular producer-consumer programming templates.^([2](#Fn2))
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 未来-承诺设计消除了与维护单个线程相关的成本，允许接近完全利用CPU。另一方面，它要求用户空间CPU调度，并且很可能限制了开发者的自愿预占调度。后者反过来又容易在流行的生产者-消费者编程模板中产生幽灵拥堵。[2](#Fn2)
- en: 'Applying future-promise design to database internals has obvious benefits.
    First of all, database workloads can be naturally CPU-bound. For example, that’s
    typically the case with in-memory database engines, and aggregates’ evaluations
    also involve pretty intensive CPU work. Even for huge on-disk datasets, when the
    query time is typically dominated by the I/O, CPU should be considered. Parsing
    a query is a CPU-intensive task regardless of whether the workload is CPU-bound
    or storage-bound, and collecting, converting, and sending the data back to the
    user also calls for careful CPU utilization. And last but not least: Processing
    the data always involves *a lot* of high-level operations and low-level instructions.
    Maintaining them in an optimal manner requires a good low-level programming paradigm
    and future-promises is one of the best choices. However, large instruction sets
    need even more care; this leads to “execution stages.”'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将未来-承诺设计应用于数据库内部具有明显的优势。首先，数据库工作负载可以自然地成为CPU密集型。例如，内存数据库引擎通常就是这样，聚合的评估也涉及到相当密集的CPU工作。即使是巨大的磁盘数据集，当查询时间通常由I/O主导时，也应该考虑CPU。解析查询是一项CPU密集型任务，无论工作负载是CPU密集型还是存储密集型，收集、转换并将数据发送回用户也需要仔细的CPU利用。最后但同样重要的是：处理数据总是涉及到*大量*高级操作和低级指令。以最佳方式维护它们需要一个良好的低级编程范式，而未来-承诺是最佳选择之一。然而，大的指令集需要更多的关注；这导致了“执行阶段”。
- en: Execution Stages
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行阶段
- en: 'Let’s dive deeper into CPU microarchitecture, because (as discussed previously)
    database engine CPUs typically need to deal with millions and billions of instructions,
    and it’s essential to help the poor thing with that. In a very simplified way,
    the microarchitecture of a modern x86 CPU—from the point of view of top-down analysis—consists
    of four major components: frontend, backend, branch speculation, and retiring.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解CPU微架构，因为（如前所述）数据库引擎CPU通常需要处理数百万甚至数十亿条指令，帮助这个可怜的家伙是至关重要的。以非常简化的方式，现代x86
    CPU的微架构——从自上而下的分析角度来看——由四个主要组件组成：前端、后端、分支预测和退役。
- en: Frontend
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前端
- en: The processor’s frontend is responsible for fetching and decoding instructions
    that are going to be executed. It may become a bottleneck when there is either
    a latency problem or insufficient bandwidth. The former can be caused, for example,
    by instruction cache misses. The latter happens when the instruction decoders
    cannot keep up. In the latter case, the solution may be to attempt to make the
    hot path (or at least significant portions of it) fit in the decoded μop cache
    (DSB) or be recognizable by the loop detector (LSD).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器的 frontend 负责获取和解析将要执行的指令。当存在延迟问题或带宽不足时，它可能会成为瓶颈。前者可能是由指令缓存未命中引起的。后者发生在指令解码器无法跟上时。在这种情况下，解决方案可能是尝试使热路径（或至少其重要部分）适合解码
    μop 缓存（DSB）或可被循环检测器（LSD）识别。
- en: Branch Speculation
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分支预测
- en: 'Pipeline slots that the top-down analysis classifies as *bad speculation* are
    not stalled, but wasted. This happens when a branch is incorrectly predicted and
    the rest of the CPU executes a μop that eventually cannot be committed. The branch
    predictor is generally considered to be a part of the frontend. However, its problems
    can affect the whole pipeline in ways beyond just causing the backend to be undersupplied
    by the instruction fetch and decode. (Note: Branch mispredictions are covered
    in more detail a bit later in this chapter.)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层分析归类为 *不良推测* 的流水线槽位并未停滞，而是浪费了。这发生在分支预测错误，而 CPU 执行的 μop 最终无法提交的情况下。分支预测器通常被认为是前端的一部分。然而，它的问题可能会以超出仅仅导致后端指令获取和解码不足的方式影响整个流水线。（注意：分支误判将在本章稍后更详细地介绍。）
- en: Backend
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 后端
- en: The backend receives decoded μops and executes them. A stall may happen either
    because of an execution port being busy or a cache miss. At the lower level, a
    pipeline slot may be core bound either due to data dependency or an insufficient
    number of available execution units. Stalls caused by memory can be caused by
    cache misses at different levels of data cache, external memory latency, or bandwidth.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 后端接收解码后的 μops 并执行它们。停滞可能发生，要么是因为执行端口忙碌，要么是因为缓存未命中。在较低级别，流水线槽位可能因为数据依赖或可用的执行单元数量不足而受限。由内存引起的停滞可能由不同级别的数据缓存中的缓存未命中、外部内存延迟或带宽不足引起。
- en: Retiring
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 退休
- en: Finally, there are pipeline slots that get classified as *retiring*. They are
    the lucky ones that were able to execute and commit their μop without any problems.
    When 100 percent of the pipeline slots are able to retire without a stall, the
    program has achieved the maximum number of instructions per cycle for that model
    of the CPU. Although this is very desirable, it doesn’t mean that there’s no opportunity
    for improvement. Rather, it means that the CPU is fully utilized and the only
    way to improve the performance is to reduce the number of instructions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有一些流水线槽位被归类为 *退休*。它们是那些能够无任何问题执行和提交 μop 的幸运儿。当 100% 的流水线槽位能够无停滞地退休时，该 CPU
    模型的程序已经实现了每周期最大指令数。尽管这非常理想，但这并不意味着没有改进的机会。相反，这意味着 CPU 已被充分利用，提高性能的唯一方法是减少指令数量。
- en: Implications for Databases
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对数据库的影响
- en: The way CPUs are architectured has direct implications on the database design.
    It may very well happen that individual requests involve a lot of logic and relatively
    little data, which is a scenario that stresses the CPU significantly. This kind
    of workload will be completely dominated by the frontend—instruction cache misses
    in particular. If you think about this for a moment, it shouldn’t be very surprising.
    The pipeline that each request goes through is quite long. For example, write
    requests may need to go through transport protocol logic, query parsing code,
    look up in the caching layer, or be applied to the memtable, and so on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 的架构方式对数据库设计有直接影响。完全有可能发生单个请求涉及大量逻辑和相对较少数据的情况，这是一个会显著增加 CPU 负担的场景。这种类型的工作负载将完全由前端主导——特别是指令缓存未命中。如果你稍微思考一下，这不应该非常令人惊讶。每个请求通过的流水线相当长。例如，写请求可能需要通过传输协议逻辑、查询解析代码、在缓存层中查找，或者应用到
    memtable 等。
- en: The most obvious way to solve this is to attempt to reduce the amount of logic
    in the hot path. Unfortunately, this approach does not offer a huge potential
    for significant performance improvement. Reducing the number of instructions needed
    to perform a certain activity is a popular optimization practice, but a developer
    cannot make any code shorter infinitely. At some point, the code “freezes”—literally.
    There’s some minimal amount of instructions needed even to compare two strings
    and return the result. It’s impossible to perform that with a single instruction.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题最明显的方法是尝试减少热路径中的逻辑量。不幸的是，这种方法并不提供巨大的性能提升潜力。减少执行特定活动所需的指令数量是一种流行的优化实践，但开发者不能无限地缩短代码。在某个时刻，代码“冻结”——字面意义上的。即使是比较两个字符串并返回结果，也需要一些最小数量的指令。用单个指令完成这是不可能的。
- en: A higher-level way of dealing with instruction cache problems is called Staged
    Event-Driven Architecture (SEDA for short). It’s an architecture that splits the
    request processing pipeline into a graph of stages—thereby decoupling the logic
    from the event and thread scheduling. This tends to yield greater performance
    improvements than the previous approach.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 处理指令缓存问题的更高层次方法被称为阶段事件驱动架构（简称SEDA）。这是一种将请求处理管道分割成一系列阶段的架构——从而将逻辑与事件和线程调度解耦。这种方法通常比之前的方法带来更大的性能提升。
- en: Memory
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存
- en: Memory management is the central design point in all aspects of programming.
    Even comparing programming languages to one another always involves discussions
    about the way programmers are supposed to handle memory allocation and freeing.
    No wonder memory management design affects the performance of a database so much.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理是编程所有方面的核心设计点。甚至比较编程语言时，也总是涉及关于程序员应该如何处理内存分配和释放的讨论。难怪内存管理设计对数据库性能的影响如此之大。
- en: 'Applied to database engineering, memory management typically falls into two
    related but independent subsystems: memory allocation and cache control. The former
    is in fact a very generic software engineering issue, so considerations about
    it are not extremely specific to databases (though they are crucial and are worth
    studying). As opposed to that, the latter topic is itself very broad, affected
    by the usage details and corner cases. Respectively, in the database world, cache
    control has its own flavor.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库工程中，内存管理通常分为两个相关但独立的子系统：内存分配和缓存控制。前者实际上是一个非常通用的软件工程问题，因此对其的考虑并不特别针对数据库（尽管它们至关重要，值得研究）。相比之下，后者本身非常广泛，受使用细节和边缘情况的影响。在数据库世界中，缓存控制有其独特的风格。
- en: Allocation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分配
- en: The manner in which programs or subsystems allocate and free memory lies at
    the core of memory management. There are several approaches worth considering.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 程序或子系统分配和释放内存的方式是内存管理的核心。有几个方法值得考虑。
- en: As illustrated by Figure [3-2](#Fig2), a so-called “log-structured allocation”
    is known from filesystems where it puts sequential writes to a circular log on
    the persisting storage and handles updates the very same way. At some point, this
    filesystem must reclaim blocks that became obsolete entries in the log area to
    make some more space available for future writes. In a naive implementation, unused
    entries are reclaimed by rereading and rewriting the log from scratch; obsolete
    blocks are then skipped in the process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[3-2](#Fig2)所示，所谓的“日志结构化分配”在文件系统中是已知的，它将顺序写入持久存储上的循环日志，并以完全相同的方式处理更新。在某个时刻，该文件系统必须回收日志区域中成为过时条目的块，以便为未来的写入腾出更多空间。在简单的实现中，未使用的条目通过重新读取和从头开始重写日志来回收；然后在这个过程中跳过过时的块。
- en: '![](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig2_HTML.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig2_HTML.jpg)'
- en: An illustration of an allocation structure of data and superblock using logs.
    Four types of logs used are file, inode, directory, and inode map.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用日志展示数据分配结构和超级块的一个示例。使用的四种日志类型是文件、inode、目录和inode映射。
- en: Figure 3-2
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-2
- en: A log-structured allocation puts sequential writes to a circular log on the
    persisting storage and handles updates the same way
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 日志结构化分配将顺序写入持久存储上的循环日志，并以相同的方式处理更新
- en: A memory allocator for naive code can do something similar. In its simplest
    form, it would allocate the next block of memory by simply advancing a next-free
    pointer. Deallocation would just need to mark the allocated area as freed. One
    advantage of this approach is the speed of allocation. Another is the simplicity
    and efficiency of deallocation if it happens in FIFO order or affects the whole
    allocation space. Stack memory allocations are later released in the order that’s
    reverse to allocation, so this is the most prominent and the most efficient example
    of such an approach.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的代码，内存分配器可以执行类似操作。在其最简单形式中，它会通过移动一个指向下一个空闲指针来分配下一块内存。释放分配只需要将分配区域标记为已释放。这种方法的优点是分配速度快。另一个优点是，如果按照先进先出（FIFO）顺序进行或影响整个分配空间，则释放的简单性和效率。堆栈内存分配在释放时按照与分配相反的顺序进行，因此这是此类方法最突出且最有效率的例子。
- en: Using linear allocators as general-purpose allocators can be more problematic
    because of the difficulty of space reclamation. To reclaim space, it’s not enough
    to just mark entries as free. This leads to memory fragmentation, which in turn
    outweighs the advantages of linear allocation. So, as with the filesystem, the
    memory must be reclaimed so that it only contains allocated entries and the free
    space can be used again. Reclamation requires moving allocated entries around—a
    process that changes and invalidates their previously known addresses. In naive
    code, the locations of references to allocated entries (addresses stored as pointers)
    are unknown to the allocator. Existing references would have to be patched to
    make the allocator action transparent to the caller; that’s not feasible for a
    general-purpose allocator like malloc. Logging allocator use is tied to the programming
    language selection. Some RTTIs, like C++, can greatly facilitate this by providing
    move-constructors. However, passing pointers to libraries that are outside of
    your control (e.g., glibc) would still be an issue.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性分配器作为通用分配器可能会更成问题，因为空间回收困难。要回收空间，仅仅将条目标记为空闲是不够的。这会导致内存碎片化，从而抵消了线性分配的优势。因此，与文件系统一样，必须回收内存，使其只包含已分配条目，而空闲空间可以再次使用。回收需要移动已分配条目——这个过程会改变并使它们之前已知的地址无效。在原始代码中，分配条目的引用位置（存储为指针的地址）对分配器来说是未知的。现有的引用必须进行修补，以便使分配器操作对调用者透明；对于像malloc这样的通用分配器来说，这是不可行的。记录分配器使用与编程语言选择有关。一些RTTIs，如C++，可以通过提供移动构造函数来极大地简化这一点。然而，传递指向您无法控制的库（例如，glibc）的指针仍然是一个问题。
- en: Another alternative is adopting a strategy of pool allocators, which provide
    allocation spaces for allocation of entries of a fixed size (see Figure [3-3](#Fig3)).
    By limiting the allocation space that way, fragmentation can be reduced. A number
    of general-purpose allocators use pool allocators for small allocations. In some
    cases, those application spaces exist on a per-thread basis to eliminate the need
    for locking and improve CPU cache utilization.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个替代方案是采用池分配器的策略，它为固定大小的条目分配提供分配空间（见图[3-3](#Fig3)）。通过这种方式限制分配空间，可以减少碎片化。许多通用分配器使用池分配器进行小规模分配。在某些情况下，这些应用程序空间基于每个线程存在，以消除锁定的需要并提高CPU缓存利用率。
- en: '![](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig3_HTML.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图3-3](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig3_HTML.jpg)'
- en: An illustration of the allocation of data to arrays of slots. The three types
    of pool allocation arrays are full, partial, and free. The freed object slots
    are shaded.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分配到槽位数组的示例。三种类型的池分配数组是满的、部分满的和空的。释放的对象槽位被阴影覆盖。
- en: Figure 3-3
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-3
- en: Pool allocators provide allocation spaces for allocation of entries of a fixed
    size. Fragmentation is reduced by limiting the allocation space
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 池分配器为固定大小的条目分配提供分配空间。通过限制分配空间来减少碎片化
- en: 'This pool allocation strategy provides two core benefits. First, it saves you
    from having to search for available memory space. Second, it alleviates memory
    fragmentation because it pre-allocates in memory a cache for use with a collection
    of object sizes. Here’s how it works to achieve that:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种池分配策略提供了两个核心好处。首先，它让您免于搜索可用内存空间。其次，它通过在内存中预先分配用于一组对象大小的缓存来减轻内存碎片化。以下是它是如何实现这一点的：
- en: The region for each of the sizes has fixed-size memory chunks that are suitable
    for the contained objects, and those chunks are all tracked by the allocator.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个大小区域都有固定大小的内存块，这些内存块适合包含的对象，并且这些块都由分配器跟踪。
- en: When it’s time for the allocator to allocate memory for a certain type of data
    object, it’s typically possible to use a free slot (chunk) in one of the existing
    memory slabs.^([3](#Fn3))
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当分配器需要为某种类型的数据对象分配内存时，通常可以使用现有内存slab中的一个空闲槽位（块）。^([3](#Fn3))
- en: When it’s time for the allocator to free the object’s memory, it can simply
    move that slot over to the containing slab’s list of unused/free memory slots.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当分配器需要释放对象的内存时，它可以将该槽位简单地移动到包含的slab的未使用/空闲内存槽位列表中。
- en: That memory slot (or some other free slot) will be removed from the list of
    free slots whenever there’s a call to create an object of the same type (or a
    call to allocate memory of the same size).
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当调用创建相同类型的对象（或调用分配相同大小的内存）时，该内存槽位（或某些其他空闲槽位）将从空闲槽位列表中移除。
- en: The best allocation approach to pick heavily depends on the usage scenario.
    One great benefit of a log-structured approach is that it handles fragmentation
    of small sub-pools in a more efficient way. Pool allocators, on the other hand,
    generate less background load on the CPU because of the lack of compacting activity.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的分配方法很大程度上取决于使用场景。日志结构化方法的一个巨大好处是它以更有效的方式处理了小子池的碎片化。另一方面，池分配器由于缺乏压缩活动，对CPU的背景负载较少。
- en: Cache Control
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缓存控制
- en: When it comes to memory management in a software application that stores lots
    of data on disk, you cannot overlook the topic of cache control. Caching is always
    a must in data processing, and it’s crucial to decide what and where to cache.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到存储大量数据在磁盘上的软件应用程序的内存管理时，你不能忽视缓存控制这个话题。在数据处理中，缓存总是必须的，并且决定缓存什么和在哪里缓存是至关重要的。
- en: If caching is done at the I/O level, for both read/write and mmap, caching can
    become the responsibility of the kernel. The majority of the system’s memory is
    given over to the page cache. The kernel decides which pages should be evicted
    when memory runs low, decides when pages need to be written back to disk, and
    controls read-ahead. The application can provide some guidance to the kernel using
    the `madvise(2)` and `fadvise(2)` system calls.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果缓存是在I/O级别进行的，无论是读取/写入还是mmap，缓存就可能成为内核的责任。系统的大部分内存都分配给了页面缓存。内核决定在内存不足时哪些页面应该被移除，决定何时需要将页面写回磁盘，并控制预读。应用程序可以通过使用`madvise(2)`和`fadvise(2)`系统调用来向内核提供一些指导。
- en: The main advantage of letting the kernel control caching is that great effort
    has been invested by the kernel developers over many decades into tuning the algorithms
    used by the cache. Those algorithms are used by thousands of different applications
    and are generally effective. The disadvantage, however, is that these algorithms
    are general-purpose and not tuned to the application. The kernel must guess how
    the application will behave next. Even if the application knows differently, it
    usually has no way to help the kernel guess correctly. This results in the wrong
    pages being evicted, I/O scheduled in the wrong order, or read-ahead scheduled
    for data that will not be consumed in the near future.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让内核控制缓存的优点是内核开发者已经投入了数十年的努力来调整缓存所使用的算法。这些算法被成千上万的不同的应用程序使用，并且通常是有效的。然而，缺点是这些算法是通用性的，并没有针对特定应用程序进行优化。内核必须猜测应用程序下一步的行为。即使应用程序知道不同的情况，通常也没有办法帮助内核做出正确的猜测。这导致错误的页面被移除，I/O被错误地调度，或者为不久的将来不会消费的数据安排了预读。
- en: Next, doing the caching at the I/O level interacts with the topic often referred
    to as IMR—*in memory representation*. No wonder that the format in which data
    is stored on disk differs from the form the same data is allocated in memory as
    objects. The simplest reason that it’s not the same is byte-ordering. With that
    in mind, if the data is cached once it’s read from the disk, it needs to be further
    converted or parsed into the object used in memory. This can be a waste of CPU
    cycles, so applications may choose to cache at the object level.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在I/O级别进行缓存与通常称为IMR（内存表示）的话题有关。不奇怪的是，数据在磁盘上存储的格式与相同数据在内存中作为对象分配的格式不同。最简单的理由是字节序。考虑到这一点，如果数据从磁盘读取后缓存，它需要进一步转换或解析成内存中使用的对象。这可能会浪费CPU周期，因此应用程序可能会选择在对象级别进行缓存。
- en: Choosing to cache at the object level affects a lot of other design points.
    With that, the cache management is all on the application side including cross-core
    synchronization, data coherence, invalidation, and so on. Next, since objects
    can be (and typically are) much smaller than the average I/O size, caching millions
    and billions of those objects requires a collection selection that can handle
    it (you’ll learn about this quite soon). Finally, caching on the object level
    greatly affects the way I/O is done.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 选择在对象级别进行缓存会影响许多其他设计点。因此，缓存管理完全在应用端，包括跨核心同步、数据一致性、失效等。接下来，由于对象可以（并且通常）比平均I/O大小小得多，缓存数百万甚至数十亿个这样的对象需要一个能够处理它的集合选择（你很快就会了解到这一点）。最后，对象级别的缓存极大地影响了I/O的执行方式。
- en: I/O
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I/O
- en: 'Unless the database engine is an in-memory one, it will have to keep the data
    on external storage. There can be many options to do that, including local disks,
    network-attached storage, distributed file- and object- storage systems, and so
    on. The term “I/O” typically refers to accessing data on local storage—disks or
    filesystems (that, in turn, are located on disks as well). And in general, there
    are four choices for accessing files on a Linux server: read/write, mmap, Direct
    I/O (DIO) read/write, and Asynchronous I/O (AIO/DIO, because this I/O is rarely
    used in cached mode).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除非数据库引擎是内存中的，否则它必须在外部存储上保留数据。这可以有多种选择，包括本地磁盘、网络附加存储、分布式文件和对象存储系统等等。术语“I/O”通常指的是访问本地存储上的数据——磁盘或文件系统（这些文件系统本身也位于磁盘上）。一般来说，在Linux服务器上访问文件有四种选择：读写、mmap、直接I/O（DIO）读写和异步I/O（AIO/DIO，因为这种I/O很少在缓存模式下使用）。
- en: Traditional Read/Write
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传统读写
- en: The traditional method is to use the `read(2`*)* and `write(2)` system calls.
    In a modern implementation, the read system call (or one of its many variants—`pread`,
    `readv`, `preadv`, etc.) asks the kernel to read a section of a file and copy
    the data into the calling process address space. If all of the requested data
    is in the page cache, the kernel will copy it and return immediately; otherwise,
    it will arrange for the disk to read the requested data into the page cache, block
    the calling thread, and when the data is available, it will resume the thread
    and copy the data. A write, on the other hand, will usually¹ just copy the data
    into the page cache; the kernel will write back the page cache to disk some time
    afterward.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的做法是使用`read(2`*)* 和 `write(2)`系统调用。在现代实现中，读取系统调用（或其许多变体之一——`pread`、`readv`、`preadv`等）请求内核读取文件的一部分并将数据复制到调用进程的地址空间。如果所有请求的数据都在页面缓存中，内核将复制它并立即返回；否则，它将安排磁盘读取请求的数据到页面缓存，阻塞调用线程，当数据可用时，它将恢复线程并复制数据。另一方面，写入通常¹只是将数据复制到页面缓存；内核将在之后某个时间将页面缓存写回磁盘。
- en: mmap
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: mmap
- en: An alternative and more modern method is to memory-map the file into the application
    address space using the `mmap(2)` system call. This causes a section of the address
    space to refer directly to the page cache pages that contain the file’s data.
    After this preparatory step, the application can access file data using the processor’s
    memory read and memory write instructions. If the requested data happens to be
    in cache, the kernel is completely bypassed and the read (or write) is performed
    at memory speed. If a cache miss occurs, then a page-fault happens and the kernel
    puts the active thread to sleep while it goes off to read the data for that page.
    When the data is finally available, the memory-management unit is programmed so
    the newly read data is accessible to the thread, which is then awoken.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代的更现代的方法是使用`mmap(2)`系统调用将文件内存映射到应用程序地址空间。这导致地址空间的一部分直接引用包含文件数据的页面缓存页。在此准备步骤之后，应用程序可以使用处理器的内存读取和内存写入指令访问文件数据。如果请求的数据恰好位于缓存中，内核将被完全绕过，并且以内存速度执行读取（或写入）。如果发生缓存未命中，则发生页面错误，内核将挂起活动线程，然后去读取该页的数据。当数据最终可用时，内存管理单元被编程，以便新读取的数据对线程可用，然后线程被唤醒。
- en: Direct I/O (DIO)
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接I/O (DIO)
- en: 'Both traditional read/write and mmap involve the kernel page cache and defer
    the scheduling of I/O to the kernel. When the application wants to schedule I/O
    itself (for reasons that we will explain later), it can use Direct I/O, as shown
    in Figure [3-4](#Fig4). This involves opening the file with the `O_DIRECT` flag;
    further activity will use the normal read and write family of system calls. However,
    their behavior is now altered: Instead of accessing the cache, the disk is accessed
    directly, which means that the calling thread will be put to sleep unconditionally.
    Furthermore, the disk controller will copy the data directly to userspace, bypassing
    the kernel.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的读写和mmap都涉及内核页面缓存并将I/O调度推迟到内核。当应用程序想要自己调度I/O（原因我们稍后解释）时，可以使用直接I/O，如图[3-4](#Fig4)所示。这涉及到使用`O_DIRECT`标志打开文件；进一步的活动将使用正常的读写系统调用。然而，它们的行为现在已改变：不再是访问缓存，而是直接访问磁盘，这意味着调用线程将被无条件地挂起。此外，磁盘控制器将直接将数据复制到用户空间，绕过内核。
- en: '![](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig4_HTML.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图3-4](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig4_HTML.jpg)'
- en: An illustration of input and output of app, thread, kernel, and disk. The app
    reads and writes data to the kernel. The D M A data and the context switch are
    used between the app and the disk.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序、线程、内核和磁盘的输入和输出示意图。应用程序将数据读入和写入内核。应用程序和磁盘之间使用DMA数据和上下文切换。
- en: Figure 3-4
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-4
- en: Direct I/O involves opening the file with the O_DIRECT flag; further activity
    will use the normal read and write family of system calls, but their behavior
    is now altered
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 直接I/O涉及使用O_DIRECT标志打开文件；进一步的活动将使用正常的读写系统调用系列，但它们的行为现在已改变
- en: Asynchronous I/O (AIO/DIO)
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异步I/O (AIO/DIO)
- en: A refinement of Direct I/O, Asynchronous Direct I/O, behaves similarly but prevents
    the calling thread from blocking (see Figure [3-5](#Fig5)). Instead, the application
    thread schedules Direct I/O operations using the `io_submit(2)` system call, but
    the thread is not blocked; the I/O operation runs in parallel with normal thread
    execution. A separate system call, `io_getevents(2)`, waits for and collects the
    results of completed I/O operations. Like DIO, the kernel’s page cache is bypassed,
    and the disk controller is responsible for copying the data directly to userspace.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 直接I/O的改进版，异步直接I/O，行为类似但防止调用线程阻塞（见图[3-5](#Fig5)）。相反，应用程序线程使用`io_submit(2)`系统调用安排直接I/O操作，但线程不会被阻塞；I/O操作与正常线程执行并行运行。一个单独的系统调用`io_getevents(2)`等待并收集完成的I/O操作的结果。与DIO一样，内核的页面缓存被绕过，磁盘控制器负责将数据直接复制到用户空间。
- en: '![](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig5_HTML.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig5_HTML.jpg)'
- en: An illustration of input and output of app, thread, kernel, and disk. The app
    reads and writes data to the kernel. The D M A data is used between the app and
    the disk.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序、线程、内核和磁盘的输入和输出示意图。应用程序将数据读入和写入内核。应用程序和磁盘之间使用DMA数据。
- en: Figure 3-5
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-5
- en: A refinement of Direct I/O, Asynchronous Direct I/O behaves similarly but prevents
    the calling thread from blocking
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 直接I/O的改进版，异步直接I/O行为类似但防止调用线程阻塞
- en: 'Note: io_uring'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：io_uring
- en: The API to perform asynchronous I/O appeared in Linux long ago, and it was warmly
    met by the community. However, as it often happens, real-world usage quickly revealed
    many inefficiencies, such as blocking under some circumstances (despite the name),
    the need to call the kernel too often, and poor support for canceling the submitted
    requests. Eventually, it became clear that the updated requirements were not compatible
    with the existing API and the need for a new one arose.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 执行异步I/O的API在Linux中很久以前就出现了，并且受到了社区的欢迎。然而，正如经常发生的那样，实际使用很快揭示了许多低效之处，例如在某些情况下会阻塞（尽管名称上不是这样），需要频繁调用内核，以及提交请求取消支持不佳。最终，很明显，更新的要求与现有的API不兼容，因此需要一个新的API。
- en: This is how the `io_uring()` API appeared. It provides the same facilities as
    AIO does, but in a much more convenient and performant way (it also has notably
    better documentation). Without diving into implementation details, let’s just
    say that it exists and is preferred over the legacy AIO.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`io_uring()` API出现的原因。它提供了与AIO相同的设施，但以更方便、更高效的方式（它还具有显著更好的文档）。不深入实现细节，只需说它存在，并且比传统的AIO更受欢迎。
- en: Understanding the Tradeoffs
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解权衡
- en: The different access methods share some characteristics and differ in others.
    Table [3-1](#Tab1) summarizes these characteristics, which are discussed further
    in this section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的访问方法在某些方面具有一些共同特性，在其他方面则有所不同。表[3-1](#Tab1)总结了这些特性，这些特性将在本节中进一步讨论。
- en: Table 3-1
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-1
- en: Comparing Different I/O Access Methods
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同的I/O访问方法
- en: '| Characteristic | R/W | mmap | DIO | AIO/DIO |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 特性 | R/W | mmap | DIO | AIO/DIO |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Cache control | Kernel | Kernel | User | User |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 缓存控制 | 内核 | 内核 | 用户 | 用户 |'
- en: '| Copying | Yes | No | No | No |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 复制 | 是 | 否 | 否 | 否 |'
- en: '| MMU activity | Low | High | None | None |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| MMU活动 | 低 | 高 | 无 | 无 |'
- en: '| I/O scheduling | Kernel | Kernel | Mixed | User |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| I/O调度 | 内核 | 内核 | 混合 | 用户 |'
- en: '| Thread scheduling | Kernel | Kernel | Kernel | User |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 线程调度 | 内核 | 内核 | 内核 | 用户 |'
- en: '| I/O alignment | Automatic | Automatic | Manual | Manual |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| I/O对齐 | 自动 | 自动 | 手动 | 手动 |'
- en: '| Application complexity | Low | Low | Moderate | High |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 应用复杂性 | 低 | 低 | 中等 | 高 |'
- en: Copying and MMU Activity
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 复制和MMU活动
- en: One of the benefits of the mmap method is that if the data is in cache, then
    the kernel is bypassed completely. The kernel does not need to copy data from
    the kernel to userspace and back, so fewer processor cycles are spent on that
    activity. This benefits workloads that are mostly in cache (for example, if the
    ratio of storage size to RAM size is close to 1:1).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: mmap 方法的一个好处是，如果数据在缓存中，则内核可以完全绕过。内核不需要在内核空间和用户空间之间复制数据，因此可以节省处理器周期用于这项活动。这对主要在缓存中的工作负载有益（例如，如果存储大小与
    RAM 大小的比例接近 1:1）。
- en: The downside of mmap, however, occurs when data is not in the cache. This usually
    happens when the ratio of storage size to RAM size is significantly higher than
    1:1\. Every page that is brought into the cache causes another page to be evicted.
    Those pages have to be inserted into and removed from the page tables; the kernel
    has to scan the page tables to isolate inactive pages, making them candidates
    for eviction, and so forth. In addition, mmap requires memory for the page tables.
    On x86 processors, this requires 0.2 percent of the size of the mapped files.
    This seems low, but if the application has a 100:1 ratio of storage to memory,
    the result is that 20 percent of memory (0.2% * 100) is devoted to page tables.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，mmap 的缺点在于数据不在缓存中时。这通常发生在存储大小与 RAM 大小的比例显著高于 1:1 的情况下。每个被带入缓存的一页都会导致另一页被驱逐。这些页面需要被插入和从页面表中移除；内核必须扫描页面表以隔离不活跃的页面，使它们成为驱逐的候选者，等等。此外，mmap
    需要内存用于页面表。在 x86 处理器上，这需要映射文件大小的 0.2%。这似乎很低，但如果应用程序的存储与内存的比例为 100:1，那么结果是 20% 的内存（0.2%
    * 100）被用于页面表。
- en: I/O Scheduling
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: I/O 调度
- en: 'One of the problems with letting the kernel control caching (with the mmap
    and read/write access methods) is that the application loses control of I/O scheduling.
    The kernel picks whichever block of data it deems appropriate and schedules it
    for write or read. This can result in the following problems:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让内核控制缓存（使用 mmap 和读写访问方法）的一个问题是，应用程序失去了对 I/O 调度的控制。内核选择它认为合适的任何数据块，并为其安排写入或读取。这可能导致以下问题：
- en: '**A write storm.** When the kernel schedules large amounts of writes, the disk
    will be busy for a long while and impact read latency.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**写风暴**。当内核调度大量写入时，磁盘将长时间忙碌，影响读取延迟。'
- en: '**The kernel cannot distinguish between “important” and “unimportant” I/O.**
    I/O belonging to background tasks can overwhelm foreground tasks, impacting their
    latency²'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内核无法区分“重要”和“不重要”的 I/O**。属于后台任务的 I/O 可能会压倒前台任务，影响其延迟²'
- en: By bypassing the kernel page cache, the application takes on the burden of scheduling
    I/O. This doesn’t mean that the problems are solved, but it does mean that the
    problems *can* be solved—with sufficient attention and effort.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绕过内核页面缓存，应用程序承担了调度 I/O 的负担。这并不意味着问题已经解决，但确实意味着问题*可以*解决——只要给予足够的关注和努力。
- en: When using Direct I/O, each thread controls when to issue I/O. However, the
    kernel controls when the thread runs, so responsibility for issuing I/O is shared
    between the kernel and the application. With AIO/DIO, the application is in full
    control of when I/O is issued.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用直接 I/O 时，每个线程控制何时发起 I/O。然而，内核控制线程的运行时间，因此发起 I/O 的责任在内核和应用程序之间共享。使用 AIO/DIO，应用程序完全控制
    I/O 的发起时间。
- en: Thread Scheduling
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线程调度
- en: An I/O intensive application using mmap or read/write cannot guess what its
    cache hit rate will be. Therefore, it has to run a large number of threads (significantly
    larger than the core count of the machine it is running on). Using too few threads,
    they may all be waiting for the disk leaving the processor underutilized. Since
    each thread usually has at most one disk I/O outstanding, the number of running
    threads must be around the concurrency of the storage subsystem multiplied by
    some small factor in order to keep the disk fully occupied. However, if the cache
    hit rate is sufficiently high, then these large numbers of threads will contend
    with each other for the limited number of cores.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 mmap 或读写操作的 I/O 密集型应用程序无法预测其缓存命中率。因此，它必须运行大量线程（显著大于其运行的机器的核心数）。使用线程过少，它们可能都在等待磁盘，导致处理器利用率低下。由于每个线程通常最多只有一个磁盘
    I/O 正在执行，因此运行线程的数量必须大约是存储子系统并发性的乘以某个小因子，以便使磁盘保持完全占用。然而，如果缓存命中率足够高，那么这些大量线程将相互竞争有限的核数。
- en: When using Direct I/O, this problem is somewhat mitigated. The application knows
    exactly when a thread is blocked on I/O and when it can run, so the application
    can adjust the number of running threads according to runtime conditions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用直接I/O时，这个问题得到了一定程度的缓解。应用程序确切地知道何时线程因I/O而被阻塞，何时可以运行，因此应用程序可以根据运行时条件调整运行线程的数量。
- en: With AIO/DIO, the application has full control over both running threads and
    waiting I/O (the two are completely divorced), so it can easily adjust to in-memory
    or disk-bound conditions or anything in between.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AIO/DIO时，应用程序对运行线程和等待I/O（这两者完全分离）都有完全的控制权，因此它可以很容易地调整到内存密集型或磁盘密集型条件或两者之间的任何条件。
- en: I/O Alignment
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: I/O对齐
- en: Storage devices have a block size; all I/O must be performed in multiples of
    this block size which is typically 512 or 4096 bytes. Using read/write or mmap,
    the kernel performs the alignment automatically; a small read or write is expanded
    to the correct block boundary by the kernel before it is issued.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 存储设备有块大小；所有I/O都必须以这个块大小的倍数进行，这通常是512或4096字节。使用读写或mmap，内核会自动执行对齐；内核会在发出之前将小的读取或写入扩展到正确的块边界。
- en: 'With DIO, it is up to the application to perform block alignment. This incurs
    some complexity, but also provides an advantage: The kernel will usually over-align
    to a 4096 byte boundary even when a 512-byte boundary suffices. However, a user
    application using DIO can issue 512-byte aligned reads, which results in saving
    bandwidth on small items.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DIO时，块对齐的责任由应用程序承担。这带来了一些复杂性，但也提供了一些优势：内核通常会过度对齐到4096字节边界，即使512字节边界就足够了。然而，使用DIO的用户应用程序可以发出512字节对齐的读取操作，这在小项上可以节省带宽。
- en: Application Complexity
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用程序复杂性
- en: 'While the previous discussions favored AIO/DIO for I/O intensive applications,
    that method comes with a significant cost: complexity. Placing the responsibility
    of cache management on the application means it can make better choices than the
    kernel and make those choices with less overhead. However, those algorithms need
    to be written and tested. Using asynchronous I/O requires that the application
    is written using callbacks, coroutines, or a similar method, and often reduces
    the reusability of many available libraries.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然之前的讨论倾向于为I/O密集型应用选择AIO/DIO，但这种方法伴随着一个显著的成本：复杂性。将缓存管理的责任放在应用程序上意味着它可以做出比内核更好的选择，并且以更低的开销做出这些选择。然而，这些算法需要编写和测试。使用异步I/O要求应用程序使用回调、协程或类似的方法编写，这通常降低了许多可用库的可重用性。
- en: Choosing the Filesystem and/or Disk
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择文件系统或磁盘
- en: Beyond performing the I/O itself, the database design must consider the medium
    against which this I/O is done. In many cases, the choice is often between a filesystem
    or a raw block device, which in turn can be a choice of a traditional spinning
    disk or an SSD drive. In cloud environments, however, there can be the third option
    because local drives are always ephemeral—which imposes strict requirements on
    the replication.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了执行I/O本身之外，数据库设计还必须考虑执行此I/O的介质。在许多情况下，选择通常是文件系统或原始块设备，这反过来又可以是传统旋转磁盘或SSD驱动器的选择。然而，在云环境中，可能还有第三个选项，因为本地驱动器总是短暂的——这给复制带来了严格的要求。
- en: Filesystems vs Raw Disks
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件系统与原始磁盘
- en: 'This decision can be approached from two angles: management costs and performance.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策可以从两个角度来考虑：管理成本和性能。
- en: If you’re accessing the storage as a raw block device, all the difficulties
    with block allocation and reclamation are on the application side. We touched
    on this topic slightly earlier when we talked about memory management. The same
    set of challenges apply to RAM as well as disks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将存储作为原始块设备访问，所有与块分配和回收相关的困难都在应用程序一侧。我们之前在讨论内存管理时略微提到了这个话题。相同的挑战也适用于RAM和磁盘。
- en: A connected, though very different, challenge is providing data integrity in
    case of crashes. Unless the database is purely in-memory, the I/O should be done
    in a way that avoids losing data or reading garbage from disk after a restart.
    Modern filesystems, however, provide both and are very mature to trust the efficiency
    of allocations and integrity of data. Accessing raw block devices unfortunately
    lacks those features (so they need to be implemented at the same quality on the
    application side).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相关但非常不同的挑战是在发生崩溃的情况下提供数据完整性。除非数据库完全是内存中的，否则I/O操作应该以避免在重启后丢失数据或从磁盘读取垃圾数据的方式进行。然而，现代文件系统提供了这两者，并且非常成熟，可以信赖其分配效率和数据的完整性。不幸的是，访问原始块设备却缺少这些功能（因此它们需要在应用层面以相同的质量实现）。
- en: From the performance point of view, the difference is not that drastic. On one
    hand, writing data to a file is always accompanied by associated metadata updates.
    This consumes both disk space and I/O bandwidth. However, some modern filesystems
    provide a very good balance of performance and efficiency, almost eliminating
    the I/O latency. (One of the most prominent examples is XFS. Another really good
    and mature piece of software is Ext4). The great ally in this camp is the `fallocate(2)`
    system call that makes the filesystem preallocate space on disk. When used, filesystems
    also have a chance to make full use of the extents mechanisms, thus bringing the
    QoS of using files to the same performance level as when using raw block devices.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能角度来看，这种差异并不那么剧烈。一方面，将数据写入文件总是伴随着相关的元数据更新。这消耗了磁盘空间和I/O带宽。然而，一些现代文件系统提供了非常好的性能和效率的平衡，几乎消除了I/O延迟。（最突出的例子之一是XFS。另一个非常好且成熟的软件是Ext4）。在这个领域中的强大盟友是`fallocate(2)`系统调用，它使文件系统在磁盘上预先分配空间。当使用时，文件系统也有机会充分利用范围机制，从而将使用文件的质量服务提升到与使用原始块设备相同的表现水平。
- en: Appending Writes
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 追加写入
- en: The database may have a heavy reliance on appends to files or require in-place
    updates of individual file blocks. Both approaches need special attention from
    the system architect because they call for different properties from the underlying
    system.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库可能对文件的追加操作有很强的依赖性，或者需要单个文件块的就地更新。这两种方法都需要系统架构师给予特别的关注，因为它们对底层系统提出了不同的要求。
- en: On one hand, appending writes requires careful interaction with the filesystem
    so that metadata updates (file size, in particular) do not dominate the regular
    I/O. On the other hand, appending writes (being sort of cache-oblivious algorithms)
    handle the disk overwriting difficulties in a natural manner. Contrary to this,
    in-place updates cannot happen at random offsets and sizes because disks may not
    tolerate this kind of workload, even if they’re used in a raw block device manner
    (not via a filesystem).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，追加写入需要与文件系统进行仔细的交互，以确保元数据更新（尤其是文件大小）不会主导常规I/O。另一方面，追加写入（作为一种某种程度上无缓存意识的算法）以自然的方式处理磁盘覆盖困难。与此相反，就地更新不能在随机的偏移量和大小处发生，因为磁盘可能无法容忍这种类型的工作负载，即使它们是以原始块设备的方式使用（而不是通过文件系统）。
- en: That being said, let’s dive even deeper into the stack and descend into the
    hardware level.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们更深入地探讨堆栈，并深入到硬件层面。
- en: How Modern SSDs Work
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何现代固态硬盘工作
- en: Like other computational resources, disks are limited in the speed they can
    provide. This speed is typically measured as a two-dimensional value with *Input/Output
    Operations per Second* (IOPS) and *bytes per second* (throughput). Of course,
    these parameters are not cut in stone even for each particular disk, and the maximum
    number of requests or bytes greatly depends on the requests’ distribution, queuing
    and concurrency, buffering or caching, disk age, and many other factors. So when
    performing I/O, a disk must always balance between two inefficiencies—overwhelming
    the disk with requests and underutilizing it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 就像其他计算资源一样，磁盘在提供速度方面是有限的。这个速度通常以每秒的二维值来衡量，即*每秒输入/输出操作数*（IOPS）和*每秒字节数*（吞吐量）。当然，即使是对于每个特定的磁盘，这些参数也不是一成不变的，最大请求数或字节数很大程度上取决于请求的分布、排队和并发、缓冲或缓存、磁盘年龄以及许多其他因素。因此，在进行I/O操作时，磁盘必须始终在两种低效状态之间取得平衡——请求过多压垮磁盘和磁盘利用率不足。
- en: Overwhelming the disk should be avoided because when the disk is full of requests
    it cannot distinguish between the criticality of certain requests over others.
    Of course, all requests are important, but it makes sense to prioritize latency-sensitive
    requests. For example, ScyllaDB serves real-time queries that need to be completed
    in single-digit milliseconds or less and, in parallel, it processes terabytes
    of data for compaction, streaming, decommission, and so forth. The former have
    strong latency sensitivity; the latter are less so. Good I/O maintenance that
    tries to maximize the I/O bandwidth while keeping latency as low as possible for
    latency-sensitive tasks is complicated enough to become a standalone component
    called the *I/O Scheduler*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 应避免过度负载磁盘，因为当磁盘充满请求时，它无法区分某些请求相对于其他请求的重要性。当然，所有请求都很重要，但优先处理对延迟敏感的请求是有意义的。例如，ScyllaDB提供需要以个位数毫秒或更少的时间完成的实时查询，同时并行处理用于压缩、流式传输、退役等操作的数据。前者对延迟敏感度强；后者则不那么敏感。试图在保持延迟敏感任务延迟尽可能低的同时最大化I/O带宽的良好I/O维护变得足够复杂，以至于成为一个独立的组件，称为*I/O
    Scheduler*。
- en: When evaluating a disk, you would most likely be looking at its four parameters—read/write
    IOPS and read/write throughput (such as in MB/s). Comparing these numbers to one
    another is a popular way of claiming one disk is better than the other and estimating
    the aforementioned “bandwidth capacity” of the drive by applying Little’s Law.
    With that, the I/O Scheduler’s job is to provide a certain level of concurrency
    inside the disk to get maximum bandwidth from it, but not to make this concurrency
    too high in order to prevent the disk from queueing requests internally for longer
    than needed.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估磁盘时，你很可能会查看其四个参数——读写IOPS和读写吞吐量（如MB/s）。将这些数字相互比较是声称一个磁盘比另一个磁盘更好的一种流行方式，并通过应用Little's
    Law来估计所提到的“带宽容量”。因此，I/O调度器的任务是向磁盘提供一定程度的并发性，以获得最大的带宽，但不要使这种并发性过高，以防止磁盘内部队列请求的时间超过所需时间。
- en: For instance, Figure [3-6](#Fig6) illustrates how read request latency depends
    on the intensity of small reads (challenging disk IOPS capacity) vs the intensity
    of large writes (pursuing the disk bandwidth). The latency value is color-coded,
    and the “interesting area” is painted in cyan—this is where the latency stays
    below 1 millisecond. The drive measured is the NVMe disk that comes with the AWS
    EC2 i3en.3xlarge instance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图[3-6](#Fig6)说明了读取请求延迟如何取决于小读取的强度（挑战磁盘IOPS容量）与大型写入的强度（追求磁盘带宽）。延迟值用颜色编码，而“有趣区域”用青色标出——这是延迟保持在1毫秒以下的地方。所测量的驱动器是随AWS
    EC2 i3en.3xlarge实例提供的NVMe磁盘。
- en: '![](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig6_HTML.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/541783_1_En_3_Chapter/541783_1_En_3_Fig6_HTML.jpg)'
- en: Two graphs of p 50 and p 95 latency from 0 to 1 G B per second. The graphs depict
    a decreasing trend. Two shaded strips are given to the right of the graphs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 两个从0到1 G B每秒的p 50和p 95延迟图。图显示了下降趋势。在图的右侧给出了两个阴影条带。
- en: Figure 3-6
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-6
- en: Bandwidth/latency graphs showing how read request latency depends on the intensity
    of small reads (challenging disk IOPS capacity) vs the intensity of large writes
    (pursuing the disk bandwidth)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 显示读取请求延迟如何取决于小读取的强度（挑战磁盘IOPS容量）与大型写入的强度（追求磁盘带宽）的带宽/延迟图
- en: This drive demonstrates almost perfect half-duplex behavior—increasing the read
    intensity several times requires roughly the same reduction in write intensity
    to keep the disk operating at the same speed.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 该驱动器展示了几乎完美的半双工行为——增加读取强度几倍需要大约相同程度的写入强度减少，以保持磁盘以相同的速度运行。
- en: 'Tip: How to Measure Your Own Disk Behavior Under Load'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：如何测量你的磁盘在负载下的行为
- en: The better you understand how your own disks perform under load, the better
    you can tune them to capitalize on their “sweet spot.” One way to do this is with
    Diskplorer,^([4](#Fn4)) an open-source disk latency/bandwidth exploring toolset.
    By using Linux fio under the hood it runs a battery of measurements to discover
    performance characteristics for a specific hardware configuration, giving you
    an at-a-glance view of how server storage I/O will behave under load.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你对自己的磁盘在负载下的性能理解得越好，你就能更好地调整它们以利用它们的“最佳性能点”。实现这一目标的一种方法是用Diskplorer^([4](#Fn4))，这是一个开源的磁盘延迟/带宽探索工具集。通过在底层使用Linux
    fio，它运行一系列测量以发现特定硬件配置的性能特征，让你一目了然地看到服务器存储I/O在负载下的行为。
- en: For a walkthrough of how to use this tool, see the Linux Foundation video, “Understanding
    Storage I/O Under Load.”^([5](#Fn5))
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何使用此工具的教程，请参阅Linux基金会视频，“在负载下理解存储I/O。”^([5](#Fn5))
- en: Networking
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络
- en: The conventional networking functionality available in Linux is remarkably full-featured,
    mature, and performant. Since the database rarely imposes severe per-ping latency
    requirements, there are very few surprises that come from it when properly configured
    and used. Nonetheless, some considerations still need to be made.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Linux中可用的传统网络功能非常全面、成熟且性能出色。由于数据库很少对每个ping施加严重的延迟要求，因此当正确配置和使用时，很少会有惊喜。尽管如此，仍需考虑一些因素。
- en: As explained by David Ahern, “Linux will process a fair amount of packets in
    the context of whatever is running on the CPU at the moment the IRQ is handled.
    System accounting will attribute those CPU cycles to any process running at that
    moment even though that process is not doing any work on its behalf. For example,
    ‘top’ can show a process that appears to be using 99+% CPU, but in reality, 60
    percent of that time is spent processing packets—meaning the process is really
    only getting 40 percent of the CPU to make progress on its workload.”^([6](#Fn6))
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如大卫·阿赫尔所述，“Linux将在处理中断时处理一定数量的数据包，这些数据包是在处理中断时CPU上正在运行的任何进程的上下文中。系统会计将那些CPU周期归因于当时运行的任何进程，即使该进程没有为其做任何工作。例如，‘top’可以显示一个看似使用99%以上CPU的进程，但实际上，60%的时间是用于处理数据包——这意味着该进程实际上只有40%的CPU用于在其工作负载上取得进展。”^([6](#Fn6))
- en: 'However, for truly networking-intensive applications, the Linux stack is constrained:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于真正密集型网络应用，Linux 栈受到限制：
- en: '**Kernel space implementation:** Separation of the network stack into kernel
    space means that costly context switches are needed to perform network operations,
    and that data copies must be performed to transfer data from kernel buffers to
    user buffers and vice versa.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内核空间实现**：将网络栈分离到内核空间意味着执行网络操作需要昂贵的上下文切换，并且必须执行数据复制以将数据从内核缓冲区传输到用户缓冲区，反之亦然。'
- en: '**Time sharing:** Linux is a time-sharing system, and so must rely on slow,
    expensive interrupts to notify the kernel that there are new packets to be processed.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间共享**：Linux是一个时间共享系统，因此必须依赖于缓慢且昂贵的中断来通知内核有新的数据包需要处理。'
- en: '**Threaded model:** The Linux kernel is heavily threaded, so all data structures
    are protected with locks. While a huge effort has made Linux very scalable, this
    is not without limitations and contention occurs at large core counts. Even without
    contention, the locking primitives themselves are relatively slow and impact networking
    performance.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程模型**：Linux内核高度线程化，因此所有数据结构都由锁保护。尽管Linux已经非常可扩展，但这并非没有限制，并且在大核心计数时会发生竞争。即使没有竞争，锁原语本身也相对较慢，并影响网络性能。'
- en: As before, the way to overcome this limitation is to move the packet processing
    to the userspace. There are plenty of out-of-kernel implementations of the TCP
    algorithm that are worth considering.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，克服这种限制的方法是将数据包处理移动到用户空间。有许多值得考虑的内核外TCP算法实现。
- en: DPDK
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DPDK
- en: One of the generic approaches that’s often referred to in the networking area
    is the poll mode vs interrupt model. When a packet arrives, the system may have
    two options for how to get informed—set up and interrupt from the hardware (or,
    in the case of the userspace implementation, from the kernel file descriptor using
    the *poll* family of system calls) or keep polling the network card on its own
    from time to time until the packet is noticed.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 网络领域中经常被提及的一种通用方法是轮询模式与中断模型。当一个数据包到达时，系统可能有两种方式来获取通知——通过硬件设置和中断（或者在用户空间实现的情况下，通过使用系统调用中的
    *poll* 家族从内核文件描述符）或者时不时地自行轮询网络卡，直到注意到数据包。
- en: The famous userspace network toolkit, called *DPDK*, is designed specifically
    for fast packet processing, usually in fewer than 80 CPU cycles per packet.^([7](#Fn7))
    It integrates seamlessly with Linux in order to take advantage of high-performance
    hardware.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 著名的用户空间网络工具包，称为 *DPDK*，专门设计用于快速数据包处理，通常每个数据包少于80个CPU周期.^([7](#Fn7)) 它与Linux无缝集成，以便利用高性能硬件。
- en: IRQ Binding
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IRQ 绑定
- en: As stated earlier, packet processing may take up to 60 percent of the CPU time,
    which is way too much. This percentage leaves too few CPU ticks for the database
    work itself. Even though in this case the backpressure mechanism would most likely
    keep the external activity off and the system would likely find its balance, the
    resulting system throughput would likely be unacceptable.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据包处理可能占用高达60%的CPU时间，这远远太多。这个百分比留给数据库本身的工作的CPU周期太少。即使在这种情况下，背压机制最有可能保持外部活动关闭，系统可能会找到平衡，但结果系统的吞吐量可能是不可以接受的。
- en: System architects may consider the non-symmetrical CPU approach to mitigate
    this. If you’re letting the Linux kernel process network packets, there are several
    ways to localize this processing on separate CPUs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 系统架构师可以考虑采用非对称CPU方法来减轻这一问题。如果你让Linux内核处理网络数据包，有几种方法可以将这种处理本地化到单独的CPU上。
- en: The simplest way is to bind the IRQ processing from the NIC to specific cores
    or hyper-threads. Linux uses two-step processing of incoming packets called IRQ
    and soft-IRQ. If the IRQs are properly bound to cores, the soft-IRQ also happens
    on those cores—thus completely localizing the processing.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是将网络接口卡（NIC）的IRQ处理绑定到特定的核心或超线程。Linux使用两步处理传入数据包，称为IRQ和软IRQ。如果IRQ被正确绑定到核心，软IRQ也会在这些核心上发生——从而完全本地化处理。
- en: For huge-scale nodes running tens to hundred(s) of cores, the number of network-only
    cores may become literally more than one. In this case, it might make sense to
    localize processing even further by assigning cores from different NUMA nodes
    and teaching the NIC to balance the traffic between those using the receive packet
    steering facility of the Linux kernel.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于运行着数十到数百个核心的巨大规模节点，仅网络核心的数量可能实际上会超过一个。在这种情况下，通过分配来自不同NUMA节点的核心，并教会网络接口卡（NIC）利用Linux内核的接收数据包引导功能来平衡这些核心之间的流量，进一步本地化处理可能是有意义的。
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced a number of ways that database engineering decisions
    enable database users to squeeze more power out of modern infrastructure. For
    CPUs, the chapter talked about taking advantage of multicore servers by limiting
    resource sharing across cores and using future-promise design to coordinate work
    across cores. The chapter also provided a specific example of how low-level CPU
    architecture has direct implications on the database.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了几种数据库工程决策的方式，这些决策使数据库用户能够从现代基础设施中获得更多动力。对于CPU，本章讨论了通过限制核心间的资源共享和利用未来承诺设计来协调核心间工作，从而利用多核服务器。本章还提供了一个具体的例子，说明了底层CPU架构对数据库的直接影响。
- en: 'Moving on to memory, you read about two related but independent subsystems:
    memory allocation and cache control. For I/O, the chapter discussed Linux options
    such as traditional read/write, mmap, Direct I/O (DIO) read/write, and Asynchronous
    I/O—including the various tradeoffs of each. This was followed by a deep dive
    into how modern SSDs work and how a database can take advantage of a drive’s unique
    characteristics. Finally, you looked at constraints associated with the Linux
    networking stack and explored alternatives such as DPDK and IRQ binding. The next
    chapter shifts the focus from hardware interactions to algorithmic optimizations:
    pure software challenges.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是内存部分，你了解到两个相关但独立的子系统：内存分配和缓存控制。对于I/O，本章讨论了Linux的选项，如传统的读写、mmap、直接I/O（DIO）读写和异步I/O——包括每种方法的各项权衡。随后深入探讨了现代SSD的工作原理以及数据库如何利用驱动器的独特特性。最后，你了解了与Linux网络堆栈相关的约束，并探讨了DPDK和中断请求（IRQ）绑定等替代方案。下一章将重点从硬件交互转移到算法优化：纯软件挑战。
- en: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
- en: '**Open Access** This chapter is licensed under the terms of the Creative Commons
    Attribution 4.0 International License ([http://​creativecommons.​org/​licenses/​by/​4.​0/​](http://creativecommons.org/licenses/by/4.0/)),
    which permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons license and indicate if changes
    were made.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放获取** 本章节根据Creative Commons Attribution 4.0国际许可协议（[http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/)）授权，允许在任何媒介或格式中使用、分享、改编、分发和复制，只要您适当引用原始作者和来源，提供Creative
    Commons许可的链接，并指出是否进行了修改。'
- en: The images or other third party material in this chapter are included in the
    chapter's Creative Commons license, unless indicated otherwise in a credit line
    to the material. If material is not included in the chapter's Creative Commons
    license and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中包含的图片或其他第三方材料均包含在章节的Creative Commons许可证中，除非在材料引用行中另有说明。如果材料未包含在章节的Creative
    Commons许可证中，且您的使用意图不受法定法规允许或超出允许的使用范围，您将需要直接从版权持有人处获得许可。
