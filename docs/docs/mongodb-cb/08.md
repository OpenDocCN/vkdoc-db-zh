# 八、与 Hadoop 的集成

在本章中，我们将介绍以下配方：

*   使用 mongo hadoop 连接器执行我们的第一个示例 MapReduce 作业
*   编写我们的第一个 Hadoop MapReduce 作业
*   使用流在 Hadoop 上运行 MapReduce 作业
*   在 Amazon EMR 上运行 MapReduce 作业

# 导言

Hadoop是一款著名的处理大型数据集的开源软件。它还有一个用于 MapReduce 编程模型的 API，该模型被广泛使用。几乎所有的大数据解决方案都有某种支持，可以将它们与 Hadoop 集成，以使用其 MapReduce 框架。MongoDB 还有一个与 Hadoop 集成的连接器，允许我们使用 Hadoop MapReduce API 编写 MapReduce 作业，处理 MongoDB/MongoDB 转储中的数据，并将结果写入 MongoDB/MongoDB 转储文件。在本章中，我们将介绍一些基本 MongoDB 和 Hadoop 集成的方法。

# 使用 mongo hadoop 连接器执行我们的第一个示例 MapReduce 作业

在这个配方中，我们将看到如何从源代码构建 mongo hadoop 连接器，并仅为了在独立模式下运行示例而设置 hadoop。连接器是使用 Mongo 中的数据在 Hadoop 上运行 Hadoop MapReduce 作业的主干。

## 准备好了吗

Hadoop 有各种分布；但是，我们将使用 ApacheHadoop（[http://hadoop.apache.org/](http://hadoop.apache.org/) 。安装将在 Ubuntu Linux 上完成。ApacheHadoop 始终在 Linux 环境下运行以进行生产，而 Windows 没有针对生产系统进行测试。出于开发目的，可以使用 Windows。如果您是 Windows 用户，我建议您安装一个虚拟化环境，如 VirtualBox（[https://www.virtualbox.org/](https://www.virtualbox.org/) ），设置 Linux 环境，然后在其上安装 Hadoop。在上面设置 VirtualBox 和 Linux 并没有显示在这个配方中，但这不是一项乏味的任务。这个配方的先决条件是机器上有 Linux 操作系统和互联网连接。我们将在这里设置的版本是 ApacheHadoop 的 2.4.0。在撰写本书时，由 mongo Hadoop 连接器支持的 Apache Hadoop 的最新版本是 2.4.0。

需要 Git 客户端将 mongo hadoop 连接器的存储库克隆到本地文件系统。参见至[http://git-scm.com/book/en/Getting-Started-Installing-Git](http://git-scm.com/book/en/Getting-Started-Installing-Git) 安装 Git。

您还需要在操作系统上安装 MongoDB。参见[http://docs.mongodb.org/manual/installation/](http://docs.mongodb.org/manual/installation/) 并相应安装。启动监听端口`27017`的`mongod`实例。我们并不期望您成为 Hadoop 方面的专家，但对 Hadoop 的一些熟悉会有所帮助。了解 MapReduce 的概念很重要，了解 Hadoop MapReduce API 将是一个优势。在这个食谱中，我们将解释完成这项工作需要什么。您可以从其他来源获得有关 Hadoop 及其 MapReduceAPI 的更多详细信息。位于[的维基页面 http://en.wikipedia.org/wiki/MapReduce](http://en.wikipedia.org/wiki/MapReduce) 提供了一些关于 MapReduce编程的好信息。

## 怎么做…

1.  我们将首先安装Java、Hadoop 和所需的软件包。我们将从在操作系统上安装 JDK 开始。在操作系统的命令提示符下键入以下内容：

    ```js
    $ javac –version

    ```

2.  If the program doesn't execute and you are told about various packages that contain javac and program, then we need to install Java as follows:

    ```js
    $ sudo apt-get install default-jdk

    ```

    这就是安装 Java 所需要做的全部工作。

3.  从[下载当前版本的 Hadoophttp://www.apache.org/dyn/closer.cgi/hadoop/common/](http://www.apache.org/dyn/closer.cgi/hadoop/common/) 下载 2.4.0 版（或最新的 mongo hadoop 连接器支持）。
4.  After the `.tar.gz` file is downloaded, execute the following on the command prompt:

    ```js
    $ tar –xvzf <name of the downloaded .tar.gz file>
    $ cd <extracted directory>

    ```

    打开`etc/hadoop/hadoop-env.sh`文件，将导出`JAVA_HOME = ${JAVA_HOME}`替换为导出`JAVA_HOME = /usr/lib/jvm/default-java`。

    现在，我们将从本地文件系统上的 GitHub 获取 mongo hadoop 连接器代码。请注意，克隆存储库不需要 GitHub 帐户。从操作系统命令提示符克隆 Git 项目，如下所示：

    ```js
    $git clone https://github.com/mongodb/mongo-hadoop.git
    $cd mongo-hadoop 

    ```

5.  Create a soft link—the Hadoop installation directory is the same as the one that we extracted in step 3:

    ```js
    $ln –s <hadoop installation directory> ~/hadoop-binaries

    ```

    例如，如果在主目录中提取/安装 Hadoop，则将执行以下命令：

    ```js
    $ln –s ~/hadoop-2.4.0 ~/hadoop-binaries

    ```

    默认情况下，mongo hadoop 连接器将在`~/hadoop-binaries`文件夹下查找 hadoop 发行版。因此，即使 Hadoop 归档文件是从别处提取的，我们也可以创建到它的软链接。一旦创建了这个链接，Hadoop 二进制文件就应该位于`~/hadoop-binaries/hadoop-2.4.0/bin`路径中。

6.  We will now build the mongo-hadoop connector from the source for the Apache Hadoop version 2.4.0\. The build-by-default builds for the latest version, so as of now, the `-Phadoop_version` parameter can be left out as 2.4 is the latest.

    ```js
    $./gradlew jar –Phadoop_version='2.4'

    ```

    此构建过程需要一些时间才能完成。

7.  一旦构建成功完成，我们将准备好执行第一个 MapReduce 作业。我们将使用 mongo hadoop 连接器项目提供的一个`treasuryYield`示例来实现这一点。第一个活动是将数据导入 Mongo 中的集合。
8.  假设`mongod`实例已启动并正在运行，正在监听端口`27017`进行连接，并且当前目录是 mongo hadoop 连接器代码库的根目录，则执行以下命令：

    ```js
    $ mongoimport -c yield_historical.in -d mongo_hadoop --drop examples/treasury_yield/src/main/resources/yield_historical_in.json

    ```

9.  Once the import action is successful, we are left with copying two jar files to the `lib` directory. Execute the following in the operating system shell:

    ```js
    $ wget http://repo1.maven.org/maven2/org/mongodb/mongo-java-driver/2.12.0/mongo-java-driver-2.12.0.jar
    $ cp core/build/libs/mongo-hadoop-core-1.2.1-SNAPSHOT-hadoop_2.4.jar ~/hadoop-binaries/hadoop-2.4.0/lib/
    $ mv mongo-java-driver-2.12.0.jar ~/hadoop-binaries/hadoop-2.4.0/lib

    ```

    ### 注

    为要复制的 mongo hadoop 核心构建的 JAR 命名如前一节代码主干版本所示，并为 hadoop-2.4.0 构建。当您自己为连接器和 Hadoop 的不同版本构建 JAR 时，相应地更改 JAR 的名称。Mongo 驱动可以是最新版本。版本 2.12.0 是撰写本书时的最新版本。

10.  现在，在操作系统 shell 的命令提示符下执行以下命令：

    ```js
     ~/hadoop-binaries/hadoop-2.4.0/bin/hadoop     jar     examples/treasury_yield/build/libs/treasury_yield-1.2.1-SNAPSHOT-hadoop_2.4.jar  \com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig  \-Dmongo.input.split_size=8     -Dmongo.job.verbose=true  \-Dmongo.input.uri=mongodb://localhost:27017/mongo_hadoop.yield_historical.in  \-Dmongo.output.uri=mongodb://localhost:27017/mongo_hadoop.yield_historical.out

    ```

11.  输出应该打印出很多东西；但是，输出中的以下行告诉我们 map reduce 作业成功：

    ```js
     14/05/11 21:38:54 INFO mapreduce.Job: Job job_local1226390512_0001 completed successfully

    ```

12.  从 mongo 客户端连接本地主机上运行的`mongod`实例，并对以下集合执行查找：

    ```js
    $ mongo
    > use mongo_hadoop
    switched to db mongo_hadoop
    > db.yield_historical.out.find()

    ```

## 它是如何工作的…

安装Hadoop 并不是一项琐碎的任务，我们不需要进入这个阶段来尝试我们的 Hadoop mongo 连接器示例。要了解 Hadoop 及其安装和其他方面的知识，有专门的书籍和文章可供参考。在本章中，我们只需下载归档文件，并在独立模式下提取和运行 MapReduce 作业。这是使用 Hadoop 最快的方法。安装 Hadoop 需要执行到步骤 6 的所有步骤。在接下来的两个步骤中，我们将克隆 mongo hadoop 连接器配方。您还可以在[下载 Hadoop 版本的稳定版本 https://github.com/mongodb/mongo-hadoop/releases](https://github.com/mongodb/mongo-hadoop/releases) 如果你不想从源头上构建。然后，我们为 Hadoop（2.4.0）版本构建连接器，直到步骤 13。接下来的第 14 步是运行实际的 MapReduce 作业来处理 MongoDB 中的数据。我们将数据导入到`yield_historical.in`集合，该集合将用作 MapReduce 作业的输入。继续使用`mongo_hadoop`数据库查询 mongo shell 中的集合以查看文档。如果你不理解内容，不要担心；我们想看看在这个例子中我们打算如何处理这些数据。

下一步是对数据调用 MapReduce 操作。执行 Hadoop 命令时给出了一个 jar 的路径（`examples/treasury_yield/build/libs/treasury_yield-1.2.1-SNAPSHOT-hadoop_2.4.jar`）。这是一个 jar，其中包含实现国债收益率的样例 MapReduce 操作的类。这个 JAR 文件中的`com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig`类是包含 main 方法的引导类。我们将很快参观这个班。连接器支持多种配置。完整的配置列表见[https://github.com/mongodb/mongo-hadoop/](https://github.com/mongodb/mongo-hadoop/) 。现在，我们只需要记住，`mongo.input.uri`和`mongo.output.uri`是 map reduce 操作的输入和输出集合。

克隆项目后，现在可以将其导入到您选择的任何 JavaIDE 中。我们特别感兴趣的是位于`/examples/treasury_yield`的项目和克隆存储库根目录中的核心。

让我们看看`com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig`课程。这是 MapReduce 方法的入口点，其中包含一个 main 方法。要使用 mongo hadoop 连接器为 mongo 编写 MapReduce 作业，主类必须始终从`com.mongodb.hadoop.util.MongoTool`扩展。这个类实现了`org.apache.hadoop.Tool`接口，这个接口有 run 方法，由`MongoTool`类为我们实现。main 方法所需要做的就是使用`org.apache.hadoop.util.ToolRunner`类通过调用传递我们的主类实例（它是`Tool`的实例）的静态`run`方法来执行这个类。

有一个静态块从两个 XML 文件`hadoop-local.xml`和`mongo-defaults.xml`加载一些配置。这些文件（或任何 XML 文件）的格式如下所示。文件的根节点是配置节点，其下有多个属性节点：

```js
<configuration>
  <property>
    <name>{property name}</name> 
    <value>{property value}</value>
  </property>
  ...
</configuration>
```

在此上下文中有意义的属性值是我们在前面提供的 URL 中提到的所有属性值。我们在引导类`TreasuryYieldXmlConfig`的构造函数中实例化`com.mongodb.hadoop.MongoConfig`包装`org.apache.hadoop.conf.Configuration`的实例。`MongoConfig`类提供了合理的默认值，足以满足大多数用例。在`MongoConfig`实例中，我们需要设置的一些最重要的东西是输出和输入格式、`mapper`和`reducer`类、映射器的输出键和值以及减速机的输出键和值。输入格式和输出格式始终是由 mongo hadoop 连接器库提供的`com.mongodb.hadoop.MongoInputFormat`和`com.mongodb.hadoop.MongoOutputFormat`类。对于 mapper 和 reducer 输出键和值，我们有任何`org.apache.hadoop.io.Writable`实现。关于`org.apache.hadoop.io`包中不同类型的可写实现，请参阅 Hadoop 文档。除此之外，mongo hadoop 连接器还在`com.mongodb.hadoop.io`包中为我们提供了一些实现。对于国债收益率的例子，我们使用了`BSONWritable`实例。这些可配置值可以在我们前面看到的 XML 文件中提供，也可以通过编程进行设置。最后，我们可以选择提供它们作为我们为`mongo.input.uri`和`mongo.output.uri`所做的`vm`参数。这些参数可以在 XML 中提供，也可以直接从`MongoConfig`实例上的代码调用；这两种方法分别是`setInputURI`和`setOutputURI`。

现在我们来看看`mapper`和`reducer`类的实现。我们将在这里复制课程的重要部分，以便进行分析。有关整个实施，请参阅克隆项目：

```js
public class TreasuryYieldMapper
    extends Mapper<Object, BSONObject, IntWritable, DoubleWritable> {

    @Override
    public void map(final Object pKey,
                    final BSONObject pValue,
                    final Context pContext)
        throws IOException, InterruptedException {
        final int year = ((Date) pValue.get("_id")).getYear() + 1900;
        double bid10Year = ((Number) pValue.get("bc10Year")).doubleValue();
        pContext.write(new IntWritable(year), new DoubleWritable(bid10Year));
    }
}
```

我们的映射器扩展了`org.apache.hadoop.mapreduce.Mapper`类。这四个通用参数是键类、输入值类型、输出键类型和输出值。map 方法的主体从输入文档中读取`_id`值，即日期，并从中提取年份。然后，它从文档中获取`bc10Year`字段的双精度值，并简单地写入上下文键值对，其中 key 是年份，double 的值写入上下文键值对。这里的实现不依赖于传递的`pKey`参数的值，它可以用作键，而不是在实现中硬编码`_id`值。该值基本上与使用 XML 中的`mongo.input.key`属性或`MongoConfig.setInputKey`方法设置的字段相同。如果没有设置，`_id`是默认值。

让我们看看 reducer 实现（删除了日志记录语句）：

```js
public class TreasuryYieldReducer extends Reducer<IntWritable, DoubleWritable, IntWritable, BSONWritable> {

    @Override
    public void reduce(final IntWritable pKey, final Iterable<DoubleWritable> pValues, final Context pContext)throws IOException, InterruptedException {
      int count = 0;
      double sum = 0;
      for (final DoubleWritable value : pValues) {
        sum += value.get();
        count++;
      }
      final double avg = sum / count;
      BasicBSONObject output = new BasicBSONObject();
      output.put("count", count);
      output.put("avg", avg);
      output.put("sum", sum);
      pContext.write(pKey, new BSONWritable(output));
    }
}
```

此类从`org.apache.hadoop.mapreduce.Reducer`扩展而来，具有四个通用参数：输入键、输入值、输出键和输出值。reducer 的输入是映射器的输出，因此，如果您仔细注意的话，前两个泛型参数的类型与我们前面看到的映射器的最后两个泛型参数的类型相同。第三个和第四个参数是 reduce 发出的键和值的类型。值的类型是`BSONDocument`，因此我们将`BSONWritable`作为类型。

现在我们有了 reduce 方法，它有两个参数：第一个是 key，它与 map 函数发出的 key 相同，第二个参数是为同一个 key 发出的值的`java.lang.Iterable`。这就是标准 map reduce 函数的工作方式。例如，如果 map 函数给出了以下键值对（1950，10），（1960，20），（1950，20），（1950，30），那么 reduce 将使用两个唯一的键 1950 和 1960 调用，键 1950 的值将是带有（10，20，30）的`Iterable`，而 1960 的值将是单个元素（20）的`Iterable`。reducer 的 reduce 函数只需迭代两个数字中的`Iterable`，找到这些数字的总和和计数，并写入一个键值对，其中键与传入键相同，输出值为`BasicBSONObject`，计算值的总和、计数和平均值。

在克隆的 mongo hadoop 连接器示例中有一些很好的示例，包括安然数据集。如果您想玩一玩，我建议您看看这些示例项目并运行它们。

## 还有更多…

我们在这里看到的是我们执行的一个现成样本。没有什么比亲自编写一份MapReduce 作业更能澄清我们的理解。在下一个配方中，我们将使用 Java 中的 HadoopAPI 编写一个示例 MapReduce 作业，并查看它的实际操作。

## 另见…

如果您想知道`Writable`接口是关于什么的，为什么不应该使用普通的旧序列化，那么请参考这个 URL，它给出了 Hadoop 创建者自己的解释：[http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00378.html](http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00378.html)。

# 编写我们的第一个 Hadoop MapReduce 作业

在这个配方中，我们将使用 Hadoop MapReduce API 编写第一个 MapReduce 作业，并使用从 MongoDB 获取数据的 mongo Hadoop 连接器运行它。参考[第 3 章](03.html "Chapter 3. Programming Language Drivers")、*编程语言驱动*中的*使用 Java 客户端*配方在 Mongo 中执行 MapReduce，了解 MapReduce 是如何使用 Java 客户端实现的、测试数据创建和问题陈述。

## 准备好了吗

请参阅前面的*使用 mongo hadoop 连接器*方法执行我们的第一个示例 MapReduce 作业，以设置 mongo hadoop 连接器。本配方的先决条件以及使用[第 3 章](03.html "Chapter 3. Programming Language Drivers")、*编程语言驱动*中的 Java 客户端配方在 Mongo 中执行 MapReduce 的*都是本配方所需要的。这是一个 maven 项目，因此需要设置和安装 maven。请参阅[第一章](01.html "Chapter 1. Installing and Starting the Server")*安装和启动服务器*中的*从 Java 客户端*连接到单节点的配方，其中提供了在 Windows 中设置 maven 的步骤；此项目构建在 Ubuntu Linux 上，以下是您需要在操作系统 shell 中执行的命令，以获取 maven：*

```js
$ sudo apt-get install maven

```

## 怎么做…

1.  我们有一个 Java`mongo-hadoop-mapreduce-test`项目，可以从 Packt 网站下载。该项目旨在实现我们在[第 3 章](03.html "Chapter 3. Programming Language Drivers")、*编程语言驱动*中使用 MongoDB 的 MapReduce 框架的配方中实现的相同用例。在以前的情况下，我们使用 Python 和 Java 客户端调用了 MapReduce 作业。
2.  在命令提示符下，当前目录位于项目根目录中，其中存在`pom.xml`文件，执行以下命令：

    ```js
    $ mvn clean package

    ```

3.  JAR 文件`mongo-hadoop-mapreduce-test-1.0.jar`将被构建并保存在目标目录中。
4.  在假设 CSV 文件已经导入`postalCodes`集合的情况下，执行以下命令，当前目录仍然位于我们刚刚构建的`mongo-hadoop-mapreduce-test`项目的根目录中：

    ```js
    ~/hadoop-binaries/hadoop-2.4.0/bin/hadoop \
     jar target/mongo-hadoop-mapreduce-test-1.0.jar \
     com.packtpub.mongo.cookbook.TopStateMapReduceEntrypoint \
     -Dmongo.input.split_size=8 \
    -Dmongo.job.verbose=true \
    -Dmongo.input.uri=mongodb://localhost:27017/test.postalCodes \
    -Dmongo.output.uri=mongodb://localhost:27017/test.postalCodesHadoopmrOut

    ```

5.  MapReduce 作业完成后，通过在操作系统命令提示符下键入以下命令打开 mongo shell，并在 shell 中执行以下查询：

    ```js
    $ mongo
    > db.postalCodesHadoopmrOut.find().sort({count:-1}).limit(5)

    ```

6.  将输出与我们之前使用 mongo 的 map reduce 框架（在[第 3 章](03.html "Chapter 3. Programming Language Drivers")、*编程语言驱动*中）执行 MapReduce 作业时得到的输出进行比较。

## 它是如何工作的…

我们使这些类非常简单，只提供了我们所需要的最少的东西。我们的项目中只有三个类：`TopStateMapReduceEntrypoint`、`TopStateReducer`和`TopStatesMapper`，都在相同的`com.packtpub.mongo.cookbook`包中。映射器的`map`函数只是将一个键值对写入上下文，其中键是状态的名称，值是一个整数值，即 1。以下是来自`mapper`函数的代码片段：

```js
context.write(new Text((String)value.get("state")), new IntWritable(1));
```

reducer 得到的是相同的键，即状态列表和整型值的 Iterable，1。我们所要做的就是在上下文中写下相同的国家名称和所有可数名词的总和。现在，由于 Iterable 中没有可以在恒定时间内给出计数的大小方法，我们只能将线性时间内得到的所有值相加。以下是减速器方法中的代码：

```js
int sum = 0;
for(IntWritable value : values) {
  sum += value.get();
}
BSONObject object = new BasicBSONObject();
object.put("count", sum);
context.write(text, new BSONWritable(object));
```

我们将作为键的文本字符串和包含计数的 JSON 文档的值写入上下文。然后，mongo hadoop 连接器负责将`postalCodesHadoopmrOut`文档写入我们拥有的输出集合，`_id`字段与发出的键相同。因此，当我们执行以下操作时，我们会得到数据库中城市数量最多的前五个州：

```js
> db. postalCodesHadoopmrOut.find().sort({count:-1}).limit(5)
{ "_id" : "Maharashtra", "count" : 6446 }
{ "_id" : "Kerala", "count" : 4684 }
{ "_id" : "Tamil Nadu", "count" : 3784 }
{ "_id" : "Andhra Pradesh", "count" : 3550 }
{ "_id" : "Karnataka", "count" : 3204 }

```

最后，主入口点类的主要方法如下：

```js
Configuration conf = new Configuration();
MongoConfig config = new MongoConfig(conf);
config.setInputFormat(MongoInputFormat.class);
config.setMapperOutputKey(Text.class);
config.setMapperOutputValue(IntWritable.class);
config.setMapper(TopStatesMapper.class);
config.setOutputFormat(MongoOutputFormat.class);
config.setOutputKey(Text.class);
config.setOutputValue(BSONWritable.class);
config.setReducer(TopStateReducer.class);
ToolRunner.run(conf, new TopStateMapReduceEntrypoint(), args);
```

我们所做的就是用`com.mongodb.hadoop.MongoConfig`实例包装`org.apache.hadoop.conf.Configuration`对象以设置各种属性，然后使用 ToolRunner 提交 MapReduce 作业以执行。

## 另见

我们使用 Hadoop API 在 Hadoop 上执行了一个简单的 MapReduce 作业，从 MongoDB 获取数据，并将数据写入 MongoDB 集合。如果我们想用另一种语言编写`map`和`reduce`函数呢？幸运的是，使用名为 Hadoop streaming 的概念可以实现这一点，其中`stdout`被用作程序和 Hadoop MapReduce 框架之间的通信手段。在下一个菜谱中，我们将演示如何使用 Python 实现我们在本菜谱中使用 Hadoop 流的相同用例。

# 使用流媒体在 Hadoop 上运行 MapReduce 作业

在前面的方法中，我们使用 Hadoop 的 JavaAPI 实现了一个简单的 MapReduce 作业。用例与[第 3 章](03.html "Chapter 3. Programming Language Drivers")*编程语言驱动*中的相同，我们在其中使用 Python 和 Java 中的 Mongo客户端 API 实现了 MapReduce。在此配方中，我们将使用 Hadoop 流来实现 MapReduce 作业。

流式传输的概念适用于使用`stdin`和`stdout`的通信。您可以在[获得更多关于 Hadoop 流媒体及其工作原理的信息 http://hadoop.apache.org/docs/r1.2.1/streaming.html](http://hadoop.apache.org/docs/r1.2.1/streaming.html) 。

## 准备好了…

请参阅本章中的*使用 mongo hadoop 连接器*配方执行我们的第一个示例 MapReduce 作业，了解如何为开发目的设置 hadoop 并使用 Gradle 构建 mongo hadoop 项目。就 Python 库而言，我们将从源代码安装所需的库；但是，如果不希望从源代码构建，可以使用`pip`（Python 的包管理器）进行设置。我们还将看到如何使用`pip`设置 pymongo hadoop。

请参阅[第 1](01.html "Chapter 1. Installing and Starting the Server")章*安装和启动服务器*中的配方*使用 Python 客户端*连接到单个节点，了解如何为主机操作系统安装 PyMongo。

## 它是如何工作的…

1.  我们将首先从源代码构建 pymongo–hadoop。将项目克隆到本地文件系统后，在克隆项目的根目录中执行以下操作：

    ```js
    $ cd streaming/language_support/python
    $ sudo python setup.py install

    ```

2.  输入密码后，安装程序将继续安装在您机器上的 pymongo hadoop 上。
3.  这就是从源代码构建 pymongo hadoop 所需要做的全部工作。但是，如果您选择不从源代码生成，则可以在操作系统 shell 中执行以下命令：

    ```js
    $ sudo pip install pymongo_hadoop

    ```

4.  在以任何一种方式安装 pymongo hadoop之后，我们现在将用 Python 实现我们的`mapper`和`reducer`函数。`mapper`功能如下：

    ```js
    #!/usr/bin/env python

    import sys
    from pymongo_hadoop import BSONMapper
    def mapper(documents):
      print >> sys.stderr, 'Starting mapper'
      for doc in documents:
        yield {'_id' : doc['state'], 'count' : 1}
      print >> sys.stderr, 'Mapper completed'

    BSONMapper(mapper)
    ```

5.  现在来看`reducer`函数，它将如下所示：

    ```js
    #!/usr/bin/env python

    import sys
    from pymongo_hadoop import BSONReducer
    def reducer(key, documents):
      print >> sys.stderr, 'Invoked reducer for key "', key, '"'
      count = 0
      for doc in documents:
        count += 1
      return {'_id' : key, 'count' : count}

    BSONReducer(reducer)
    ```

6.  The environment variables, `$HADOOP_HOME` and `$HADOOP_CONNECTOR_HOME`, should point to the base directory of Hadoop and the mongo-hadoop connector project, respectively. Now, we will invoke the `MapReduce` function using the following command in the operating system shell. The code available with the book on the Packt website has the `mapper`, `reduce` Python script, and shell script that will be used to invoke the `mapper` and `reducer` function:

    ```js
    $HADOOP_HOME/bin/hadoop jar \
    $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming* \
    -libjars $HADOOP_CONNECTOR_HOME/streaming/build/libs/mongo-hadoop-streaming-1.2.1-SNAPSHOT-hadoop_2.4.jar \
    -input /tmp/in \
    -output /tmp/out \
    -inputformat com.mongodb.hadoop.mapred.MongoInputFormat \
    -outputformat com.mongodb.hadoop.mapred.MongoOutputFormat \
    -io mongodb \
    -jobconf mongo.input.uri=mongodb://127.0.0.1:27017/test.postalCodes \
    -jobconf mongo.output.uri=mongodb://127.0.0.1:27017/test.pyMRStreamTest \
    -jobconf stream.io.identifier.resolver.class=com.mongodb.hadoop.streaming.io.MongoIdentifierResolver \
    -mapper mapper.py \
    -reducer reducer.py

    ```

    执行此命令时，`mapper.py`和`reducer.py`文件存在于当前目录中。

7.  在执行命令时，需要一段时间才能成功执行 MapReduce 作业，在操作系统命令提示符下键入以下命令打开 mongo shell，并从 shell 执行以下查询：

    ```js
    $ mongo
    > db.pyMRStreamTest.find().sort({count:-1}).limit(5)

    ```

8.  将输出与我们之前在[第 3 章](03.html "Chapter 3. Programming Language Drivers")、*编程语言驱动*中使用 mongo 的 MapReduce 框架执行 MapReduce 作业时得到的输出进行比较。

## 怎么做…

让我们看一下第 5 步和第 6 步，其中我们编写了`mapper`和`reducer`函数。我们定义了一个`map`函数，它接受所有文档的列表。我们遍历这些文档并生成文档，`_id`字段是键的名称，count value 字段的值为 1。生成的文档数与输入文档总数相同。

最后我们实例化了`BSONMapper`，它接受`mapper`函数作为参数。该函数返回一个生成器对象，然后该`BSONMapper`类使用该对象将值提供给 MapReduce 框架。我们需要记住的是，`mapper`函数需要返回一个生成器（在循环中调用 yield 时返回），然后实例化`pymongo_hadoop`模块提供给我们的`BSONMapper`类。对于感兴趣的人，您可以选择查看`streaming/language_support/python/pymongo_hadoop/mapper.py`文件中本地文件系统上克隆的项目下的源代码，看看它的作用。这是一段小而容易理解的代码。

对于`reducer`函数，我们获取该键的键和文档列表作为值。该键与在`map`函数中从文档发出的`_id`字段的值相同。我们只需在这里返回一个新文档，其中`_id`是该州的名称，count 是该州的文档数。请记住，我们返回一个文档，而不是像在 map 中那样发出一个文档。最后实例化`BSONReducer`并传递`reducer`函数。在`streaming/language_support/python/pymongo_hadoop/reducer.py`文件的本地文件系统上克隆的项目下的源代码实现了`BSONReducer`类。

最后，我们调用了 shell 中的命令来启动使用流的 MapReduce 作业。这里需要注意的是，我们需要两个 JAR 文件：一个在 Hadoop 发行版的`share/hadoop/tools/lib`中，另一个在 mongo Hadoop 连接器中，该连接器位于`streaming/build/libs/`目录中。输入和输出格式分别为`com.mongodb.hadoop.mapred.MongoInputFormat`和`com.mongodb.hadoop.mapred.MongoOutputFormat`。

正如我们前面看到的，`sysout`和`sysin`构成了流媒体的主干。因此，基本上，我们需要对我们的 BSON 对象进行编码，以写入`sysout`，然后，我们应该能够读取`sysin`，以再次将内容转换为 BSON 对象。为此，mongo hadoop 连接器为我们提供了两个框架类，`com.mongodb.hadoop.streaming.io.MongoInputWriter`和`com.mongodb.hadoop.streaming.io.MongoOutputReader`来编码和解码 BSON 对象。这些类分别从`org.apache.hadoop.streaming.io.InputWriter`和`org.apache.hadoop.streaming.io.OutputReader`扩展而来。

`stream.io.identifier.resolver.class`属性的值为`com.mongodb.hadoop.streaming.io.MongoIdentifierResolver`。这个类从`org.apache.hadoop.streaming.io.IdentifierResolver`扩展而来，让我们有机会在框架中注册`org.apache.hadoop.streaming.io.InputWriter`和`org.apache.hadoop.streaming.io.OutputReader`的实现。我们还使用我们的自定义`IdentifierResolver`注册输出键和输出值类。只要记住，在使用 mongo hadoop 连接器的流式处理时，一定要使用这个解析器。

我们最终执行了前面讨论过的`mapper`和`reducer`python 函数。需要记住的一点是，不要将`mapper`和`reducer`函数中的日志打印到`sysout`。`sysout`和`sysin`映射器和减速器是通信手段，向其写入日志可能会产生不希望的行为。正如我们在示例中看到的，写入标准错误（`stderr`或日志文件。

### 注

在 Unix 中使用多行命令时，可以使用`\`在下一行继续该命令。但是，请记住在`\`之后不要有空格。

# 在 Amazon EMR 上运行 MapReduce 作业

此配方涉及使用 AWS 在云上运行 MapReduce 作业。您需要一个 AWS 帐户才能继续。在 AWS 注册[http://aws.amazon.com/](http://aws.amazon.com/) 。我们将看到如何使用**Amazon Elastic Map Reduce**（**Amazon EMR**在云上运行 MapReduce 作业。亚马逊 EMR 是亚马逊在云上提供的托管 MapReduce服务。参见[https://aws.amazon.com/elasticmapreduce/](https://aws.amazon.com/elasticmapreduce/) 了解更多详情。AmazonEMR 使用 AWSS3 存储桶中的数据、二进制文件/JAR 等，处理它们并将结果写回 S3 存储桶。**亚马逊简单存储服务****亚马逊 S3**是 AWS 的另一项云上数据存储服务。参见[http://aws.amazon.com/s3/](http://aws.amazon.com/s3/) 了解亚马逊 S3 的更多详情。尽管我们将使用 mongo hadoop 连接器，但一个有趣的事实是，我们不需要启动和运行 MongoDB 实例。我们将使用存储在 S3 存储桶中的 MongoDB 数据转储进行数据分析。MapReduce 程序将在输入 BSON dump 上运行，并在输出 bucket 中生成结果 BSON dump。MapReduce 程序的日志将写入另一个专用于日志的存储桶。下图向我们展示了我们的设置在高层次上的表现：

![Running a MapReduce job on Amazon EMR](img/B04831_08_12.jpg)

## 准备好了吗

我们将使用与*编写第一个 Hadoop MapReduce 作业*配方时相同的 Java 示例。要了解更多关于`mapper`和`reducer`类实现的，您可以参考同一配方的*如何工作*部分。我们有一个`mongo-hadoop-emr-test`项目，其代码可以从 Packt 网站下载，用于使用 AWS EMR API 在云上创建 MapReduce 作业。为了简化工作，我们只将一个 JAR 上传到 S3 bucket 以执行 MapReduce 作业。这个 JAR 将在基于 Unix 的操作系统上使用 Windows 的 BAT 文件和 shell 脚本进行组装。`mongo-hadoop-emr-test`Java 项目有`mongo-hadoop-emr-binaries`子目录，其中包含必要的二进制文件以及将它们组装到一个 JAR 中的脚本。

子目录中还提供了组装的`mongo-hadoop-emr-assembly.jar`文件。运行`.bat`或`.sh`文件将删除此 JAR 并重新生成组装好的 JAR，这不是强制性的。已经提供的组装罐足够好，可以正常工作。Java 项目包含子目录数据，其中包含一个`postalCodes.bson`文件。这是从包含`postalCodes`集合的数据库中生成的 BSON 转储。mongo 发行版附带的`mongodump`实用程序用于提取该转储。

## 怎么做…

1.  The first step of this exercise is to create a bucket on S3\. You can choose to use an existing bucket; however, for this recipe, I am creating a `com.packtpub.mongo.cookbook.emr-in` bucket. Remember that the name of the bucket has to be unique across all the S3 buckets and you will not be able to create a bucket with this very name. You will have to create one with a different name and use it in place of `com.packtpub.mongo.cookbook.emr-in` that is used in this recipe.

    ### 提示

    不要使用下划线（`_`创建 bucket 名称）；相反，请使用连字符（`-`）。带下划线的 bucket 创建不会失败；但是，MapReduce 作业稍后将失败，因为它不接受 bucket 名称中的下划线。

2.  We will upload the assembled JAR files and a `.bson` file for the data to the newly created (or existing) S3 bucket. To upload the files, we will use the AWS web console. Click on the **Upload** button and select the assembled JAR file and the `postalCodes.bson` file to be uploaded to the S3 bucket. After uploading, the contents of the bucket should look as follows:

    ![How to do it…](img/B04831_08_01.jpg)

3.  以下步骤是从 AWS 控制台启动 EMR 作业，无需编写一行代码。我们还将看到如何使用 AWS Java SDK 启动此功能。如果您希望从 AWS 控制台启动 EMR 作业，请遵循步骤 4 至 9。按照步骤 10 和 11 使用 Java SDK 启动 EMR 作业。
4.  We will first initiate a MapReduce job from the AWS console. Visit [https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/) and click on the **Create Cluster** button. In the **Cluster Configuration** screen, enter the details as shown in the image, except for the logging bucket, which you need to select as your bucket to which the logs need to be written. You can also click on the folder icon next to the textbox for the bucket name and select the bucket present for your account to be used as the logging bucket.

    ![How to do it…](img/B04831_08_07.jpg)

    ### 注

    终端保护选项设置为**否**，因为这是一个测试实例。如果出现任何错误，我们希望实例终止，以避免它们继续运行并产生费用。

5.  In the **Software Configuration** section, select the **Hadoop version** as **2.4.0** and **AMI version** as **3.1.0 (hadoop 2.4.0)**. Remove the additional applications by clicking on the cross next to their names, as shown in the following image:

    ![How to do it…](img/B04831_08_08.jpg)

6.  In the **Hardware Configuration** section, select the **EC2 instance type** as **m1.medium**. This is the minimum that we need to select for the Hadoop version 2.4.0\. The number of instances for the slave and task instances is zero. The following image shows the configuration that is selected:

    ![How to do it…](img/B04831_08_09.jpg)

7.  在的**安全和访问**部分，保留所有默认值。我们也不需要**引导动作**，所以也请留下这个。
8.  The final step is to set up **Steps** for the MapReduce job. In the **Add step** drop down, select the **Custom JAR** option, and then select the **Auto-terminate** option as **Yes**, as shown in the following image:

    ![How to do it…](img/B04831_08_10.jpg)

    现在点击**配置**和**添加**按钮，输入详细信息。

    **JAR S3 位置**的值为`s3://com.packtpub.mongo.cookbook.emr-in/mongo-hadoop-emr-assembly.jar`。这是我的输入桶中的位置；您需要根据自己的输入 bucket 更改输入 bucket。JAR 文件的名称应该是相同的。

    在**参数**文本区输入以下参数；主类的名称位于列表的第一位：

    `com.packtpub.mongo.cookbook.TopStateMapReduceEntrypoint`

    `-Dmongo.job.input.format=com.mongodb.hadoop.BSONFileInputFormat`

    `-Dmongo.job.mapper=com.packtpub.mongo.cookbook.TopStatesMapper`

    `-Dmongo.job.reducer=com.packtpub.mongo.cookbook.TopStateReducer`

    `-Dmongo.job.output=org.apache.hadoop.io.Text`

    `-Dmongo.job.output.value=org.apache.hadoop.io.IntWritable`

    `-Dmongo.job.output.value=org.apache.hadoop.io.IntWritable`

    `-Dmongo.job.output.format=com.mongodb.hadoop.BSONFileOutputFormat`

    `-Dmapred.input.dir=s3://com.packtpub.mongo.cookbook.emr-in/postalCodes.bson`

    `-Dmapred.output.dir=s3://com.packtpub.mongo.cookbook.emr-out/`

9.  The value of the final two arguments contains the input and output bucket used for my MapReduce sample; this value will change according to your own input and output buckets. The value of Action on failure would be Terminate. The following image shows the values filled in; click on **Save** after all these details have been entered:

    ![How to do it…](img/B04831_08_11.jpg)

10.  现在点击**创建集群**按钮。这将需要一些时间来配置和启动集群。
11.  在以下几个步骤中，我们将使用 AWS Java API 在 EMR 上创建 MapReduce 作业。将代码示例附带的`EMRTest`项目导入到您喜爱的 IDE 中。导入后，打开`com.packtpub.mongo.cookbook.AWSElasticMapReduceEntrypoint`类。
12.  类中有五个需要更改的常量。它们是您将用于示例的输入、输出和日志存储桶，以及 AWS 访问和密钥。使用 AWS SDK 时，访问密钥和密钥充当用户名和密码。相应地更改这些值并运行程序。成功执行后，它应为新启动的作业提供作业 ID。
13.  Irrespective of how you initiated the EMR job, visit the EMR console at [https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/) to see the status of your submitted ID. The Job ID that you can see in the second column of your initiated job will be same as the job ID printed to the console when you executed the Java program (if you initiated using the Java program). Click on the name of the job initiated, which should direct you to the job details page. The hardware provisioning will take some time, and then finally, your map reduce step will run. Once the job has been completed, the status of the job should look as follows on the Job details screen:

    ![How to do it…](img/B04831_08_04.jpg)

    展开时，**步骤**部分应如下所示：

    ![How to do it…](img/B04831_08_05.jpg)

14.  单击日志文件部分下面的 stderr 链接，查看 MapReduce 作业的所有日志输出。
15.  Now that the MapReduce job is complete, our next step is to see the results of it. Visit the S3 console at [https://console.aws.amazon.com/s3](https://console.aws.amazon.com/s3) and visit the output bucket. In my case, the following is the content of the out bucket:

    ![How to do it…](img/B04831_08_06.jpg)

    `part-r-0000.bson`档案是我们感兴趣的。此文件包含 MapReduce 作业的结果。

16.  将文件下载到您的本地文件系统，并使用 mongorestore 实用程序在本地导入到正在运行的 mongo 实例。请注意，以下命令的还原实用程序需要一个 mongod 实例启动并运行，并侦听端口`27017`，当前目录中有`part-r-0000.bson`文件：

    ```js
    $ mongorestore part-r-00000.bson -d test -c mongoEMRResults

    ```

17.  Now, connect to the `mongod` instance using the mongo shell and execute the following query:

    ```js
    > db.mongoEMRResults.find().sort({count:-1}).limit(5)

    ```

    我们将看到查询的以下结果：

    ```js
    { "_id" : "Maharashtra", "count" : 6446 }
    { "_id" : "Kerala", "count" : 4684 }
    { "_id" : "Tamil Nadu", "count" : 3784 }
    { "_id" : "Andhra Pradesh", "count" : 3550 }
    { "_id" : "Karnataka", "count" : 3204 }

    ```

18.  这是前五名结果的预期结果。如果我们将使用第 3 章[中的 Java 客户端在 Mongo 中*执行 MapReduce 时得到的结果与使用 Mongo 的 MapReduce 框架的*编程语言驱动*和本章中*编写的第一个 Hadoop MapReduce 作业*配方进行比较，我们可以看到结果是一致的。*](03.html "Chapter 3. Programming Language Drivers")

## 它是如何工作的…

AmazonEMR 是一个托管 Hadoop 服务，它负责硬件资源调配，让您不必为设置自己的集群而烦恼。与我们的 MapReduce 程序相关的概念已经包含在*编写我们的第一个 Hadoop MapReduce 作业*配方中，没有更多的内容要提及。我们做的一件事是将我们需要的罐子组装到一个大罐子中，以执行我们的 MapReduce 任务。这种方法适合我们的小型 MapReduce 工作；对于需要大量第三方 JAR 的大型作业，我们必须采用一种方法，将 JAR 添加到 Hadoop 安装的`lib`目录中，并以与本地执行的 MapReduce 作业相同的方式执行。与本地设置不同的另一件事是，我们没有使用`mongid`实例来获取数据并将数据写入，而是使用 mongo 数据库中的 BSON dump文件作为输入，并将输出写入 BSON 文件。然后将输出转储导入本地 mongo 数据库，并对结果进行分析。将数据转储上载到 S3 存储桶是非常常见的，使用云基础设施对已上载到 S3 的数据运行分析作业是一个不错的选择。EMR 集群从存储桶访问的数据不需要具有公共访问权限，因为 EMR 作业使用我们的帐户凭据运行；我们可以访问自己的存储桶来读取和写入数据/日志。

## 另见

在尝试了这个简单的 MapReduce 作业之后，强烈建议您了解Amazon EMR 服务及其所有功能。电子病历开发者指南可在[中找到 http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/) 。

Enron 数据集中有一个 MapReduce 作业示例，它是 mongo hadoop 连接器示例的一部分。可在[找到 https://github.com/mongodb/mongo-hadoop/tree/master/examples/elastic-mapreduce](https://github.com/mongodb/mongo-hadoop/tree/master/examples/elastic-mapreduce) 。您也可以根据给定的说明选择在 AmazonEMR 上实现此示例。