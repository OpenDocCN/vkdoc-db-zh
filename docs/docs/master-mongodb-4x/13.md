# 十三、分片

分片是通过在不同的服务器（分片）上对数据集进行分区来横向扩展数据库的能力。自 2010 年 8 月 1.6 版发布以来，它一直是 MongoDB 的一项功能。Foursquare 和 Bitly 是 MongoDB 最著名的两个早期客户，从一开始到正式发布都使用了分片功能。

在本章中，我们将学习以下主题：

*   如何设计分片集群，以及如何选择分片密钥来做出关于其使用的最重要决策
*   不同的分片技术以及如何监控和管理分片集群
*   `mongos`路由器以及如何使用它在不同碎片之间路由我们的查询
*   如何从碎片中的错误中恢复

# 我们为什么使用切分？

一般来说，在数据库系统和计算系统中，我们有两种提高性能的方法。第一个是简单地用更强大的服务器替换我们的服务器，保持相同的网络拓扑和系统架构。这称为**垂直缩放**。

垂直扩展的一个优点是，从操作的角度来看，它很简单，特别是亚马逊等云提供商只需点击几下就可以将**m2.medium**替换为**m2.超大**服务器实例。另一个优点是，我们不需要对代码进行任何更改，因此几乎不存在发生灾难性错误的风险。

垂直缩放的主要缺点是有一个限制；我们只能得到与云提供商提供给我们的服务器一样强大的服务器。

一个相关的缺点是，获得更强大的服务器通常伴随着成本的增加，而成本的增加不是线性的，而是指数型的。因此，即使我们的云提供商提供了更强大的实例，我们也会在达到部门信用卡限额之前达到成本效益壁垒。

第二种提高性能的方法是使用具有相同容量的相同服务器并增加其数量。这称为**水平缩放**。

水平缩放提供了理论上能够以指数方式缩放的优势，同时对于现实世界的应用程序来说仍然足够实用。主要缺点是，它在操作上可能更复杂，需要更改代码和预先仔细设计系统。对于系统来说，水平扩展也更为复杂，因为它需要不同服务器之间通过网络链路进行通信，而网络链路不如单个服务器上的进程间通信可靠。下图显示了水平缩放和垂直缩放之间的差异：

![](img/18bb77a8-2036-44b5-b85b-16f97f38f403.png)

要了解可伸缩性，了解单服务器系统的局限性很重要。服务器通常由以下一个或多个特征绑定：

*   **CPU**：受 CPU 限制的系统是受我们 CPU 速度限制的系统。一项任务，例如可以放入 RAM 的矩阵乘法，将受到 CPU 的限制，因为必须在 CPU 中执行特定数量的步骤，而不需要任何磁盘或内存访问来完成任务。在这种情况下，CPU 使用率是我们需要跟踪的指标。

*   **I/O**：输入输出绑定系统同样受到存储系统（HDD 或 SSD）速度的限制。从磁盘读取大文件加载到内存等任务将受到 I/O 限制，因为在 CPU 处理方面几乎没有什么可做的；大部分时间都花在从磁盘读取文件上。与存储系统的实际限制相比，需要跟踪的重要指标是与磁盘访问、每秒读取和每秒写入相关的所有指标。

*   **内存和缓存**：内存绑定和缓存绑定系统受到可用 RAM 内存量和/或我们分配给它们的缓存大小的限制。将大于 RAM 大小的矩阵相乘的任务将受到内存限制，因为它需要从磁盘分页输入/输出数据以执行乘法。要跟踪的重要指标是使用的内存。这在 MongoDB MMAPv1 中可能会产生误导，因为存储引擎将通过文件系统缓存分配尽可能多的内存。

另一方面，在 WiredTiger 存储引擎中，如果我们没有为核心 MongoDB 进程分配足够的内存，内存不足错误可能会杀死它，这是我们无论如何都要避免的。

必须通过操作系统直接监控内存使用情况，也可以通过跟踪页面输入/输出数据间接监控内存使用情况。内存分页数的增加通常表明内存不足，操作系统正在使用虚拟地址空间来保持内存。

MongoDB, being a database system, is generally memory and I/O bound. Investing in SSD and more memory for our nodes is almost always a good investment. Most systems are a combination of one or more of the preceding limitations. Once we add more memory, our system may become CPU bound, as complex operations are almost always a combination of CPU, I/O, and memory usage.

MongoDB 的分片非常简单，可以进行设置和操作，这为其多年来的巨大成功做出了贡献，因为它提供了横向扩展的优势，而无需大量投入工程和操作资源。

也就是说，从一开始就进行切分非常重要，因为从操作角度来看，一旦设置好切分，就很难更改配置。切分不应该是事后的想法，而应该是设计过程早期的一个关键架构设计决策。

# 架构概述

分片群集由以下元素组成：

*   两个或多个碎片。每个碎片必须是一个副本集。
*   一个或多个查询路由器（`mongos`。`mongos`提供了我们的应用程序和数据库之间的接口。
*   配置服务器的副本集。配置服务器存储整个集群的元数据和配置设置。

这些元素之间的关系如下图所示：

![](img/0b87bede-ff79-4863-a498-c5c3cf5b86b1.png)

As of MongoDB 3.6, shards must be implemented as replica sets.

# 开发、连续部署和暂存环境

在预生产环境中，使用全套服务器可能有点过头了。出于效率原因，我们可以选择使用更简化的体系结构。

我们可以为分片部署的最简单配置如下：

*   一个`mongos`路由器
*   一个分片副本集，带有一个 MongoDB 服务器和两个仲裁器
*   一个配置服务器副本集，带有一个 MongoDB 服务器和两个仲裁器

这应该严格用于开发和测试，因为此体系结构违背了副本集提供的大多数优势，例如高可用性、可扩展性和数据复制。

Staging is strongly recommended to mirror our production environment in terms of its servers, configuration, and (if possible) dataset requirements too, in order to avoid surprises at deployment time.

# 提前计划切分

正如我们将在下一节中看到的，切分是复杂且昂贵的。重要的是要提前计划，确保在达到系统极限之前很久就开始分片过程。

关于何时需要开始分片的一些粗略准则如下：

*   当您的 CPU 利用率平均低于 70%时
*   当 I/O（尤其是写）容量小于 80%时
*   平均内存利用率低于 70%时

由于分片有助于提高写入性能，因此关注我们的 I/O 写入容量和应用程序的需求非常重要。

Don't wait until the last minute to start sharding in an already busy-up-to-the-neck MongoDB system, as it can have unintended consequences.

# 分片设置

分片是在集合级别执行的。出于几个原因，我们可以拥有不想或不需要切分的集合。我们可以将这些收藏保留为未归档。

这些集合将存储在主碎片中。MongoDB 中每个数据库的主碎片都不同。当我们在分片环境中创建新数据库时，MongoDB 会自动选择主分片。MongoDB 将选择创建时存储的数据最少的碎片。

如果要在任何其他点更改主碎片，可以发出以下命令：

```js
> db.runCommand( { movePrimary : "mongo_books", to : "UK_based" } )
```

这样，我们将名为`mongo_books`的数据库移动到名为`UK_based`的碎片上。

# 选择分片键

选择切分密钥是我们需要做出的最重要的决定：一旦我们切分数据并部署集群，更改切分密钥就变得非常困难。首先，我们将经历更改碎片密钥的过程。

# 更改分片键

在 MongoDB 中没有命令或简单的过程来更改碎片密钥。更改碎片密钥的唯一方法是备份和恢复所有数据，这在高负载生产环境中可能非常困难，甚至不可能。

以下是更改碎片密钥需要执行的步骤：

1.  从 MongoDB 导出所有数据
2.  删除原始碎片集合
3.  使用新密钥配置分片
4.  预裂新的碎片关键帧范围
5.  将数据恢复回 MongoDB

在这些步骤中，步骤 4 需要进一步解释。

MongoDB 使用块分割分片集合中的数据。如果我们从头开始引导 MongoDB 分片集群，MongoDB 将自动计算块。然后，MongoDB 会将区块分布到不同的碎片上，以确保每个碎片中的区块数量相等。

只有当我们想将数据加载到一个新切分的集合中时，我们才能真正做到这一点。

原因有三：

*   MongoDB 仅在`insert`操作后创建拆分。
*   区块迁移将该区块中的所有数据从一个碎片复制到另一个碎片。
*   `floor(n/2)`区块迁移可以在任何给定的时间发生，其中`n`是我们拥有的碎片数量。即使有三个碎片，这也只是一次`floor(1.5)=1`块迁移。

这三个限制意味着让 MongoDB 自己解决问题肯定需要更长的时间，并可能导致最终的失败。这就是为什么我们想要预分割数据，并给 MongoDB 一些关于块应该去哪里的指导。

在我们的`mongo_books`数据库和`books`集合示例中，如下所示：

```js
> db.runCommand( { split : "mongo_books.books", middle : { id : 50 } } )
```

`middle`命令参数将在`id`小于或等于`50`的文档和`id`大于`50`的文档中分割我们的密钥空间。我们的集合中不需要存在与`id`相等的`50`文档，因为这仅作为分区的指导值。

在本例中，我们选择了`50`，因为我们假设我们的键在`0`到`100`的值范围内遵循均匀分布（即，每个值的键数相同）。

We should aim to create at least 20-30 chunks to grant MongoDB flexibility in potential migrations. We can also use `bounds` and `find` instead of `middle` if we want to manually define the partition key, but both parameters need data to exist in our collection before applying them.

# 选择正确的分片键

在上一节之后，不言而喻的是，我们需要考虑碎片密钥的选择，因为这是一个我们必须坚持的决定。

一个好的切分键有三个特点：

*   高基数
*   低频
*   值的非单调变化

我们将首先回顾这三个属性的定义，以了解它们的含义：

*   **高基数**：表示 shard 键必须有尽可能多的不同值。布尔值只能接受`true`/`false`的值，因此这是一个错误的分片键选择。一个 64 位长的值字段，可以接受*中的任何值−（2^63）*至*2^63−就基数而言，1*是一个很好的切分键选择。

*   **低频**：与高基数的争论直接相关。低频切分键的值分布将接近完美的随机/均匀分布。以我们的 64 位长值为例，如果我们有一个字段可以取值范围从*到*的值，那么它对我们没有什么用处−（2^63）*至*2^63−1*如果我们一直观察 0 和 1 的值。事实上，这和使用布尔字段一样糟糕，布尔字段也只能接受两个值。如果我们有一个具有高频值的切分键，我们将得到不可分割的块。这些块无法进一步划分，并且会增大大小，从而对包含它们的碎片的性能产生负面影响。*
*   **非单调变化值**：**这意味着我们的切分键不应该是，例如，一个总是随着每次新插入而增加的整数。如果我们选择一个单调递增的值作为分片键，这将导致所有写操作都以最后一个分片结束，从而限制我们的写性能。**

**If we want to use a monotonically changing value as the shard key, we should consider using hash-based sharding.

在下一节中，我们将描述不同的分片策略，包括它们的优缺点。

# 基于距离的分片

默认和最广泛使用的分片策略是基于范围的分片。此策略将我们集合的数据分割成块，将文档与同一碎片中的邻近值分组。

对于我们的示例数据库和集合，`mongo_books`和`books`分别有以下内容：

```js
> sh.shardCollection("mongo_books.books", { id: 1 } )
```

这将在`id`上以上升方向创建一个基于范围的切分键。我们的分片键的方向将决定哪些文档将在第一个分片中结束，哪些文档将在后续的分片中结束。

如果我们计划进行基于范围的查询，这是一个很好的策略，因为这些查询将被定向到保存结果集的碎片，而不必查询所有碎片。

# 基于散列的分片

如果我们没有实现前面提到的三个目标的切分键（或者无法创建切分键），那么我们可以使用基于散列的切分的替代策略。在本例中，我们使用查询隔离进行数据分发交易。

基于散列的分片将采用我们的分片键的值，并以一种确保接近均匀分布的方式对它们进行散列。这样，我们可以确保我们的数据将均匀地分布在碎片上。缺点是只有精确匹配查询会被路由到保存值的精确切分。任何范围查询都必须从所有碎片中取出数据。

对于我们的示例数据库和集合（`mongo_books`和`books`分别），我们有以下内容：

```js
> sh.shardCollection("mongo_books.books", { id: "hashed" } )
```

与前面的示例类似，我们现在使用`id`字段作为哈希切分键。

Suppose we use fields with float values for hash-based sharding. Then we will end up with collisions if the precision of our floats is more than *2^53*. These fields should be avoided where possible.

# 拿出我们自己的钥匙

基于范围的分片不需要局限于单个键。事实上，在大多数情况下，我们希望组合多个键以实现高基数和低频率。

一种常见的模式是将低基数第一部分（但仍然具有比我们拥有的碎片数量多两倍的不同值）与高基数键组合作为其第二个字段。这从切分键的第一部分实现读写分布，然后从第二部分实现基数和读局部性。

另一方面，如果我们没有范围查询，那么我们可以在主键上使用基于散列的分片，因为这将准确地针对我们要查找的分片和文档。

为了使事情变得更复杂，这些考虑因素可能会随着我们的工作量而变化。几乎完全由读取（比如 99.5%）组成的工作负载不关心写入分布。我们可以使用内置的`_id`字段作为切分键，这只会给最后一个切分增加 0.5%的负载。我们的读取仍将分布在碎片上。不幸的是，在大多数情况下，这并不简单。

# 基于位置的数据

由于政府法规和希望我们的数据尽可能接近用户的愿望，通常存在限制，需要限制特定数据中心中的数据。通过在不同的数据中心放置不同的碎片，我们可以满足这一要求。

Every shard is essentially a replica set. We can connect to it as we would connect to a replica set for administrative and maintenance operations. We can query one shard's data directly, but the results will only be a subset of the full sharded result set.

# 分片管理和监测

与单服务器或副本集部署相比，分片 MongoDB 环境有一些独特的挑战和限制。在本节中，我们将探讨 MongoDB 如何使用块在碎片之间平衡数据，以及如果需要，我们如何调整它们。我们将一起探讨 sharding 的一些设计限制。

# 平衡数据–如何跟踪和保持数据平衡

MongoDB 中分片的优点之一是它对应用程序几乎是透明的，并且只需要很少的管理和操作工作。

MongoDB 需要持续执行的核心任务之一是在碎片之间平衡数据。无论我们实现的是基于范围的分片还是基于散列的分片，MongoDB 都需要计算散列字段的边界，以便能够确定每个新文档插入或更新指向哪个分片。随着数据的增长，这些界限可能需要重新调整，以避免出现一个以我们的大部分数据结束的热碎片。

在本例中，假设有一个名为`extra_tiny_int`的数据类型，其整数值来自`-12`、`12`。如果我们在`extra_tiny_int`字段上启用分片，那么数据的初始边界将是`$minKey: -12`和`$maxKey: 11`表示的整个值范围。

插入一些初始数据后，MongoDB 将生成块并重新计算每个块的边界，以尝试平衡数据。

By default, the initial number of chunks created by MongoDB is *2 × number of shards*.

在两个碎片和四个初始块的情况下，初始边界的计算如下：

*区块 1:[-12..-6）*

*Chunk2:[-6..0）*

*Chunk3:[0..6）*

*Chunk4:[6,12）*其中*['*包含在内，而*''*不包含在内

下图说明了上述说明：

![](img/75712448-e2de-4265-a8ce-458548db4503.png)

插入一些数据后，我们的块将如下所示：

*   *沙尔达*：
    *   *区块 1*：*-12、-8、-7*
    *   *区块 2*：*-6*
*   **ShardB*：

    *   *区块 3*：*0、2*
    *   *区块 4*：*7,8,9,10,11,11,11,11** 

 *下图说明了上述说明：

![](img/7b631e16-8db1-4e9d-b9d3-56585f600f75.png)

在这种情况下，我们观察到 c*hunk4*比任何其他块都有更多的项。MongoDB 将首先将*chunk4*拆分为两个新的区块，试图将每个区块的大小保持在某个阈值（默认为 64MB）以下。

现在，我们有了*chunk4A*：**7,8,9,10*和*chunk4B*：*11,11,11,11*，而不是 c*hunk4*。*

 *下图说明了上述说明：

![](img/88357d07-47cd-4fc3-b6b3-a01a761d08f2.png)

这方面的新界限如下：

*   *区块 4A*：*[6,11】*
*   *区块 4B*：*【11,12】*

注意*chunk4B*只能保存一个值。现在，这是一个不可分割的块—一个不能再分解为更小的块的块，它的大小将无限增长，从而导致潜在的性能问题。

这澄清了为什么我们需要使用一个高基数字段作为切分键，以及为什么像布尔值这样只有`true`/`false`值的字段对于切分键来说是一个不好的选择。

在我们的例子中，*ShardA*中有两个块，*ShardB*中有三个块。让我们看下表：

| **块数** | **迁移阈值** |
| *≤19* | *2* |
| *20-79* | *4* |
| *≥80* | *8* |

我们还没有达到迁移阈值，因为*3-2=1*。

迁移阈值计算为具有最高块计数的碎片中的块数和具有最低块计数的碎片中的块数，如下所示：

*   *Shard1->85 块*
*   *Shard2->86 块*
*   *碎片 3->92 块*

在上例中，由于*的迁移阈值为*8*，所以在*Shard3*（或*Shard2*达到*93*块之前不会发生平衡≥80 个*块，*Shard1*和*Shard3*之间的差异仍然是*7*块（*92-85*。

如果我们继续在*chunk4A*中添加数据，最终会被拆分为*chunk4A1*和*chunk4A2*。

现在我们在*ShardB*（*chunk3*、*chunk4A1*、*chunk4A2*、*chunk4B*中有四个块，*ShardA*中有两个块（*chunk1*和【T14 chunk2】。

下图说明了区块与碎片的关系：

![](img/c609adaf-fda7-40e0-b569-ccb75e021fcc.png)

MongoDB 平衡器现在将一个区块从*ShardB*迁移到*ShardA*作为*4-2=2*，达到迁移阈值的区块少于*20*。均衡器将调整两个碎片之间的边界，以便能够更有效地查询（目标查询）。下图说明了前面的解释：

![](img/43018e6d-96c8-4920-a51f-566b0211645a.png)

如上图所示，MongoDB 将尝试将*>64*MB 块大小一分为二。如果我们的数据分布从一开始就不均匀，那么两个结果块之间的界限可能完全不均匀。MongoDB 可以将块分割成更小的块，但不能自动合并它们。我们需要手动合并块，这是一个微妙且操作成本高昂的过程。

# 区块管理

大多数情况下，我们应该将区块管理留给 MongoDB。当我们将配置从副本集更改为分片时，在接收到初始数据加载时，我们应该在开始时手动管理块。

# 移动块

要手动移动区块，我们需要在连接到`mongos`和`admin`数据库后发出以下命令：

```js
> db.runCommand( { moveChunk : 'mongo_books.books' ,
 find : {id: 50},
 to : 'shard1.packtdb.com' } )
```

使用前面的命令，我们将包含带有`id: 50`的文档的区块（必须是碎片密钥）从`mongo_books`数据库的`books`集合移动到名为`shard1.packtdb.com`的新碎片。

我们还可以更明确地定义要移动的块的边界。现在语法如下所示：

```js
> db.runCommand( { moveChunk : 'mongo_books.books' ,
 bounds :[ { id : <minValue> } ,
 { id : <maxValue> } ],
 to : 'shard1.packtdb.com' } )
```

这里，`minValue`和`maxValue`是我们从`db.printShardingStatus()`得到的值。

在前面使用的示例中，对于*chunk2*，`minValue`将是`-6`，而`maxValue`将是`0`。

Do not use `find` in hash-based sharding. Use `bounds` instead.

# 更改默认块大小

要更改默认块大小，我们需要连接到`mongos`路由器，然后连接到`config`数据库。

然后我们发出以下命令，将我们的全局`chunksize`更改为`16`MB：

```js
> db.settings.save( { _id:"chunksize", value: 16 } )
```

更改`chunksize`的主要原因来自以下情况：64MB 的默认`chunksize`可能会导致超出硬件处理能力的 I/O。在这种情况下，定义较小的`chunksize`将导致更频繁但数据密集型的迁移。

更改默认块大小有以下缺点：

*   通过定义较小的块大小来创建更多拆分无法自动撤消。
*   增加区块大小不会强制任何区块迁移；相反，块将通过插入和更新而增长，直到它们达到新的大小。
*   降低区块大小可能需要相当长的时间才能完成。

*   只有在插入或更新时，才会根据新块大小（如果较低）进行自动拆分。我们可能有没有得到任何写操作的块，因此大小不会改变。

块大小可以是 1 到 1024MB。

# 大块

在极少数情况下，我们可能会得到巨型块，这些块大于块大小，并且不能被 MongoDB 分割。如果区块中的文档数量超过最大文档限制，我们也可能会遇到同样的情况。

这些区块将启用`jumbo`标志。理想情况下，MongoDB 将跟踪它是否可以分割数据块，并且，一旦可以，它将被分割；但是，我们可能决定在 MongoDB 之前手动触发剥离。

方法如下：

1.  通过 shell 连接到您的`mongos`路由器并运行以下操作：

```js
> sh.status(true)
```

2.  使用以下代码识别其描述中包含`jumbo`的区块：

```js
databases:
…
mongo_books.books
...
chunks:
…
 shardB  2
 shardA  2
 { "id" : 7 } -->> { "id" : 9 } on : shardA Timestamp(2, 2) jumbo
```

3.  手动调用`splitAt()`或`splitFind()`，使用以下代码拆分`id`处`mongo_books`数据库`books`集合上等于`8`的区块：

```js
> sh.splitAt( "mongo_books.books", { id: 8 })
```

The `splitAt()` function will split based on the split point we define. The two new splits may or may not be balanced.

或者，如果我们想将其留给 MongoDB 来找到分割区块的位置，我们可以使用`splitFind`，如下所示：

```js
> sh.splitFind("mongo_books.books", {id: 7})
```

`splitFind`短语将尝试找到`id:7`查询所属的块，并自动为分割的块定义新的边界，以便它们大致平衡。

在这两种情况下，MongoDB 都将尝试分割区块，如果成功，它将从区块中删除`jumbo`标志。

4.  如果前面的操作不成功，那么，也只有到那时，我们才应该尝试首先停止平衡器，同时验证输出并等待任何挂起的迁移首先完成，如以下代码所示：

```js
> sh.stopBalancer()
> sh.getBalancerState() > use config
while( sh.isBalancerRunning() ) {
 print("waiting...");
 sleep(1000);
} 
```

这应该返回`false `

5.  等待任何`waiting…`消息停止打印，然后以与之前相同的方式找到`jumbo`标记的区块。
6.  然后更新`mongos`路由器`config`数据库中的`chunks`集合，如下所示：

```js
> db.getSiblingDB("config").chunks.update(
 { ns: "mongo_books.books", min: { id: 7 }, jumbo: true },
 { $unset: { jumbo: "" } }
)
```

前面的命令是一个常规的`update()`命令，第一个参数是确定要更新哪个文档的`find()`部分，第二个参数是要应用于该文档的操作（`$unset: jumbo flag`。

7.  完成所有这些操作后，我们重新启用平衡器，如下所示：

```js
> sh.setBalancerState(true)
```

8.  然后，我们连接到`admin`数据库，将新配置刷新到所有节点，如下所示：

```js
> db.adminCommand({ flushRouterConfig: 1 } )
```

Always back up the `config` database before modifying any state manually.

# 合并块

如前所述，MongoDB 通常会调整切分中每个块的边界，以确保数据分布均匀。这在某些情况下可能不起作用，特别是当我们手动定义块时，如果我们的数据分布异常不平衡，或者如果我们的碎片中有许多`delete`操作。

拥有空块将调用不必要的块迁移，并给 MongoDB 一个错误的印象，即需要迁移哪个块。正如我们之前所解释的，区块迁移的阈值取决于每个碎片所持有的区块数量。当需要时，拥有空块可能会触发平衡器，也可能不会触发平衡器。

块合并只能在至少一个块为空时发生，并且只能在相邻块之间发生。

为了找到空块，我们需要连接到我们想要检查的数据库（在我们的例子中为`mongo_books`），并使用`runCommand`，其中`dataSize`设置如下：

```js
> use mongo_books
> db.runCommand({
 "dataSize": "mongo_books.books",
 "keyPattern": { id: 1 },
 "min": { "id": -6 },
 "max": { "id": 0 }
})
```

`dataSize`短语遵循`database_name.collection_name`模式，而`keyPattern`是我们为这个集合定义的切分键。

`min`和`max`值应该通过我们在这个集合中拥有的块来计算。在我们的例子中，我们输入了本章前面示例中的*chunkB*详细信息。

如果我们查询的边界（在我们的例子中是*chunkB*的边界）没有返回任何文档，结果将类似于以下内容：

```js
{ "size" : 0, "numObjects" : 0, "millis" : 0, "ok" : 1 }
```

既然我们知道*chunkB*没有数据，我们可以将其与另一个 chunk 合并（在我们的例子中，这只能是*chunkA*），如下所示：

```js
> db.runCommand( { mergeChunks: "mongo_books.books",
 bounds: [ { "id": -12 },
 { id: 0 } ]
 } )
```

成功后，将返回 MongoDB 的默认`ok`状态消息，如下代码所示：

```js
{ "ok" : 1 }
```

然后我们可以通过再次调用`sh.status()`来验证*ShardA*上只有一个区块。

# 添加和删除碎片

向我们的集群添加一个新的分片就像连接到`mongos`一样简单，连接到`admin`数据库，然后通过以下方式调用`runCommand`：

```js
> db.runCommand( {
addShard: "mongo_books_replica_set/rs01.packtdb.com:27017", maxSize: 18000, name: "packt_mongo_shard_UK"
} )
```

这将从运行在端口`27017`上的`rs01.packtdb.com`主机的名为`mongo_books_replica_set`的副本集中添加一个新的碎片。我们还将此分片的数据的`maxSize`定义为`18000`MB（或者我们可以将其设置为`0`以无限制），并将新分片的名称定义为`packt_mongo_shard_UK`。

This operation will take quite some time to complete as chunks will have to be rebalanced and migrated to the new shard.

另一方面，删除碎片需要更多的参与，因为我们必须确保在删除过程中不会丢失任何数据。我们的做法如下：

1.  首先，我们需要确保使用`sh.getBalancerState()`启用平衡器。然后，在使用`sh.status()`、`db.printShardingStatus()`或`listShards admin`命令中的任何一个确定要删除的碎片后，我们连接到`admin`数据库并调用`removeShard`，如下所示：

```js
> use admin
> db.runCommand( { removeShard: "packt_mongo_shard_UK" } )
```

输出应包含以下内容：

```js
...
 "msg" : "draining started successfully",
 "state" : "started",
...
```

2.  然后，如果我们再次调用相同的命令，我们将得到以下结果：

```js
> db.runCommand( { removeShard: "packt_mongo_shard_UK" } )
…
"msg" : "draining ongoing",
 "state" : "ongoing",
 "remaining" : {
 "chunks" : NumberLong(2),
 "dbs" : NumberLong(3)
 },
…
```

结果中的剩余文档包含仍在传输的`chunks`和`dbs`的编号。在我们的例子中，它分别是`2`和`3`。

All the commands need to be executed in the `admin` database.

如果要删除的碎片用作它所包含的一个或多个数据库的主碎片，则删除碎片时可能会出现额外的复杂性。当我们启动分片时，主分片由 MongoDB 分配，因此当我们移除分片时，我们需要手动将这些数据库移动到新的分片。

3.  我们将通过查看`removeShard()`结果的以下部分来了解是否需要执行此操作：

```js
...
"note" : "you need to drop or movePrimary these databases",
 "dbsToMove" : [
 "mongo_books"
 ],
...
```

我们需要删除或`movePrimary`我们的`mongo_books`数据库。这样做的方法是首先确保我们已连接到`admin`数据库。

We need to wait for all of the chunks to finish migrating before running this command.

4.  继续之前，请确保结果包含以下内容：

```js
 ..."remaining" : {
 "chunks" : NumberLong(0) }...
```

5.  只有在确保要移动的块降到零后，我们才能安全地运行以下命令：

```js
> db.runCommand( { movePrimary: "mongo_books", to: "packt_mongo_shard_EU" })
```

6.  此命令将调用阻塞操作，当它返回时，应具有以下结果：

```js
{ "primary" : "packt_mongo_shard_EU", "ok" : 1 }
```

7.  在我们全部完成后调用相同的`removeShard()`命令应返回以下结果：

```js
> db.runCommand( { removeShard: "packt_mongo_shard_UK" } )

... "msg" : "removeshard completed successfully",
 "state" : "completed",
 "shard" : "packt_mongo_shard_UK"
 "ok" : 1
...
```

8.  一旦我们到达`state`as`completed`和`ok`as`1`，就可以安全地移除我们的`packt_mongo_shard_UK`碎片。

移除碎片自然比添加碎片更复杂。在我们的实时集群上执行潜在的破坏性操作时，我们需要留出一些时间，抱最好的希望，做最坏的打算。

# 切分限制

切分具有很大的灵活性。不幸的是，我们执行某些操作的方式存在一些限制。

我们将强调以下列表中最重要的内容：

*   `group()`数据库命令不起作用。无论如何都不应使用`group()`命令；改用`aggregate()`和聚合框架，或`mapreduce()`。
*   `db.eval()`命令不起作用，在大多数情况下，出于安全原因，应禁用该命令。
*   更新的`$isolated`选项不起作用。这是分片环境中缺少的功能。`update()`的`$isolated`选项保证，如果我们一次更新多个文档，其他读者和作者将不会看到使用新值更新的部分文档，而其他的仍将保留旧值。在非分片环境中实现这一点的方法是持有全局写锁和/或将操作序列化到单个线程，以确保受`update()`影响的文档的每个请求不会被其他线程/操作访问。此实现意味着它不执行且不支持任何并发，这使得禁止在分片环境中允许`$isolated`操作符。
*   不支持查询的`$snapshot`运算符。`find()`光标中的`$snapshot`运算符可防止文档在更新后被移动到磁盘上的其他位置，从而在结果中出现多次。`$snapshot`运营商的运营成本很高，通常不是一项严格的要求。替代它的方法是使用一个索引来查询一个字段，该字段的键在查询期间不会改变。

*   如果查询不包含 shard 键，则索引无法覆盖查询。分片环境中的结果将来自磁盘，而不是索引。唯一的例外是我们只查询内置的`_id`字段，只返回`_id`字段，在这种情况下，MongoDB 仍然可以使用内置索引覆盖查询。
*   `update()`和`remove()`操作的工作方式不同。切分环境中的所有`update()`和`remove()`操作必须包括受影响文档的`_id`或切分密钥；否则，`mongos`路由器将不得不对所有集合、数据库和碎片进行全表扫描，这在操作上非常昂贵。
*   跨碎片的唯一索引需要包含碎片键作为索引前缀。换句话说，为了实现跨碎片文档的唯一性，我们需要遵循 MongoDB 对碎片遵循的数据分布。
*   碎片密钥的大小必须达到 512 字节。切分键索引必须在得到切分的键字段上按升序排列，也可以选择其他字段，或者在其上按哈希索引排列。

文档中的分片键值也是不可变的。如果我们的`User`集合的切分键为`email`，则在设置之后，我们无法更新任何用户的`email`值。

# 查询分片数据

使用 MongoDB 碎片查询数据不同于单个服务器部署或副本集。我们没有连接到单一服务器或副本集的主服务器，而是连接到`mongos`路由器，该路由器决定向哪个碎片请求数据。在本节中，我们将探讨查询路由器是如何操作的，并使用 Ruby 来说明这对于开发人员来说与副本集有多么相似。

# 查询路由器

查询路由器，也称为`mongos`进程，充当 MongoDB 集群的接口和入口点。应用程序连接到它，而不是连接到底层碎片和副本集；`mongos`执行查询，收集结果并将其传递给我们的应用程序。

`mongos`进程没有任何持久状态，通常系统资源不足。

The `mongos` process is typically hosted in the same instance as the application server.

它充当请求的代理。当查询进入时，`mongos`将检查并决定哪些碎片需要执行查询，并在每个碎片中建立一个游标。

# 发现

如果我们的查询包括分片密钥或分片密钥的前缀，`mongos`将执行目标操作，仅查询包含我们正在查找的密钥的分片。

例如，在我们的`User `集合上使用`{_id, email, address}`的复合碎片密钥，我们可以对以下任何查询执行目标操作：

```js
> db.User.find({_id: 1})
> db.User.find({_id: 1, email: 'alex@packt.com'})
> db.User.find({_id: 1, email: 'janluc@packt.com', address: 'Linwood Dunn'})
```

这些查询由前缀（如前两个）或完整的切分键组成。

另一方面，`{email, address}`或`{address}`上的查询将无法针对正确的碎片，从而导致广播操作。广播操作是不包括分片密钥或分片密钥前缀的任何操作，它们导致`mongos`查询每个分片并从中收集结果。它们也称为**分散和聚合操作**或**扇出查询**。

This behavior is a direct result of the way indexes are organized, and is similar to the behavior that we identified in the chapter about indexing.

# 排序/限制/跳过

如果要对结果进行排序，我们有以下两个选项：

*   如果我们在排序条件中使用 shard 键，`mongos`可以确定查询 shard 的顺序。这将导致高效且目标明确的操作。

*   如果我们在排序条件中没有使用 shard 键，那么，就像没有任何排序条件的查询一样，它将是一个扇出查询。为了在不使用 shard 键的情况下对结果进行排序，主 shard 会在本地执行分布式合并排序，然后将排序结果集传递给`mongos`。

在每个单独的分片上执行查询限制，然后再在`mongos`级别执行查询限制，因为可能有多个分片的结果。另一方面，`skip`运算符不能传递给单个碎片，在本地检索所有结果后，`mongos`将应用该运算符。

If we combine the `skip` and `limit` operators, `mongos` will optimize the query by passing both values to individual shards. This is particularly useful in cases such as pagination. If we query without `sort` and the results are coming from more than one shard, `mongos` will round robin across shards for the results.

# 更新/删除

在文档修改器操作中，例如`update`和`remove`，我们遇到了与`find`类似的情况。如果我们在修饰符的`find`部分中有 shard 键，那么`mongos`可以将查询指向相关的 shard。

如果我们在`find`部分中没有 shard 键，那么它将再次成为扇出操作。

The `UpdateOne`, `replaceOne`, and `removeOne` operations must have the shard key or the `_id` value.

下表总结了可用于分片的操作：

| **作业类型** | **查询拓扑** |
| 插入 | 必须有碎片钥匙 |
| 使现代化 | 你能拿到碎片钥匙吗 |
| 使用分片键进行查询 | 目标操作 |
| 无分片键查询 | 分散和聚合操作/扇出查询 |
| 带分片键的索引、排序查询 | 目标操作 |
| 无分片键的索引、排序查询 | 分布式排序合并 |

# 使用 Ruby 进行查询

使用 Ruby 连接到分片集群与连接到副本集没有什么不同。使用官方 Ruby 驱动程序，我们必须配置`client`对象来定义`mongos`服务器集，如下代码所示：

```js
client = Mongo::Client.new('mongodb://key:password@mongos-server1-host:mongos-server1-port,mongos-server2-host:mongos-server2-port/admin?ssl=true&authSource=admin')
```

然后，`mongo-ruby-driver`将返回一个`client`对象，这与从 Mongo Ruby 客户端连接到副本集没有什么不同。然后，我们可以像前几章一样使用`client`对象，并在查询和性能方面注意分片与独立服务器或副本集的行为不同。

# 与副本集的性能比较

开发人员和架构师总是在寻找比较副本集和分片配置之间性能的方法。

MongoDB 实现分片的方式是基于副本集之上的。生产中的每个碎片都应该是一个副本集。性能上的主要差异来自扇出查询。当我们在没有 shard 密钥的情况下进行查询时，MongoDB 的执行时间受到性能最差的副本集的限制。此外，当使用不带 shard 键的排序时，主服务器必须在整个数据集上实现分布式合并排序。这意味着它必须收集来自不同碎片的所有数据，对它们进行合并排序，并将它们按排序方式传递给`mongos`。在这两种情况下，网络延迟和带宽限制都会降低操作速度，这是复制集无法做到的。

另一方面，通过拥有三个分片，我们可以将工作集需求分布在不同的节点上，从而从 RAM 获得结果，而不是接触底层存储、HDD 或 SSD。

另一方面，写操作可以显著加快，因为我们不再受单个节点的 I/O 容量的限制，并且我们可以在尽可能多的节点中进行写操作，就像有碎片一样。总之，在大多数情况下，特别是对于使用 shard 键的情况，通过切分，查询和修改操作都会显著加快。

The shard key is the single most important decision in sharding, and should reflect and apply to our most common application use cases.

# 碎片恢复

在本节中，我们将探讨不同的故障类型，以及如何在分片环境中进行恢复。分布式系统中的故障可能有多种形式。在本节中，我们将介绍所有可能的情况，从像`mongos`这样的无状态组件失败的最简单情况到整个碎片崩溃的情况。

# 芒戈人

`mongos`进程是一个相对轻量级的进程，不包含任何状态。如果进程失败，我们可以重新启动它或在另一台服务器上启动一个新进程。建议将`mongos`进程与我们的应用程序放在同一台服务器上，因此使用我们在应用程序服务器中共用的`mongos`服务器组从我们的应用程序进行连接是有意义的，以确保`mongos`进程的高可用性。

# 蒙哥德

`mongod`进程在分片环境中失败与在副本集中失败无异。如果是次节点，则主节点和其他次节点（假设有三个节点副本集）将照常继续。

如果它是一个`mongod`进程作为一个主要进程，那么一轮选举将开始在这个碎片（实际上是一个复制集）中选举一个新的主要进程。

在这两种情况下，我们都应该积极监控并尝试尽快修复节点，因为我们的可用性可能会受到影响。

# 配置服务器

从 MongoDB 3.4 开始，配置服务器也配置为副本集。配置服务器失败与常规`mongod`进程失败没有什么不同。我们应该监控、记录和修复该过程。

# 碎片掉下来了

丢失整个碎片是非常罕见的，并且在许多情况下可以归因于网络分区，而不是进程失败。当一个碎片下降时，所有将进入该碎片的操作都将失败。我们可以（也应该）在应用程序级别实现容错，允许应用程序恢复可以完成的操作。

选择一个可以轻松映射到我们操作端的切分键也会有所帮助；例如，如果我们的分片密钥基于位置，我们可能会丢失欧盟分片，但仍然能够通过我们的美国分片写入和读取有关美国客户的数据。

# 整个集群都会崩溃

如果我们失去了整个集群，我们除了尽快恢复并运行它之外，别无选择。重要的是要进行监控，并制定适当的流程，以了解在发生这种情况时需要做什么、何时以及由谁来做。

在整个集群停机时进行恢复基本上需要从备份中恢复并设置新的碎片，这很复杂，而且需要时间。在登台环境中进行干式测试也是可取的，通过 MongoDB Ops Manager 或任何其他备份解决方案投资于常规备份也是可取的。

A member of each shard's replica set could be in a different location for disaster-recovery purposes.

# 进一步阅读

建议您使用以下来源深入研究分片：

*   Kristina Chodorow 的*Scaling MongoDB*
*   *MongoDB：克里斯蒂娜·乔多罗和迈克尔·迪洛夫的权威指南*
*   [https://docs.mongodb.com/manual/sharding/](https://docs.mongodb.com/manual/sharding/)
*   [https://www.mongodb.com/blog/post/mongodb-16-released](https://www.mongodb.com/blog/post/mongodb-16-released)
*   [https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469](https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469)

*   [https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and](https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and)
*   [http://plusnconsulting.com/post/mongodb-sharding-and-chunks/](http://plusnconsulting.com/post/mongodb-sharding-and-chunks/)
*   [https://github.com/mongodb/mongo/wiki/Sharding-Internals](https://github.com/mongodb/mongo/wiki/Sharding-Internals)
*   [http://learnmongodbthehardway.com/schema/sharding](http://learnmongodbthehardway.com/schema/sharding)
*   [http://learnmongodbthehardway.com/schema/sharding/](http://learnmongodbthehardway.com/schema/sharding/)
*   [https://www.infoq.com/news/2010/08/MongoDB-1.6](https://www.infoq.com/news/2010/08/MongoDB-1.6)
*   [http://www.pc-freak.nimg/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png](http://www.pc-freak.nimg/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png)

# 总结

在本章中，我们探讨了分片，这是 MongoDB 最有趣的特性之一。我们从切分的体系结构概述开始，接着讨论如何设计切分并选择正确的切分键。

我们了解了监控、管理以及切分带来的限制。我们还了解了`mongos`，MongoDB 分片路由器，它将我们的查询导向正确的分片。最后，我们讨论了 MongoDB 分片环境中常见故障类型的恢复。

关于容错和高可用性的下一章将提供一些有用的提示和技巧，这在其他 11 章中没有介绍。****