# 九、Azure Cosmos DB–MongoDB API 高级服务

最后，我们已经到了旅程的最后一章，我想分享一些适用于日常场景的要点。

## 聚合管道

这是 MongoDB 的固有特性，对于需要分析和特定聚合的工作负载至关重要。在 Azure Cosmos DB 中，支持数据聚合管道。然而，在写这本书的时候，它是公开预览的，必须通过导航到 Azure portal 中 Azure Cosmos DB blade 下的预览项目来显式启用。

现在，让我们把手弄脏。打开你最喜欢的 MongoDB 控制台，连接 Azure Cosmos DB。

```js
sudo mongo <Azure Cosmos DB Account Name>.documents.azure.com:10255/db -u < Azure Cosmos DB Account Name > -p <primary/secondary key> --ssl --sslAllowInvalidCertificates

```

以下(列表 [9-1](#Par6) )是示例文档和命令，用于汇总每个传感器的消息数(列表 [9-2](#Par7) ):

```js
{ "_id" : ObjectId("5acafc5e2a90b81dc44b3963"), "SiteId" : 0, "DeviceId" : 0, "SensorId" : 0, "Temperature" : "20.9", "TestStatus" : "Pass", "TimeStamp" : ISODate("2018-03-10T05:38:34.835Z"), "deviceidday" : "03/10/2018" }

Listing 9-1Sample Document

```

```js
globaldb:PRIMARY> db.book.aggregate([{$match:{TestStatus: "Pass", DeviceId:6} },{$group:{_id: "$SensorId", total: {$sum: 1}}},{ $sort:{SensorId: -1}}]);

Listing 9-2Aggregate Command to Count Messages per Sensor, Where DeviceId Is 6 and TestStatus Is Pass

```

输出如下:

```js
{ "_id" : 0, "total" : 173 }
{ "_id" : 1, "total" : 173 }
{ "_id" : 2, "total" : 173 }
{ "_id" : 3, "total" : 173 }
{ "_id" : 4, "total" : 173 }
{ "_id" : 5, "total" : 173 }
{ "_id" : 6, "total" : 173 }
{ "_id" : 7, "total" : 173 }
{ "_id" : 8, "total" : 173 }
{ "_id" : 9, "total" : 173 }

```

现在，让我们创建另一个示例。在这个例子中(清单 [9-3](#Par11) ，我们将使用`db.runCommand`而不是`db.collection.find().count`。原因很简单，如果孤立文档存在或者正在进行分区负载平衡，那么`db.collection.find().count()`会导致不准确的计数。为了避免这种情况，建议在 MongoDB 中使用带有分片集群的`db.runCommand`。默认情况下，在 Azure Cosmos DB 中，每个实例都由一个分区组成；所以计数用`db.runCommand`代替`db.Collection.find().` `count`比较合适。

```js
globaldb:PRIMARY> db.runCommand({ count: "book",query: {"DeviceId": {$gte:10} }} )

Listing 9-3Counting the Number of Documents in a Collection Named “book” That Has DeviceId>10

```

输出如下:

```js
{ "_t" : "CountResponse", "ok" : 1, "n" : NumberLong(81828) }

```

让我们给`count`命令列表 [9-4](#Par15) 增加一些复杂性。

```js
globaldb:PRIMARY> db.runCommand({ count: "book",query: {"DeviceId": {$gte:10} }, skip: 10} )
output
{ "_t" : "CountResponse", "ok" : 1, "n" : NumberLong(81818) }

Listing 9-4Counting the Number of Documents in a Collection Named “book” That Has DeviceId Greater Than Ten and Skips the First Ten Rows

```

清单 [9-5](#Par17) 中的代码在集合“book”中获得不同的`DeviceID`

```js
globaldb:PRIMARY> db.runCommand({distinct: "book", key:"DeviceId"})
{
    "_t" : "DistinctResponse",
    "ok" : 1,
    "waitedMS" : NumberLong(0),
    "values" : [
            "20.9"
    ]
}

Listing 9-5Selecting a Distinct Value for a Specified Key

```

列表 [9-6](#Par19) 根据基础对设备进行分组，并计算每个设备中的传感器数量。

```js
globaldb:PRIMARY> db.runCommand({aggregate: "book", pipeline:[{$group: { _id: "$DeviceId", count: {$sum : 1 }} }] })

Listing 9-6Grouping Devices by Basis and Counting the Number of Sensors in Each

```

上述清单的输出如下:

```js
{   "result" : [
    "_t" : "AggregationPipelineResponse`1",
    "ok" : 1,       "_id" : "DeviceId",
    "waitedMS" : NumberLong(0),051
    "cursor" : {
    ]       "ns" : "db.book",
}           "id" : NumberLong(0),
globaldb:PRIMARY"firstBatch" : [
            {
                    "_id" : 0,
                    "count" : 20
            },
            {
                    "_id" : 1,
                    "count" : 2
                    },
                    {
                            "_id" : 2,
                            "count" : 20
                    },
                    {
                            "_id" : 3,
                            "count" : 2
                    }
                    ]}}

```

使用`$match`，您可以聚合命令(清单 [9-7](#Par23) )。

```js
db.book.aggregate( [
  { $match: { "$Temperature": { gte: 0 } } },
  {
    $group: {
      "_id": "$DeviceId",
      "avgTemperature": { "$avg": "$Temperature" }
    }
  }
] )

Listing 9-7Aggregating the Query to Identify the Average Temperature vs. DeviceId

```

输出如下:

```js
{ "_id" : 0, "avgDevice" : 0 }
{ "_id" : 1, "avgDevice" : 1 }
{ "_id" : 2, "avgDevice" : 2 }

```

让我们用`$project`和`$match`来表示条件。

```js
db.eventmsgsd.aggregate( [ {$match:{DeviceId:1001}},  {      $project: {      Temperature: 1,         "DeviceId": 1,         "SensorId" : 1,         "SiteId": {            $cond: {               if: { $eq: [ 1, "$SiteId" ] },               then: "$$REMOVE",               else: "$SiteId"            }         }      }   }] );

```

输出如下:

```js
{ "_id" : ObjectId("5b0449132a90b84018822f96"), "Temperature" : "20.9", "DeviceId" : 1001, "SensorId" : 1001003 }
{ "_id" : ObjectId("5b0449132a90b84018822f97"), "Temperature" : "20.9", "DeviceId" : 1001, "SensorId" : 1001004 }

```

正如你所看到的，Azure Cosmos DB 支持大多数聚合表达式和管道阶段，在大多数情况下，允许应用开发者快速迁移到 Azure Cosmos DB，而无需更改任何代码。

## 火花连接器

这是收集/分析数据的最丰富、最有效的方式。MongoDB 的 Spark 连接器也可以用在这里。

让我们一步一步地经历这个过程。

第 1 步:调配 HD Insight 并为其增添活力。为此，导航至`portal.azure.com`，点击创建资源，搜索 HDInsight，然后选择适当的选项(参见图 [9-1](#Fig1) )。

![A462104_1_En_9_Fig1_HTML.jpg](img/A462104_1_En_9_Fig1_HTML.jpg)

图 9-1

Creating HDInsight from the Azure portal (search and select the image)

将出现一个页面，提供有关 HDInsight 的详细信息。在此页面上，点击创建(图 [9-2](#Fig2) )。

![A462104_1_En_9_Fig2_HTML.jpg](img/A462104_1_En_9_Fig2_HTML.jpg)

图 9-2

HDInsight service details page

现在，会出现一个表格，你必须在上面填写必要的信息。完成后，点击下一步(见图 [9-3](#Fig3) )。

![A462104_1_En_9_Fig3_HTML.jpg](img/A462104_1_En_9_Fig3_HTML.jpg)

图 9-3

Fill in the basic details

点击集群类型并选择您的首选处理框架(图 [9-4](#Fig4) )。

![A462104_1_En_9_Fig4_HTML.jpg](img/A462104_1_En_9_Fig4_HTML.jpg)

图 9-4

Click Cluster type and choose Spark (version 2.2.0)

现在返回并单击下一步。在这里，您可以指定与存储相关的信息。现在，您可以保留其默认设置，并点击下一步(见图 [9-5](#Fig5) )。

![A462104_1_En_9_Fig5_HTML.jpg](img/A462104_1_En_9_Fig5_HTML.jpg)

图 9-5

Specify the storage information

现在，单击 Create 提交带有 Spark 的 HDInsight 集群的部署(参见图 [9-6](#Fig6) )。

![A462104_1_En_9_Fig6_HTML.jpg](img/A462104_1_En_9_Fig6_HTML.jpg)

图 9-6

Summary form

第二步:让我们使用 SSH 进入 Spark 集群。导航到 SSH ➤集群登录，然后从下拉菜单中选择主机名，并在它下面的框中复制 SSH 命令(参见图 [9-7](#Fig7) )。现在打开 SSH 工具，复制粘贴命令连接到头节点(参见图 [9-8](#Fig8) )。

![A462104_1_En_9_Fig8_HTML.jpg](img/A462104_1_En_9_Fig8_HTML.jpg)

图 9-8

Connect to Head Node using SSH

![A462104_1_En_9_Fig7_HTML.png](img/A462104_1_En_9_Fig7_HTML.png)

图 9-7

Locating the SSH command

步骤 3:使用以下代码下载 Spark 连接器:

```js
wget https://scaleteststore.blob.core.windows.net/mongo/mongo-spark-connector-assembly-2.2.0.jar

```

第 4 步:运行 Spark shell 命令，用您的 Mongo 端点、数据库和集合细节替换它。输入和输出可以是相同的。

```js
spark-shell --conf "spark.mongodb.input.uri=mongodb://testmongo:jsFlj8MAqCDqjaPBE2DWRhm9jRx5QfMQ3SYf9vwGxElPjZmeQKO1vbA==@testmongo.documents.azure.com:10255/?ssl=true&replicaSet=globaldb" --conf "spark.mongodb.output.uri=mongodb://testmongobook:jsF6xFsNXz6lZ3tGVjx7bErkQCzoJUzyI2lj8MAqCDqjaPBE2DWRhm9jRx5QfMQ3SYf9vwGxElPjZmeQKO1vbA==@testmongobook.documents.azure.com:10255/?ssl=true&replicaSet=globaldb" --conf "spark.mongodb.input.database=db" --conf="spark.mongodb.input.collection=eventmsgss" --conf "spark.mongodb.output.database=db" --conf="spark.mongodb.output.collection=coll" --jars mongo-spark-connector-assembly-2.2.0.jar

```

现在，在成功执行上述代码后，您将获得 Scala 控制台，这是 SparkSQL 的游乐场。执行以下代码:

```js
scala> import com.mongodb.spark._
import com.mongodb.spark._
scala> import org.bson.Document
import org.bson.Document
scala> val rdd = MongoSpark.load(sc)

```

现在，让我们执行几个聚合查询(参见清单 [9-8](#Par47) 和 [9-9](#Par48) )。

```js
scala> val agg = rdd.withPipeline(Seq(Document.parse("{ $match: { DeviceId : { $eq : 1004 } } }")))
scala> println(agg.count)

Listing 9-8Counting the Number of Records on the Basis of the Filter

```

让我们看另一个例子。

```js
val agg = rdd.withPipeline(Seq(Document.parse("{ $group: { _id : '$SensorId', total:{$sum:1} } } ")))

Listing 9-9Grouping by SensorId and Counting the Values

```

现在，您可以更高效地运行所有聚合查询。您可以将结果导出到 Azure Cosmos DB 的另一个集合中，这将帮助您分析所有数据，并离线聚合这些数据。然后可以使用这个集合在 UI 上快速展示结果。

## 结论

现在，您已经了解了 Azure Cosmos DB–Mongo DB API 的大部分特性和功能。协议支持保持了无缝的迁移路径和最小的学习曲线。在单个 Azure Cosmos DB 实例中，如果您配置了最少一个地理复制，您将获得每个分区的高可用性和内置的灾难恢复。此外，您还可以获得针对一致性、可用性、延迟和吞吐量的全面 SLA，这可能是一件非常昂贵的事情。这将帮助您使您的应用全年高效可用，尽可能减少延迟。所有这些都将提高应用的用户体验。考虑到这些事实，选择 Azure Cosmos DB 升级您的应用并添加特色架构工具没有太多麻烦是显而易见的。

Note

Azure Cosmos DB–MongoDB API 受到 MongoDB 中不存在的特性的限制，例如存储过程、函数、变更提要等。然而，我希望所有这些特性将很快成为这个 API 的一部分。